{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94EwqyAsZoYD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Google Drive'\u0131 ba\u011fla\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab'da yeni h\u00fccre a\u00e7\u0131n ve \u00e7al\u0131\u015ft\u0131r\u0131n:\n",
        "import json\n",
        "\n",
        "# Mevcut notebook'u oku\n",
        "with open('/content/drive/MyDrive/Makine \u00d6\u011frenmesi/Makine\u00d6\u011fernmesi.ipynb', 'r') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# SADECE widget metadata'y\u0131 temizle (kodlar ve sonu\u00e7lar KALIR)\n",
        "if 'metadata' in nb and 'widgets' in nb['metadata']:\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Temizlenmi\u015f versiyonu kaydet\n",
        "with open('/content/drive/MyDrive/Makine \u00d6\u011frenmesi/Makine\u00d6\u011fernmesi_clean.ipynb', 'w') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"\u2705 Temizleme tamamland\u0131! Kodlar ve sonu\u00e7lar korundu.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qASF6oZFa9dd",
        "executionInfo": {
          "status": "error",
          "timestamp": 1755608593019,
          "user_tz": -180,
          "elapsed": 54,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          }
        },
        "outputId": "5c05f289-31f9-4f98-d2b9-fc0055278690"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/Makine\u00d6\u011fernmesi.ipynb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4277094849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mevcut notebook'u oku\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Makine \u00d6\u011frenmesi/Makine\u00d6\u011fernmesi.ipynb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/Makine\u00d6\u011fernmesi.ipynb'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Makine \u00d6\u011frenmesi klas\u00f6r\u00fc varsa:\n",
        "print(\"\ud83d\udcc1 Makine \u00d6\u011frenmesi klas\u00f6r\u00fc i\u00e7eri\u011fi:\")\n",
        "os.listdir('/content/drive/MyDrive/Makine \u00d6\u011frenmesi/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "K9fAdhCNbdOF",
        "executionInfo": {
          "status": "error",
          "timestamp": 1755608636610,
          "user_tz": -180,
          "elapsed": 147,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          }
        },
        "outputId": "fb23effc-6763-411d-ccd6-681742b80b99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcc1 Makine \u00d6\u011frenmesi klas\u00f6r\u00fc i\u00e7eri\u011fi:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-361784845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Makine \u00d6\u011frenmesi klas\u00f6r\u00fc varsa:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\ud83d\udcc1 Makine \u00d6\u011frenmesi klas\u00f6r\u00fc i\u00e7eri\u011fi:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Makine \u00d6\u011frenmesi/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "executionInfo": {
          "elapsed": 12864,
          "status": "error",
          "timestamp": 1755261279970,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "Ec-X5R-BuWLR",
        "outputId": "1e4d4324-961d-4787-d300-85f5011f47e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 BERT + RoBERTa FUSION - FULL DATASET (A100)\n",
            "======================================================================\n",
            "\ud83c\udfaf 15,167 Turkish reviews - 4 fusion strategies\n",
            "\ud83d\ude80 GPU: A100 - Production ready testing\n",
            "\u23f0 Estimated time: 2-3 hours\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.5 GB\n",
            "\ud83d\udd25 STARTING FULL DATASET BERT + RoBERTa FUSION COMPARISON\n",
            "\ud83d\ude80 A100 GPU - Production Ready Testing\n",
            "\u23f0 Estimated completion: 2-3 hours\n",
            "\n",
            "\ud83d\udcca FULL DATASET LOADING...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4067432437.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m \u001b[0mfusion_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_full_fusion_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4067432437.py\u001b[0m in \u001b[0;36mrun_full_fusion_comparison\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# Load full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'etiket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "import time\n",
        "import gc\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "print(\"\ud83d\udd25 BERT + RoBERTa FUSION - FULL DATASET (A100)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf 15,167 Turkish reviews - 4 fusion strategies\")\n",
        "print(\"\ud83d\ude80 GPU: A100 - Production ready testing\")\n",
        "print(\"\u23f0 Estimated time: 2-3 hours\")\n",
        "print()\n",
        "\n",
        "# GPU optimizasyonu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.backends.cudnn.benchmark = True  # A100 i\u00e7in optimization\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    \"\"\"Memory efficient dataset\"\"\"\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "class BertRobertaFusionModel(nn.Module):\n",
        "    def __init__(self, fusion_type='attention', max_length=128, dropout=0.3):\n",
        "        super(BertRobertaFusionModel, self).__init__()\n",
        "\n",
        "        print(f\"\ud83c\udfd7\ufe0f Building {fusion_type.upper()} fusion model...\")\n",
        "\n",
        "        # Model configurations\n",
        "        self.bert_model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "        self.roberta_model_name = \"xlm-roberta-base\"\n",
        "        self.max_length = max_length\n",
        "        self.fusion_type = fusion_type\n",
        "\n",
        "        # Load tokenizers\n",
        "        print(\"\ud83d\udce6 Loading tokenizers...\")\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_tokenizer = AutoTokenizer.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        # Load models\n",
        "        print(\"\ud83e\udd16 Loading BERT and RoBERTa models...\")\n",
        "        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_model = AutoModel.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        # Freeze backbone models (A100'de memory i\u00e7in)\n",
        "        print(\"\ud83d\udd12 Freezing backbone models...\")\n",
        "        for param in self.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.roberta_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Fusion layers\n",
        "        self.hidden_dim = 768\n",
        "\n",
        "        if fusion_type == 'concatenation':\n",
        "            self.fusion_layer = ConcatenationFusion(dropout)\n",
        "        elif fusion_type == 'attention':\n",
        "            self.fusion_layer = AttentionFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'gated':\n",
        "            self.fusion_layer = GatedFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'adaptive':\n",
        "            self.fusion_layer = AdaptiveFusion(self.hidden_dim, dropout)\n",
        "\n",
        "        # Enhanced classification head for full dataset\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fusion_layer.output_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "        print(f\"\u2705 {fusion_type.upper()} model built successfully!\")\n",
        "\n",
        "    def encode_batch(self, texts, model_type='bert'):\n",
        "        \"\"\"Batch encoding for memory efficiency\"\"\"\n",
        "        if model_type == 'bert':\n",
        "            tokenizer = self.bert_tokenizer\n",
        "            model = self.bert_model\n",
        "        else:\n",
        "            tokenizer = self.roberta_tokenizer\n",
        "            model = self.roberta_model\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                          max_length=self.max_length, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            # Use [CLS] token representation\n",
        "            features = outputs.last_hidden_state[:, 0, :]  # [batch, 768]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, texts):\n",
        "        # Batch encoding\n",
        "        bert_features = self.encode_batch(texts, 'bert')\n",
        "        roberta_features = self.encode_batch(texts, 'roberta')\n",
        "\n",
        "        # Fusion\n",
        "        fused_features = self.fusion_layer(bert_features, roberta_features)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits\n",
        "\n",
        "# Enhanced Fusion Layers\n",
        "class ConcatenationFusion(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.output_dim = 1536  # 768 + 768\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concatenated = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        return self.dropout(concatenated)\n",
        "\n",
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        # Multi-head cross attention\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            hidden_dim, num_heads=12, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        # Feature enhancement\n",
        "        self.feature_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        # Add sequence dimension\n",
        "        bert_seq = bert_features.unsqueeze(1)\n",
        "        roberta_seq = roberta_features.unsqueeze(1)\n",
        "\n",
        "        # Bidirectional cross attention\n",
        "        bert_to_roberta, _ = self.cross_attention(bert_seq, roberta_seq, roberta_seq)\n",
        "        roberta_to_bert, _ = self.cross_attention(roberta_seq, bert_seq, bert_seq)\n",
        "\n",
        "        # Combine attended features\n",
        "        combined = torch.cat([\n",
        "            bert_to_roberta.squeeze(1),\n",
        "            roberta_to_bert.squeeze(1)\n",
        "        ], dim=1)\n",
        "\n",
        "        # Feature enhancement\n",
        "        enhanced = self.feature_enhance(combined)\n",
        "        return enhanced\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        # Sophisticated gating mechanism\n",
        "        self.bert_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.roberta_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Residual connection\n",
        "        self.residual_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        # Compute sophisticated gates\n",
        "        concat_features = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        bert_gate = self.bert_gate(concat_features)\n",
        "        roberta_gate = self.roberta_gate(concat_features)\n",
        "\n",
        "        # Gated fusion with residual connection\n",
        "        gated_bert = bert_gate * bert_features\n",
        "        gated_roberta = roberta_gate * roberta_features\n",
        "\n",
        "        fused = gated_bert + gated_roberta\n",
        "        enhanced = self.residual_layer(fused)\n",
        "\n",
        "        # Residual connection\n",
        "        output = enhanced + (bert_features + roberta_features) * 0.1\n",
        "        return output\n",
        "\n",
        "class AdaptiveFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        # Context analyzer\n",
        "        self.context_analyzer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 4),  # [bert_weight, roberta_weight, attention_weight, residual_weight]\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Fusion components\n",
        "        self.attention_fusion = AttentionFusion(hidden_dim, dropout)\n",
        "        self.gated_fusion = GatedFusion(hidden_dim, dropout)\n",
        "\n",
        "        # Final enhancement\n",
        "        self.final_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        # Analyze context for adaptive weighting\n",
        "        concat_features = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        weights = self.context_analyzer(concat_features)  # [batch, 4]\n",
        "\n",
        "        # Multiple fusion strategies\n",
        "        bert_weighted = weights[:, 0:1] * bert_features\n",
        "        roberta_weighted = weights[:, 1:2] * roberta_features\n",
        "\n",
        "        # Advanced fusion for complex patterns\n",
        "        attention_fused = self.attention_fusion(bert_features, roberta_features)\n",
        "        attention_weighted = weights[:, 2:3] * attention_fused\n",
        "\n",
        "        # Residual connection\n",
        "        residual = (bert_features + roberta_features) / 2\n",
        "        residual_weighted = weights[:, 3:4] * residual\n",
        "\n",
        "        # Adaptive combination\n",
        "        adaptive_fused = bert_weighted + roberta_weighted + attention_weighted + residual_weighted\n",
        "\n",
        "        # Final enhancement\n",
        "        enhanced = self.final_enhance(adaptive_fused)\n",
        "        return enhanced\n",
        "\n",
        "def train_fusion_model_full(model, train_dataset, val_dataset, epochs=5, batch_size=16):\n",
        "    \"\"\"Full dataset training with DataLoader\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizers for A100\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-6\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"\ud83d\ude80 {model.fusion_type.upper()} FUSION - FULL DATASET TRAINING\")\n",
        "    print(f\"\ud83d\udcca Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "    print(f\"\u2699\ufe0f Batch size: {batch_size}, Epochs: {epochs}\")\n",
        "\n",
        "    best_f1 = 0\n",
        "    training_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_texts, batch_labels in train_loader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_texts)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "            # Memory cleanup\n",
        "            if train_batches % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_texts, batch_labels in val_loader:\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                logits = model(batch_texts)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                val_predictions.extend(preds)\n",
        "                val_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
        "        val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "        val_precision, val_recall, _, _ = precision_recall_fscore_support(\n",
        "            val_true_labels, val_predictions, average='macro'\n",
        "        )\n",
        "\n",
        "        # Learning rate step\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        # Logging\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"    Train Loss: {train_loss/train_batches:.4f}\")\n",
        "        print(f\"    Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "        print(f\"    Val F1: {val_f1:.4f}, Acc: {val_acc:.4f}\")\n",
        "        print(f\"    Val Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
        "        print(f\"    Time: {epoch_time:.1f}s, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(model.state_dict(), f'/content/drive/MyDrive/best_{model.fusion_type}_fusion.pth')\n",
        "            print(f\"    \ud83c\udfc6 New best F1: {best_f1:.4f} - Model saved!\")\n",
        "\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss/train_batches,\n",
        "            'val_loss': val_loss/len(val_loader),\n",
        "            'val_f1': val_f1,\n",
        "            'val_accuracy': val_acc,\n",
        "            'val_precision': val_precision,\n",
        "            'val_recall': val_recall\n",
        "        })\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model, best_f1, training_history\n",
        "\n",
        "def run_full_fusion_comparison():\n",
        "    \"\"\"Full dataset fusion comparison\"\"\"\n",
        "\n",
        "    print(\"\ud83d\udcca FULL DATASET LOADING...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load full dataset\n",
        "    df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "    # Full dataset\n",
        "    texts = df_clean['metin'].astype(str).tolist()\n",
        "    labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "    print(f\"\u2705 Full dataset loaded: {len(texts)} reviews\")\n",
        "    print(f\"\ud83d\udcca Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Stratified train/validation split\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Validation: {len(val_texts)}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels)\n",
        "\n",
        "    # Test fusion strategies\n",
        "    fusion_strategies = ['concatenation', 'attention', 'gated', 'adaptive']\n",
        "    results = []\n",
        "\n",
        "    for i, strategy in enumerate(fusion_strategies):\n",
        "        print(f\"\\n{'='*20} FUSION {i+1}/4: {strategy.upper()} {'='*20}\")\n",
        "\n",
        "        try:\n",
        "            strategy_start = time.time()\n",
        "\n",
        "            # Create model\n",
        "            model = BertRobertaFusionModel(\n",
        "                fusion_type=strategy,\n",
        "                max_length=128,\n",
        "                dropout=0.3\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            trained_model, best_f1, history = train_fusion_model_full(\n",
        "                model, train_dataset, val_dataset,\n",
        "                epochs=4, batch_size=16  # A100 i\u00e7in optimize edilmi\u015f\n",
        "            )\n",
        "\n",
        "            strategy_time = time.time() - strategy_start\n",
        "\n",
        "            # Final comprehensive evaluation\n",
        "            print(f\"\\n\ud83d\udd2c {strategy.upper()} FINAL EVALUATION...\")\n",
        "            trained_model.eval()\n",
        "\n",
        "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "            final_predictions = []\n",
        "            final_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_texts, batch_labels in val_loader:\n",
        "                    logits = trained_model(batch_texts)\n",
        "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                    final_predictions.extend(preds)\n",
        "                    final_true_labels.extend(batch_labels.numpy())\n",
        "\n",
        "            # Comprehensive metrics\n",
        "            final_acc = accuracy_score(final_true_labels, final_predictions)\n",
        "            final_f1 = f1_score(final_true_labels, final_predictions, average='macro')\n",
        "            final_precision, final_recall, _, _ = precision_recall_fscore_support(\n",
        "                final_true_labels, final_predictions, average='macro'\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': final_f1,\n",
        "                'Accuracy': final_acc,\n",
        "                'Precision': final_precision,\n",
        "                'Recall': final_recall,\n",
        "                'Best_F1_During_Training': best_f1,\n",
        "                'Training_Time_Minutes': strategy_time / 60,\n",
        "                'Status': 'Success'\n",
        "            })\n",
        "\n",
        "            print(f\"\u2705 {strategy.upper()} COMPLETED:\")\n",
        "            print(f\"   Final F1: {final_f1:.4f}\")\n",
        "            print(f\"   Final Accuracy: {final_acc:.4f}\")\n",
        "            print(f\"   Final Precision: {final_precision:.4f}\")\n",
        "            print(f\"   Final Recall: {final_recall:.4f}\")\n",
        "            print(f\"   Training Time: {strategy_time/60:.1f} minutes\")\n",
        "\n",
        "            # Clean up memory\n",
        "            del model, trained_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c {strategy.upper()} FAILED: {str(e)}\")\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': 0.0,\n",
        "                'Accuracy': 0.0,\n",
        "                'Precision': 0.0,\n",
        "                'Recall': 0.0,\n",
        "                'Best_F1_During_Training': 0.0,\n",
        "                'Training_Time_Minutes': 0.0,\n",
        "                'Status': f'Error: {str(e)[:100]}'\n",
        "            })\n",
        "\n",
        "    # Final analysis\n",
        "    print(f\"\\n\ud83c\udfc6 FULL DATASET FUSION COMPARISON RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    successful_results = results_df[results_df['Status'] == 'Success']\n",
        "\n",
        "    if not successful_results.empty:\n",
        "        successful_results = successful_results.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "        print(\"\ud83e\udd47 FUSION STRATEGY RANKINGS:\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, (_, row) in enumerate(successful_results.iterrows()):\n",
        "            rank = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\", \"4\ufe0f\u20e3\"][i] if i < 4 else f\"{i+1}\ufe0f\u20e3\"\n",
        "            print(f\"{rank} {row['Fusion_Strategy'].upper():15}\")\n",
        "            print(f\"    F1: {row['F1_Score']:.4f}, Acc: {row['Accuracy']:.4f}\")\n",
        "            print(f\"    Precision: {row['Precision']:.4f}, Recall: {row['Recall']:.4f}\")\n",
        "            print(f\"    Training Time: {row['Training_Time_Minutes']:.1f} min\")\n",
        "            print()\n",
        "\n",
        "        # Best fusion analysis\n",
        "        best_fusion = successful_results.iloc[0]\n",
        "        print(f\"\ud83c\udfc6 BEST FUSION STRATEGY: {best_fusion['Fusion_Strategy'].upper()}\")\n",
        "        print(f\"\ud83d\udcca Performance Metrics:\")\n",
        "        print(f\"   F1 Score: {best_fusion['F1_Score']:.4f}\")\n",
        "        print(f\"   Accuracy: {best_fusion['Accuracy']:.4f}\")\n",
        "        print(f\"   Precision: {best_fusion['Precision']:.4f}\")\n",
        "        print(f\"   Recall: {best_fusion['Recall']:.4f}\")\n",
        "\n",
        "        # Baseline comparison\n",
        "        bert_baseline = 0.9010  # Turkish BERT + Threshold\n",
        "        roberta_baseline = 0.8816  # XLM-RoBERTa Fine-tuned\n",
        "\n",
        "        print(f\"\\n\ud83d\udcc8 BASELINE COMPARISON:\")\n",
        "        print(f\"BERT Baseline (90.10%):     {bert_baseline:.4f}\")\n",
        "        print(f\"RoBERTa Baseline (88.16%):  {roberta_baseline:.4f}\")\n",
        "        print(f\"Best Fusion:                {best_fusion['F1_Score']:.4f}\")\n",
        "\n",
        "        bert_improvement = best_fusion['F1_Score'] - bert_baseline\n",
        "        roberta_improvement = best_fusion['F1_Score'] - roberta_baseline\n",
        "\n",
        "        print(f\"Improvement vs BERT:        {bert_improvement:+.4f} ({bert_improvement/bert_baseline*100:+.2f}%)\")\n",
        "        print(f\"Improvement vs RoBERTa:     {roberta_improvement:+.4f} ({roberta_improvement/roberta_baseline*100:+.2f}%)\")\n",
        "\n",
        "        if bert_improvement > 0.005:\n",
        "            print(\"\ud83d\ude80 FUSION SUCCESS! Significant improvement achieved!\")\n",
        "        elif bert_improvement > 0:\n",
        "            print(\"\u2705 FUSION BENEFICIAL! Modest improvement achieved!\")\n",
        "        else:\n",
        "            print(\"\ud83e\udd14 FUSION INCONCLUSIVE! Further optimization needed!\")\n",
        "\n",
        "    # Save comprehensive results\n",
        "    results_df.to_excel(\"/content/drive/MyDrive/BERT_ROBERTA_FUSION_FULL_RESULTS.xlsx\", index=False)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\u23f1\ufe0f TOTAL EXPERIMENT TIME: {total_time/3600:.1f} hours\")\n",
        "    print(f\"\u2705 Results saved: BERT_ROBERTA_FUSION_FULL_RESULTS.xlsx\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Start full fusion comparison\n",
        "print(\"\ud83d\udd25 STARTING FULL DATASET BERT + RoBERTa FUSION COMPARISON\")\n",
        "print(\"\ud83d\ude80 A100 GPU - Production Ready Testing\")\n",
        "print(\"\u23f0 Estimated completion: 2-3 hours\")\n",
        "print()\n",
        "\n",
        "fusion_results = run_full_fusion_comparison()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e2b0477c9f24405da263228792a61989",
            "3945b225f9474c309d8e81dbe2d84527",
            "0e215763c5fe4fd29747958cb6a29a61",
            "a480496f796542058c886d2fc293ec2a",
            "9e336bc47c5345a09b2e2d86264011c2",
            "a3fc2caf619640b184e96ad30ea02bb7",
            "d1202cd8e1c84682a8fb763480fd8cab",
            "c0b4e683d0134a1eaf9407ea6eb7ca6f",
            "d390948ef9f041a09a8077239b2d5210",
            "4390cf0e03dc4ef3b18933f6c81ac602",
            "b2dbb3499895460082f74e0defe00495",
            "bec85c2176844ae29b97667744d5c24a",
            "b44a0ee19c724b71af0b275dc837222b",
            "a4b25c80d4fc4396abfc6cedfc13c918",
            "8d8e27c19e124ad283d35db169b2dbe5",
            "5c5b80fbec0541459a38aa1d9caa0008",
            "b92bd65d332643bb8db94ee49bb0e5ec",
            "dc601b75e99a45e98317224b5245c6a0",
            "bfae08ce4b4f44e6bc8c2123118b7063",
            "620f9b57f53e4228b8611c23d1ce99da",
            "98904ccebdf84a91baf2d8565ba1f1f8",
            "556b14ca944b4e40bb48dc94e6709c29",
            "ca562228d128431494b39268939321c2",
            "c071a07c442e4c9fa449e19d8fdd2144",
            "3f82ce9ae43944dca6cd067b94ccba99",
            "2ca0c46a8c7d4519ab523d88bf33ddd2",
            "377a997d97f14542823791fa54ae2715",
            "2c9b0025ee6248e3bf82f0f41753e7f7",
            "adef393e665b4b31958d9021b5fbacd9",
            "4cd66e604a14460ab8c6794c7eb40a64",
            "5d9d63d00ac649eaa362893c7bcb7e7b",
            "7aa63a53d08646f0b804fbb9d111633e",
            "16ba4c031945477f8911054838cb9495",
            "a5ab252858b24d6487cda35119d3dc2c",
            "c45c8c9c068b4ae392c0c11b230ad99f",
            "89b70143b9a345cc99d74ed814e96ef3",
            "43041aabf0504302bb53fe660bf5146a",
            "da560ffb2e9b4dcca4d18f571c46e95f",
            "e9533a841c664c36ad41375b84ec742a",
            "f4c59fad733742dba5f700fd8ce6c697",
            "b84215e01d16443487a4c72ba731162a",
            "908e4b5100af4dcebd6cc70c018d4cab",
            "8955b8d00bbc4247bb0dcc5a0a898fa4",
            "45296fa905314ab9a2ef756f46cb8e0a",
            "b2a47f9ca3a646cd84e9f94bc77364e3",
            "a5a5fad0be704af6940e5f457d343fc6",
            "3f6131138a4e4b4e966ff2479f2fdb20",
            "e0915aa114054e52acfcc120344aebbe",
            "26de7b5f5f3f43b19c1d62e83c04dc7a",
            "4605e595335e4ac3a70199e82bf5774f",
            "b5ee457303cc44c3a17e2c4fc4a15514",
            "abd9d8f91bb744599e9be694b686dea1",
            "ff4f288cd7db4000a797cfbff1018c6e",
            "1c9f6b27b1ef4d03b240208ab186a0c8",
            "bc6675c2b5da4996b1e4d54634ad7f7a",
            "28982f31b31e4f2f901a68c4ae461f2b",
            "1aeb210dbeff43ce8042f0dfae5f2ef2",
            "b69ea755dca64c6f8a1b5420260b4f9e",
            "61ebb57fee3e47f78b48a8e6003bbad7",
            "71e02fd1306045b8935ba42a8809e8e4",
            "bef1321d47cb42b48ffd21ec938edb29",
            "d75048f971cc4b25aafe63ab87757e46",
            "feabfb2677964068bf39700e3556d49b",
            "10d47aebf9494299af2b420def55a675",
            "2a09571eb91d47de947c7007c9637f83",
            "d53d8f0494d947758df855b4fe580b15",
            "a1eebe15edca4921b0f11a4ec35670a8",
            "9a4cd8c4702947238be4d1d7e58ad252",
            "2549adcee0004651952c91883e57a405",
            "41091bc80798430a9f84b4caaae2987e",
            "e547ff13acff4df495dc0640f896fb8c",
            "388ff3817c6c4e8ea3209a046fe96010",
            "31475c95cabb441fa8fe08c2e8512148",
            "d102b81112ee4f60af665060cf6ca76f",
            "aedb48ab97ff439ba2ab7efcf2967d46",
            "d2587c6d2f3041809f18139f8aafb084",
            "cec026f90d36480d8e2ed4fdecbd4587",
            "22e0940e2b9a4ede862b84f4cdbdc909",
            "9d78f9ec747a45daad62ba9534d9ca02",
            "f292da4d207b47e1b97252582cbce19c",
            "82244a8bd47646c28a342ba1d690f969",
            "738c271cafa94cb9bb5a7b37842f5c92",
            "879ee85bd5074292832d7bac25b61659",
            "23cbcdea3e1f45a69fe66e3fdb3c9477",
            "4220f418e32540808e4176bae5ff99f2",
            "d13a63b2dc9b448b9cfd507710406e45",
            "0a658ee4b4b247419f92bad890a145b9",
            "d9264db50cde400bb8c9e43723e24f4b",
            "bba654c11cf14f3c88eb33bb34ecdd45",
            "612009c749bf4c28a64fce4dc0744296",
            "1dff7fd856fe4391aee001af8c5639e2",
            "4ed1b4fe8a1347d985a7f517f8763bd6",
            "d7183134b6514d739fb314f50b66b22b",
            "f735acb061d342c49167cbfa81335947",
            "1bffa5018625479a83b7cbd67a16991a",
            "3d98cc98a8f140d58c2d9f300981ed83",
            "b1424db840fb4c6b9ab0c83fe921c822",
            "e7b63cbf788d41569961246ebd7350ab",
            "91cf1d1cf64b4a41a48f2c0b8338e6dc"
          ]
        },
        "executionInfo": {
          "elapsed": 604414,
          "status": "ok",
          "timestamp": 1755262681279,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "EsX4r9mMvaZO",
        "outputId": "60766e03-83ad-4a57-ddc8-b462b4898fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 BERT + RoBERTa FUSION - FULL DATASET (A100)\n",
            "======================================================================\n",
            "\ud83c\udfaf 15,170 Turkish reviews - 4 fusion strategies\n",
            "\ud83d\ude80 GPU: A100 - Production ready testing\n",
            "\u23f0 Estimated time: 2-3 hours\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.5 GB\n",
            "\ud83d\udd25 STARTING FULL DATASET BERT + RoBERTa FUSION\n",
            "\ud83d\ude80 A100 GPU - Production Ready Testing\n",
            "\n",
            "\ud83d\udcca FULL DATASET LOADING...\n",
            "\u2705 Full dataset loaded: 15167 reviews\n",
            "\ud83d\udcca Class distribution: [6686 8481]\n",
            "\ud83d\udcca Train: 12891, Validation: 2276\n",
            "\n",
            "==================== FUSION 1/4: CONCATENATION ====================\n",
            "\ud83c\udfd7\ufe0f Building CONCATENATION fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2b0477c9f24405da263228792a61989",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bec85c2176844ae29b97667744d5c24a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca562228d128431494b39268939321c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5ab252858b24d6487cda35119d3dc2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2a47f9ca3a646cd84e9f94bc77364e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28982f31b31e4f2f901a68c4ae461f2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1eebe15edca4921b0f11a4ec35670a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83e\udd16 Loading BERT and RoBERTa models...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22e0940e2b9a4ede862b84f4cdbdc909",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bba654c11cf14f3c88eb33bb34ecdd45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd12 Freezing backbone models...\n",
            "\u2705 CONCATENATION model built successfully!\n",
            "\ud83d\ude80 CONCATENATION FUSION TRAINING\n",
            "\ud83d\udcca Train batches: 1075, Val batches: 95\n",
            "  Epoch 1/4: Loss=0.5506, Val_F1=0.7491, Val_Acc=0.7491 (34.0s)\n",
            "    \ud83c\udfc6 New best F1: 0.7491\n",
            "  Epoch 2/4: Loss=0.4811, Val_F1=0.8090, Val_Acc=0.8120 (32.9s)\n",
            "    \ud83c\udfc6 New best F1: 0.8090\n",
            "  Epoch 3/4: Loss=0.4465, Val_F1=0.8201, Val_Acc=0.8247 (32.9s)\n",
            "    \ud83c\udfc6 New best F1: 0.8201\n",
            "  Epoch 4/4: Loss=0.4315, Val_F1=0.8222, Val_Acc=0.8269 (32.8s)\n",
            "    \ud83c\udfc6 New best F1: 0.8222\n",
            "\n",
            "\ud83d\udd2c CONCATENATION FINAL EVALUATION...\n",
            "\u2705 CONCATENATION COMPLETED:\n",
            "   Final F1: 0.8222\n",
            "   Final Accuracy: 0.8269\n",
            "   Training Time: 2.5 minutes\n",
            "\n",
            "==================== FUSION 2/4: ATTENTION ====================\n",
            "\ud83c\udfd7\ufe0f Building ATTENTION fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading BERT and RoBERTa models...\n",
            "\ud83d\udd12 Freezing backbone models...\n",
            "\u2705 ATTENTION model built successfully!\n",
            "\ud83d\ude80 ATTENTION FUSION TRAINING\n",
            "\ud83d\udcca Train batches: 1075, Val batches: 95\n",
            "  Epoch 1/4: Loss=0.5151, Val_F1=0.8110, Val_Acc=0.8146 (36.1s)\n",
            "    \ud83c\udfc6 New best F1: 0.8110\n",
            "  Epoch 2/4: Loss=0.4284, Val_F1=0.8275, Val_Acc=0.8313 (36.1s)\n",
            "    \ud83c\udfc6 New best F1: 0.8275\n",
            "  Epoch 3/4: Loss=0.4149, Val_F1=0.8252, Val_Acc=0.8330 (36.1s)\n",
            "  Epoch 4/4: Loss=0.4031, Val_F1=0.8396, Val_Acc=0.8423 (36.0s)\n",
            "    \ud83c\udfc6 New best F1: 0.8396\n",
            "\n",
            "\ud83d\udd2c ATTENTION FINAL EVALUATION...\n",
            "\u2705 ATTENTION COMPLETED:\n",
            "   Final F1: 0.8396\n",
            "   Final Accuracy: 0.8423\n",
            "   Training Time: 2.5 minutes\n",
            "\n",
            "==================== FUSION 3/4: GATED ====================\n",
            "\ud83c\udfd7\ufe0f Building GATED fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading BERT and RoBERTa models...\n",
            "\ud83d\udd12 Freezing backbone models...\n",
            "\u2705 GATED model built successfully!\n",
            "\ud83d\ude80 GATED FUSION TRAINING\n",
            "\ud83d\udcca Train batches: 1075, Val batches: 95\n",
            "  Epoch 1/4: Loss=0.4894, Val_F1=0.8353, Val_Acc=0.8370 (34.4s)\n",
            "    \ud83c\udfc6 New best F1: 0.8353\n",
            "  Epoch 2/4: Loss=0.4064, Val_F1=0.8361, Val_Acc=0.8418 (34.3s)\n",
            "    \ud83c\udfc6 New best F1: 0.8361\n",
            "  Epoch 3/4: Loss=0.3881, Val_F1=0.8372, Val_Acc=0.8440 (34.4s)\n",
            "    \ud83c\udfc6 New best F1: 0.8372\n",
            "  Epoch 4/4: Loss=0.3769, Val_F1=0.8529, Val_Acc=0.8550 (34.4s)\n",
            "    \ud83c\udfc6 New best F1: 0.8529\n",
            "\n",
            "\ud83d\udd2c GATED FINAL EVALUATION...\n",
            "\u2705 GATED COMPLETED:\n",
            "   Final F1: 0.8529\n",
            "   Final Accuracy: 0.8550\n",
            "   Training Time: 2.4 minutes\n",
            "\n",
            "==================== FUSION 4/4: ADAPTIVE ====================\n",
            "\ud83c\udfd7\ufe0f Building ADAPTIVE fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading BERT and RoBERTa models...\n",
            "\ud83d\udd12 Freezing backbone models...\n",
            "\u2705 ADAPTIVE model built successfully!\n",
            "\ud83d\ude80 ADAPTIVE FUSION TRAINING\n",
            "\ud83d\udcca Train batches: 1075, Val batches: 95\n",
            "  Epoch 1/4: Loss=0.5236, Val_F1=0.8047, Val_Acc=0.8049 (34.8s)\n",
            "    \ud83c\udfc6 New best F1: 0.8047\n",
            "  Epoch 2/4: Loss=0.4261, Val_F1=0.8325, Val_Acc=0.8374 (34.6s)\n",
            "    \ud83c\udfc6 New best F1: 0.8325\n",
            "  Epoch 3/4: Loss=0.4019, Val_F1=0.8392, Val_Acc=0.8436 (34.7s)\n",
            "    \ud83c\udfc6 New best F1: 0.8392\n",
            "  Epoch 4/4: Loss=0.3946, Val_F1=0.8445, Val_Acc=0.8493 (34.7s)\n",
            "    \ud83c\udfc6 New best F1: 0.8445\n",
            "\n",
            "\ud83d\udd2c ADAPTIVE FINAL EVALUATION...\n",
            "\u2705 ADAPTIVE COMPLETED:\n",
            "   Final F1: 0.8445\n",
            "   Final Accuracy: 0.8493\n",
            "   Training Time: 2.4 minutes\n",
            "\n",
            "\ud83c\udfc6 FULL DATASET FUSION RESULTS\n",
            "================================================================================\n",
            "\ud83e\udd47 FUSION RANKINGS:\n",
            "\ud83e\udd47 GATED           F1: 0.8529 (2.4min)\n",
            "\ud83e\udd48 ADAPTIVE        F1: 0.8445 (2.4min)\n",
            "\ud83e\udd49 ATTENTION       F1: 0.8396 (2.5min)\n",
            "4\ufe0f\u20e3 CONCATENATION   F1: 0.8222 (2.5min)\n",
            "\n",
            "\ud83d\udcc8 BASELINE COMPARISON:\n",
            "BERT Baseline (90.10%): 0.9010\n",
            "Best Fusion:            0.8529\n",
            "Improvement:            -0.0481 (-5.33%)\n",
            "\ud83e\udd14 More optimization needed!\n",
            "\n",
            "\u23f1\ufe0f TOTAL TIME: 0.2 hours\n",
            "\u2705 Results saved to Drive!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "import time\n",
        "import gc\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\ud83d\udd25 BERT + RoBERTa FUSION - FULL DATASET (A100)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf 15,170 Turkish reviews - 4 fusion strategies\")\n",
        "print(\"\ud83d\ude80 GPU: A100 - Production ready testing\")\n",
        "print(\"\u23f0 Estimated time: 2-3 hours\")\n",
        "print()\n",
        "\n",
        "# GPU optimizasyonu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# \u2705 DO\u011eRU DOSYA YOLU\n",
        "CORRECT_FILE_PATH = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "class BertRobertaFusionModel(nn.Module):\n",
        "    def __init__(self, fusion_type='attention', max_length=128, dropout=0.3):\n",
        "        super(BertRobertaFusionModel, self).__init__()\n",
        "\n",
        "        print(f\"\ud83c\udfd7\ufe0f Building {fusion_type.upper()} fusion model...\")\n",
        "\n",
        "        self.bert_model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "        self.roberta_model_name = \"xlm-roberta-base\"\n",
        "        self.max_length = max_length\n",
        "        self.fusion_type = fusion_type\n",
        "\n",
        "        print(\"\ud83d\udce6 Loading tokenizers...\")\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_tokenizer = AutoTokenizer.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(\"\ud83e\udd16 Loading BERT and RoBERTa models...\")\n",
        "        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_model = AutoModel.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(\"\ud83d\udd12 Freezing backbone models...\")\n",
        "        for param in self.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.roberta_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.hidden_dim = 768\n",
        "\n",
        "        if fusion_type == 'concatenation':\n",
        "            self.fusion_layer = ConcatenationFusion(dropout)\n",
        "        elif fusion_type == 'attention':\n",
        "            self.fusion_layer = AttentionFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'gated':\n",
        "            self.fusion_layer = GatedFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'adaptive':\n",
        "            self.fusion_layer = AdaptiveFusion(self.hidden_dim, dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fusion_layer.output_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "        print(f\"\u2705 {fusion_type.upper()} model built successfully!\")\n",
        "\n",
        "    def encode_batch(self, texts, model_type='bert'):\n",
        "        if model_type == 'bert':\n",
        "            tokenizer = self.bert_tokenizer\n",
        "            model = self.bert_model\n",
        "        else:\n",
        "            tokenizer = self.roberta_tokenizer\n",
        "            model = self.roberta_model\n",
        "\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                          max_length=self.max_length, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            features = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, texts):\n",
        "        bert_features = self.encode_batch(texts, 'bert')\n",
        "        roberta_features = self.encode_batch(texts, 'roberta')\n",
        "\n",
        "        fused_features = self.fusion_layer(bert_features, roberta_features)\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits\n",
        "\n",
        "class ConcatenationFusion(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.output_dim = 1536\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concatenated = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        return self.dropout(concatenated)\n",
        "\n",
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.cross_attention = nn.MultiheadAttention(\n",
        "            hidden_dim, num_heads=8, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.feature_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        bert_seq = bert_features.unsqueeze(1)\n",
        "        roberta_seq = roberta_features.unsqueeze(1)\n",
        "\n",
        "        bert_to_roberta, _ = self.cross_attention(bert_seq, roberta_seq, roberta_seq)\n",
        "        roberta_to_bert, _ = self.cross_attention(roberta_seq, bert_seq, bert_seq)\n",
        "\n",
        "        combined = torch.cat([\n",
        "            bert_to_roberta.squeeze(1),\n",
        "            roberta_to_bert.squeeze(1)\n",
        "        ], dim=1)\n",
        "\n",
        "        enhanced = self.feature_enhance(combined)\n",
        "        return enhanced\n",
        "\n",
        "class GatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.bert_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.roberta_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.residual_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concat_features = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        bert_gate = self.bert_gate(concat_features)\n",
        "        roberta_gate = self.roberta_gate(concat_features)\n",
        "\n",
        "        gated_bert = bert_gate * bert_features\n",
        "        gated_roberta = roberta_gate * roberta_features\n",
        "\n",
        "        fused = gated_bert + gated_roberta\n",
        "        enhanced = self.residual_layer(fused)\n",
        "\n",
        "        output = enhanced + (bert_features + roberta_features) * 0.1\n",
        "        return output\n",
        "\n",
        "class AdaptiveFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.context_analyzer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 4),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.final_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concat_features = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        weights = self.context_analyzer(concat_features)\n",
        "\n",
        "        bert_weighted = weights[:, 0:1] * bert_features\n",
        "        roberta_weighted = weights[:, 1:2] * roberta_features\n",
        "\n",
        "        attention_fused = (bert_features + roberta_features) / 2\n",
        "        attention_weighted = weights[:, 2:3] * attention_fused\n",
        "\n",
        "        residual = (bert_features + roberta_features) / 2\n",
        "        residual_weighted = weights[:, 3:4] * residual\n",
        "\n",
        "        adaptive_fused = bert_weighted + roberta_weighted + attention_weighted + residual_weighted\n",
        "        enhanced = self.final_enhance(adaptive_fused)\n",
        "        return enhanced\n",
        "\n",
        "def train_fusion_model_full(model, train_dataset, val_dataset, epochs=4, batch_size=16):\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=2e-5,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"\ud83d\ude80 {model.fusion_type.upper()} FUSION TRAINING\")\n",
        "    print(f\"\ud83d\udcca Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_texts, batch_labels in train_loader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_texts)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "            if train_batches % 100 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_texts, batch_labels in val_loader:\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                logits = model(batch_texts)\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                val_predictions.extend(preds)\n",
        "                val_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
        "        val_f1 = f1_score(val_true_labels, val_predictions, average='macro')\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}: Loss={train_loss/train_batches:.4f}, Val_F1={val_f1:.4f}, Val_Acc={val_acc:.4f} ({epoch_time:.1f}s)\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            print(f\"    \ud83c\udfc6 New best F1: {best_f1:.4f}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model, best_f1\n",
        "\n",
        "def run_full_fusion_comparison():\n",
        "    print(\"\ud83d\udcca FULL DATASET LOADING...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # \u2705 DO\u011eRU DOSYA YOLU \u0130LE Y\u00dcKLEY\u0130M\n",
        "    df = pd.read_excel(CORRECT_FILE_PATH)\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "    texts = df_clean['metin'].astype(str).tolist()\n",
        "    labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "    print(f\"\u2705 Full dataset loaded: {len(texts)} reviews\")\n",
        "    print(f\"\ud83d\udcca Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Train/validation split\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Validation: {len(val_texts)}\")\n",
        "\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels)\n",
        "\n",
        "    # Test fusion strategies\n",
        "    fusion_strategies = ['concatenation', 'attention', 'gated', 'adaptive']\n",
        "    results = []\n",
        "\n",
        "    for i, strategy in enumerate(fusion_strategies):\n",
        "        print(f\"\\n{'='*20} FUSION {i+1}/4: {strategy.upper()} {'='*20}\")\n",
        "\n",
        "        try:\n",
        "            strategy_start = time.time()\n",
        "\n",
        "            model = BertRobertaFusionModel(\n",
        "                fusion_type=strategy,\n",
        "                max_length=128,\n",
        "                dropout=0.3\n",
        "            )\n",
        "\n",
        "            trained_model, best_f1 = train_fusion_model_full(\n",
        "                model, train_dataset, val_dataset,\n",
        "                epochs=4, batch_size=12  # A100 i\u00e7in optimize\n",
        "            )\n",
        "\n",
        "            strategy_time = time.time() - strategy_start\n",
        "\n",
        "            # Final evaluation\n",
        "            print(f\"\\n\ud83d\udd2c {strategy.upper()} FINAL EVALUATION...\")\n",
        "            trained_model.eval()\n",
        "\n",
        "            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "            final_predictions = []\n",
        "            final_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_texts, batch_labels in val_loader:\n",
        "                    logits = trained_model(batch_texts)\n",
        "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                    final_predictions.extend(preds)\n",
        "                    final_true_labels.extend(batch_labels.numpy())\n",
        "\n",
        "            final_acc = accuracy_score(final_true_labels, final_predictions)\n",
        "            final_f1 = f1_score(final_true_labels, final_predictions, average='macro')\n",
        "            final_precision, final_recall, _, _ = precision_recall_fscore_support(\n",
        "                final_true_labels, final_predictions, average='macro'\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': final_f1,\n",
        "                'Accuracy': final_acc,\n",
        "                'Precision': final_precision,\n",
        "                'Recall': final_recall,\n",
        "                'Training_Time_Minutes': strategy_time / 60,\n",
        "                'Status': 'Success'\n",
        "            })\n",
        "\n",
        "            print(f\"\u2705 {strategy.upper()} COMPLETED:\")\n",
        "            print(f\"   Final F1: {final_f1:.4f}\")\n",
        "            print(f\"   Final Accuracy: {final_acc:.4f}\")\n",
        "            print(f\"   Training Time: {strategy_time/60:.1f} minutes\")\n",
        "\n",
        "            del model, trained_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c {strategy.upper()} FAILED: {str(e)}\")\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': 0.0,\n",
        "                'Accuracy': 0.0,\n",
        "                'Precision': 0.0,\n",
        "                'Recall': 0.0,\n",
        "                'Training_Time_Minutes': 0.0,\n",
        "                'Status': f'Error: {str(e)[:100]}'\n",
        "            })\n",
        "\n",
        "    # Results analysis\n",
        "    print(f\"\\n\ud83c\udfc6 FULL DATASET FUSION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    successful_results = results_df[results_df['Status'] == 'Success']\n",
        "\n",
        "    if not successful_results.empty:\n",
        "        successful_results = successful_results.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "        print(\"\ud83e\udd47 FUSION RANKINGS:\")\n",
        "        for i, (_, row) in enumerate(successful_results.iterrows()):\n",
        "            rank = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\", \"4\ufe0f\u20e3\"][i]\n",
        "            print(f\"{rank} {row['Fusion_Strategy'].upper():15} F1: {row['F1_Score']:.4f} ({row['Training_Time_Minutes']:.1f}min)\")\n",
        "\n",
        "        best_fusion = successful_results.iloc[0]\n",
        "        bert_baseline = 0.9010\n",
        "\n",
        "        print(f\"\\n\ud83d\udcc8 BASELINE COMPARISON:\")\n",
        "        print(f\"BERT Baseline (90.10%): {bert_baseline:.4f}\")\n",
        "        print(f\"Best Fusion:            {best_fusion['F1_Score']:.4f}\")\n",
        "        improvement = best_fusion['F1_Score'] - bert_baseline\n",
        "        print(f\"Improvement:            {improvement:+.4f} ({improvement/bert_baseline*100:+.2f}%)\")\n",
        "\n",
        "        if improvement > 0.005:\n",
        "            print(\"\ud83d\ude80 FUSION SUCCESS! Significant improvement!\")\n",
        "        elif improvement > 0:\n",
        "            print(\"\u2705 FUSION BENEFICIAL! Modest improvement!\")\n",
        "        else:\n",
        "            print(\"\ud83e\udd14 More optimization needed!\")\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/BERT_ROBERTA_FUSION_FINAL_RESULTS.xlsx\", index=False)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\u23f1\ufe0f TOTAL TIME: {total_time/3600:.1f} hours\")\n",
        "    print(f\"\u2705 Results saved to Drive!\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# START FUSION COMPARISON\n",
        "print(\"\ud83d\udd25 STARTING FULL DATASET BERT + RoBERTa FUSION\")\n",
        "print(\"\ud83d\ude80 A100 GPU - Production Ready Testing\")\n",
        "print()\n",
        "\n",
        "fusion_results = run_full_fusion_comparison()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1664210,
          "status": "ok",
          "timestamp": 1755266098742,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "gcmfdAw03tbL",
        "outputId": "9c42f426-f698-4c68-f7c7-59c0abddae09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 ADVANCED BERT + RoBERTa FUSION - COMPREHENSIVE METRICS\n",
            "===========================================================================\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\udd25 STARTING ADVANCED FUSION WITH FULL METRICS CALCULATION\n",
            "\ud83d\udcca All metrics: F1, Accuracy, Precision, Recall (Macro & Weighted + Per-class)\n",
            "\ud83c\udfaf Target: Beat 90.10% F1 baseline\n",
            "\u23f0 Estimated time: 1-2 hours\n",
            "\n",
            "\ud83d\udcca LOADING FULL DATASET...\n",
            "\u2705 Dataset loaded: 15167 reviews\n",
            "\ud83d\udcca Class distribution: [6686 8481]\n",
            "\ud83d\udcca Train: 12891, Validation: 2276\n",
            "\n",
            "========== ADVANCED FUSION 1/4: ADVANCED_GATED ==========\n",
            "\ud83c\udfd7\ufe0f Building ADVANCED ADVANCED_GATED fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading models...\n",
            "\ud83d\udd13 Unfreezing last 2 layers...\n",
            "   \ud83d\udd13 BERT: 2/12 layers unfrozen\n",
            "   \ud83d\udd13 RoBERTa: 2/12 layers unfrozen\n",
            "\u2705 ADVANCED_GATED model built: 41,610,498 trainable parameters\n",
            "\ud83d\ude80 ADVANCED ADVANCED_GATED TRAINING\n",
            "\ud83d\udcca Train batches: 1612, Val batches: 143\n",
            "  Epoch 1/6:\n",
            "    Train Loss: 0.5313\n",
            "    Val Loss: 0.4037\n",
            "    F1: 0.8730, Acc: 0.8757\n",
            "    Precision: 0.8764, Recall: 0.8709\n",
            "    Time: 69.7s\n",
            "    \ud83c\udfc6 New best F1: 0.8730\n",
            "  Epoch 2/6:\n",
            "    Train Loss: 0.4322\n",
            "    Val Loss: 0.4094\n",
            "    F1: 0.8806, Acc: 0.8836\n",
            "    Precision: 0.8867, Recall: 0.8773\n",
            "    Time: 68.8s\n",
            "    \ud83c\udfc6 New best F1: 0.8806\n",
            "  Epoch 3/6:\n",
            "    Train Loss: 0.4081\n",
            "    Val Loss: 0.4049\n",
            "    F1: 0.8855, Acc: 0.8875\n",
            "    Precision: 0.8871, Recall: 0.8843\n",
            "    Time: 69.3s\n",
            "    \ud83c\udfc6 New best F1: 0.8855\n",
            "  Epoch 4/6:\n",
            "    Train Loss: 0.3936\n",
            "    Val Loss: 0.4012\n",
            "    F1: 0.8873, Acc: 0.8888\n",
            "    Precision: 0.8872, Recall: 0.8873\n",
            "    Time: 69.3s\n",
            "    \ud83c\udfc6 New best F1: 0.8873\n",
            "  Epoch 5/6:\n",
            "    Train Loss: 0.3832\n",
            "    Val Loss: 0.4004\n",
            "    F1: 0.8928, Acc: 0.8946\n",
            "    Precision: 0.8939, Recall: 0.8919\n",
            "    Time: 69.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8928\n",
            "  Epoch 6/6:\n",
            "    Train Loss: 0.3811\n",
            "    Val Loss: 0.3994\n",
            "    F1: 0.8945, Acc: 0.8963\n",
            "    Precision: 0.8957, Recall: 0.8936\n",
            "    Time: 69.1s\n",
            "    \ud83c\udfc6 New best F1: 0.8945\n",
            "\n",
            "\ud83d\udd2c ADVANCED_GATED FINAL EVALUATION...\n",
            "\u2705 ADVANCED_GATED COMPREHENSIVE RESULTS:\n",
            "   F1 Score: 0.8945\n",
            "   Accuracy: 0.8963\n",
            "   Precision: 0.8957\n",
            "   Recall: 0.8936\n",
            "   Baseline improvement: -0.0065\n",
            "   Beat baseline: \u274c No\n",
            "   Training time: 7.0 minutes\n",
            "\n",
            "========== ADVANCED FUSION 2/4: HIERARCHICAL ==========\n",
            "\ud83c\udfd7\ufe0f Building ADVANCED HIERARCHICAL fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading models...\n",
            "\ud83d\udd13 Unfreezing last 2 layers...\n",
            "   \ud83d\udd13 BERT: 2/12 layers unfrozen\n",
            "   \ud83d\udd13 RoBERTa: 2/12 layers unfrozen\n",
            "\u2705 HIERARCHICAL model built: 39,837,186 trainable parameters\n",
            "\ud83d\ude80 ADVANCED HIERARCHICAL TRAINING\n",
            "\ud83d\udcca Train batches: 1612, Val batches: 143\n",
            "  Epoch 1/6:\n",
            "    Train Loss: 0.5398\n",
            "    Val Loss: 0.3969\n",
            "    F1: 0.8749, Acc: 0.8770\n",
            "    Precision: 0.8760, Recall: 0.8740\n",
            "    Time: 65.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8749\n",
            "  Epoch 2/6:\n",
            "    Train Loss: 0.4332\n",
            "    Val Loss: 0.4393\n",
            "    F1: 0.8671, Acc: 0.8678\n",
            "    Precision: 0.8665, Recall: 0.8715\n",
            "    Time: 65.0s\n",
            "  Epoch 3/6:\n",
            "    Train Loss: 0.4118\n",
            "    Val Loss: 0.4028\n",
            "    F1: 0.8841, Acc: 0.8875\n",
            "    Precision: 0.8940, Recall: 0.8796\n",
            "    Time: 65.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8841\n",
            "  Epoch 4/6:\n",
            "    Train Loss: 0.3972\n",
            "    Val Loss: 0.3892\n",
            "    F1: 0.8903, Acc: 0.8919\n",
            "    Precision: 0.8906, Recall: 0.8899\n",
            "    Time: 65.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8903\n",
            "  Epoch 5/6:\n",
            "    Train Loss: 0.3862\n",
            "    Val Loss: 0.3864\n",
            "    F1: 0.8870, Acc: 0.8888\n",
            "    Precision: 0.8879, Recall: 0.8862\n",
            "    Time: 64.8s\n",
            "  Epoch 6/6:\n",
            "    Train Loss: 0.3850\n",
            "    Val Loss: 0.3881\n",
            "    F1: 0.8906, Acc: 0.8919\n",
            "    Precision: 0.8899, Recall: 0.8913\n",
            "    Time: 64.9s\n",
            "    \ud83c\udfc6 New best F1: 0.8906\n",
            "\n",
            "\ud83d\udd2c HIERARCHICAL FINAL EVALUATION...\n",
            "\u2705 HIERARCHICAL COMPREHENSIVE RESULTS:\n",
            "   F1 Score: 0.8906\n",
            "   Accuracy: 0.8919\n",
            "   Precision: 0.8899\n",
            "   Recall: 0.8913\n",
            "   Baseline improvement: -0.0104\n",
            "   Beat baseline: \u274c No\n",
            "   Training time: 6.6 minutes\n",
            "\n",
            "========== ADVANCED FUSION 3/4: CROSS_ATTENTION ==========\n",
            "\ud83c\udfd7\ufe0f Building ADVANCED CROSS_ATTENTION fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading models...\n",
            "\ud83d\udd13 Unfreezing last 2 layers...\n",
            "   \ud83d\udd13 BERT: 2/12 layers unfrozen\n",
            "   \ud83d\udd13 RoBERTa: 2/12 layers unfrozen\n",
            "\u2705 CROSS_ATTENTION model built: 46,332,930 trainable parameters\n",
            "\ud83d\ude80 ADVANCED CROSS_ATTENTION TRAINING\n",
            "\ud83d\udcca Train batches: 1612, Val batches: 143\n",
            "  Epoch 1/6:\n",
            "    Train Loss: 0.5478\n",
            "    Val Loss: 0.3962\n",
            "    F1: 0.8749, Acc: 0.8765\n",
            "    Precision: 0.8746, Recall: 0.8751\n",
            "    Time: 70.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8749\n",
            "  Epoch 2/6:\n",
            "    Train Loss: 0.4376\n",
            "    Val Loss: 0.3922\n",
            "    F1: 0.8882, Acc: 0.8902\n",
            "    Precision: 0.8897, Recall: 0.8871\n",
            "    Time: 69.5s\n",
            "    \ud83c\udfc6 New best F1: 0.8882\n",
            "  Epoch 3/6:\n",
            "    Train Loss: 0.4061\n",
            "    Val Loss: 0.4024\n",
            "    F1: 0.8830, Acc: 0.8840\n",
            "    Precision: 0.8817, Recall: 0.8853\n",
            "    Time: 69.6s\n",
            "  Epoch 4/6:\n",
            "    Train Loss: 0.3950\n",
            "    Val Loss: 0.3878\n",
            "    F1: 0.8891, Acc: 0.8906\n",
            "    Precision: 0.8888, Recall: 0.8894\n",
            "    Time: 69.5s\n",
            "    \ud83c\udfc6 New best F1: 0.8891\n",
            "  Epoch 5/6:\n",
            "    Train Loss: 0.3809\n",
            "    Val Loss: 0.3804\n",
            "    F1: 0.8898, Acc: 0.8915\n",
            "    Precision: 0.8902, Recall: 0.8895\n",
            "    Time: 69.5s\n",
            "    \ud83c\udfc6 New best F1: 0.8898\n",
            "  Epoch 6/6:\n",
            "    Train Loss: 0.3765\n",
            "    Val Loss: 0.3807\n",
            "    F1: 0.8914, Acc: 0.8932\n",
            "    Precision: 0.8927, Recall: 0.8904\n",
            "    Time: 69.5s\n",
            "    \ud83c\udfc6 New best F1: 0.8914\n",
            "\n",
            "\ud83d\udd2c CROSS_ATTENTION FINAL EVALUATION...\n",
            "\u2705 CROSS_ATTENTION COMPREHENSIVE RESULTS:\n",
            "   F1 Score: 0.8914\n",
            "   Accuracy: 0.8932\n",
            "   Precision: 0.8927\n",
            "   Recall: 0.8904\n",
            "   Baseline improvement: -0.0096\n",
            "   Beat baseline: \u274c No\n",
            "   Training time: 7.0 minutes\n",
            "\n",
            "========== ADVANCED FUSION 4/4: ADAPTIVE_WEIGHTED ==========\n",
            "\ud83c\udfd7\ufe0f Building ADVANCED ADAPTIVE_WEIGHTED fusion model...\n",
            "\ud83d\udce6 Loading tokenizers...\n",
            "\ud83e\udd16 Loading models...\n",
            "\ud83d\udd13 Unfreezing last 2 layers...\n",
            "   \ud83d\udd13 BERT: 2/12 layers unfrozen\n",
            "   \ud83d\udd13 RoBERTa: 2/12 layers unfrozen\n",
            "\u2705 ADAPTIVE_WEIGHTED model built: 39,575,048 trainable parameters\n",
            "\ud83d\ude80 ADVANCED ADAPTIVE_WEIGHTED TRAINING\n",
            "\ud83d\udcca Train batches: 1612, Val batches: 143\n",
            "  Epoch 1/6:\n",
            "    Train Loss: 0.5309\n",
            "    Val Loss: 0.3883\n",
            "    F1: 0.8770, Acc: 0.8796\n",
            "    Precision: 0.8809, Recall: 0.8745\n",
            "    Time: 67.3s\n",
            "    \ud83c\udfc6 New best F1: 0.8770\n",
            "  Epoch 2/6:\n",
            "    Train Loss: 0.4316\n",
            "    Val Loss: 0.3914\n",
            "    F1: 0.8828, Acc: 0.8858\n",
            "    Precision: 0.8893, Recall: 0.8794\n",
            "    Time: 67.1s\n",
            "    \ud83c\udfc6 New best F1: 0.8828\n",
            "  Epoch 3/6:\n",
            "    Train Loss: 0.4081\n",
            "    Val Loss: 0.3912\n",
            "    F1: 0.8891, Acc: 0.8919\n",
            "    Precision: 0.8957, Recall: 0.8856\n",
            "    Time: 67.3s\n",
            "    \ud83c\udfc6 New best F1: 0.8891\n",
            "  Epoch 4/6:\n",
            "    Train Loss: 0.3935\n",
            "    Val Loss: 0.4073\n",
            "    F1: 0.8903, Acc: 0.8919\n",
            "    Precision: 0.8906, Recall: 0.8899\n",
            "    Time: 67.0s\n",
            "    \ud83c\udfc6 New best F1: 0.8903\n",
            "  Epoch 5/6:\n",
            "    Train Loss: 0.3882\n",
            "    Val Loss: 0.4015\n",
            "    F1: 0.8901, Acc: 0.8919\n",
            "    Precision: 0.8911, Recall: 0.8893\n",
            "    Time: 67.0s\n",
            "  Epoch 6/6:\n",
            "    Train Loss: 0.3804\n",
            "    Val Loss: 0.4037\n",
            "    F1: 0.8905, Acc: 0.8924\n",
            "    Precision: 0.8917, Recall: 0.8896\n",
            "    Time: 67.2s\n",
            "    \ud83c\udfc6 New best F1: 0.8905\n",
            "\n",
            "\ud83d\udd2c ADAPTIVE_WEIGHTED FINAL EVALUATION...\n",
            "\u2705 ADAPTIVE_WEIGHTED COMPREHENSIVE RESULTS:\n",
            "   F1 Score: 0.8905\n",
            "   Accuracy: 0.8924\n",
            "   Precision: 0.8917\n",
            "   Recall: 0.8896\n",
            "   Baseline improvement: -0.0105\n",
            "   Beat baseline: \u274c No\n",
            "   Training time: 6.8 minutes\n",
            "\n",
            "\ud83c\udfc6 ADVANCED FUSION FINAL RESULTS\n",
            "================================================================================\n",
            "\ud83d\ude80 FUSION RANKINGS:\n",
            "----------------------------------------------------------------------\n",
            "\ud83e\udd47 ADVANCED_GATED       | \u274c Below baseline\n",
            "    F1: 0.8945 (-0.0065)\n",
            "    Accuracy: 0.8963\n",
            "    Precision: 0.8957\n",
            "    Recall: 0.8936\n",
            "    Time: 7.0 minutes\n",
            "\n",
            "\ud83e\udd48 CROSS_ATTENTION      | \u274c Below baseline\n",
            "    F1: 0.8914 (-0.0096)\n",
            "    Accuracy: 0.8932\n",
            "    Precision: 0.8927\n",
            "    Recall: 0.8904\n",
            "    Time: 7.0 minutes\n",
            "\n",
            "\ud83e\udd49 HIERARCHICAL         | \u274c Below baseline\n",
            "    F1: 0.8906 (-0.0104)\n",
            "    Accuracy: 0.8919\n",
            "    Precision: 0.8899\n",
            "    Recall: 0.8913\n",
            "    Time: 6.6 minutes\n",
            "\n",
            "4\ufe0f\u20e3 ADAPTIVE_WEIGHTED    | \u274c Below baseline\n",
            "    F1: 0.8905 (-0.0105)\n",
            "    Accuracy: 0.8924\n",
            "    Precision: 0.8917\n",
            "    Recall: 0.8896\n",
            "    Time: 6.8 minutes\n",
            "\n",
            "\ud83c\udfc6 BEST FUSION MODEL: ADVANCED_GATED\n",
            "============================================================\n",
            "\ud83d\udcca PERFORMANCE METRICS:\n",
            "   F1 Score: 0.8945\n",
            "   Accuracy: 0.8963\n",
            "   Precision: 0.8957\n",
            "   Recall: 0.8936\n",
            "\n",
            "\ud83d\udcc8 BASELINE COMPARISON:\n",
            "   BERT Baseline: 0.9010\n",
            "   Best Fusion: 0.8945\n",
            "   Improvement: -0.0065 (-0.72%)\n",
            "\n",
            "\ud83c\udfaf FUSION SUCCESS ANALYSIS:\n",
            "   Successful models: 4/4\n",
            "   Beat baseline: 0/4\n",
            "   Success rate: 0.0%\n",
            "   Average loss: -0.0092\n",
            "   \ud83e\udd14 Fusion needs further optimization\n",
            "\n",
            "\u23f1\ufe0f TRAINING EFFICIENCY:\n",
            "   Average training time: 6.8 minutes\n",
            "   Total experiment time: 0.5 hours\n",
            "\n",
            "\ud83c\udfd7\ufe0f ARCHITECTURE ANALYSIS:\n",
            "   advanced_gated       | F1: 0.8945 | Time: 7.0min | Efficiency: 0.128\n",
            "   cross_attention      | F1: 0.8914 | Time: 7.0min | Efficiency: 0.127\n",
            "   hierarchical         | F1: 0.8906 | Time: 6.6min | Efficiency: 0.136\n",
            "   adaptive_weighted    | F1: 0.8905 | Time: 6.8min | Efficiency: 0.131\n",
            "\n",
            "\ud83d\udcca FINAL COMPARISON TABLE:\n",
            "  Fusion_Strategy  F1_Score  Accuracy  Precision  Recall  F1_Improvement  Beat_Baseline  Training_Time_Minutes\n",
            "    BERT_Baseline    0.9010    0.9024     0.9012  0.9009          0.0000           True                 0.0000\n",
            "   advanced_gated    0.8945    0.8963     0.8957  0.8936         -0.0065          False                 6.9938\n",
            "  cross_attention    0.8914    0.8932     0.8927  0.8904         -0.0096          False                 7.0314\n",
            "     hierarchical    0.8906    0.8919     0.8899  0.8913         -0.0104          False                 6.5672\n",
            "adaptive_weighted    0.8905    0.8924     0.8917  0.8896         -0.0105          False                 6.7870\n",
            "\n",
            "\u23f1\ufe0f TOTAL EXPERIMENT TIME: 0.5 hours\n",
            "\u2705 Comprehensive results saved to Drive!\n",
            "\ud83d\udcc1 Files saved:\n",
            "   - ADVANCED_FUSION_COMPREHENSIVE_RESULTS.xlsx\n",
            "   - FUSION_vs_BASELINE_COMPARISON.xlsx\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "import time\n",
        "import gc\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\ud83d\udd25 ADVANCED BERT + RoBERTa FUSION - COMPREHENSIVE METRICS\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "\n",
        "CORRECT_FILE_PATH = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "class AdvancedGatedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.bert_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.roberta_gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.enhancement = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        bert_seq = bert_features.unsqueeze(1)\n",
        "        roberta_seq = roberta_features.unsqueeze(1)\n",
        "\n",
        "        bert_attended, _ = self.cross_attention(bert_seq, roberta_seq, roberta_seq)\n",
        "        roberta_attended, _ = self.cross_attention(roberta_seq, bert_seq, bert_seq)\n",
        "\n",
        "        bert_attended = bert_attended.squeeze(1)\n",
        "        roberta_attended = roberta_attended.squeeze(1)\n",
        "\n",
        "        concat_features = torch.cat([bert_attended, roberta_attended], dim=1)\n",
        "        bert_gate = self.bert_gate(concat_features)\n",
        "        roberta_gate = self.roberta_gate(concat_features)\n",
        "\n",
        "        gated_bert = bert_gate * bert_attended\n",
        "        gated_roberta = roberta_gate * roberta_attended\n",
        "\n",
        "        fused = gated_bert + gated_roberta\n",
        "        residual = (bert_features + roberta_features) / 2\n",
        "        fused = fused + residual * 0.2\n",
        "\n",
        "        enhanced = self.enhancement(fused)\n",
        "        return enhanced\n",
        "\n",
        "class HierarchicalFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.level1_fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.level2_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.level3_fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concat_features = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        level1_fused = self.level1_fusion(concat_features)\n",
        "\n",
        "        stacked_features = torch.stack([bert_features, roberta_features], dim=1)\n",
        "        level2_fused, _ = self.level2_attention(stacked_features, stacked_features, stacked_features)\n",
        "        level2_fused = level2_fused.mean(dim=1)\n",
        "\n",
        "        level3_input = torch.cat([level1_fused, level2_fused], dim=1)\n",
        "        level3_fused = self.level3_fusion(level3_input)\n",
        "\n",
        "        return level3_fused\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.bert_to_roberta = nn.MultiheadAttention(hidden_dim, num_heads=12, dropout=dropout, batch_first=True)\n",
        "        self.roberta_to_bert = nn.MultiheadAttention(hidden_dim, num_heads=12, dropout=dropout, batch_first=True)\n",
        "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        bert_seq = bert_features.unsqueeze(1)\n",
        "        roberta_seq = roberta_features.unsqueeze(1)\n",
        "\n",
        "        bert_cross, _ = self.bert_to_roberta(bert_seq, roberta_seq, roberta_seq)\n",
        "        roberta_cross, _ = self.roberta_to_bert(roberta_seq, bert_seq, bert_seq)\n",
        "\n",
        "        cross_fused = (bert_cross.squeeze(1) + roberta_cross.squeeze(1)) / 2\n",
        "\n",
        "        cross_fused_seq = cross_fused.unsqueeze(1)\n",
        "        self_attended, _ = self.self_attention(cross_fused_seq, cross_fused_seq, cross_fused_seq)\n",
        "        self_attended = self_attended.squeeze(1)\n",
        "\n",
        "        fused = self.layer_norm1(cross_fused + self_attended)\n",
        "        ffn_output = self.ffn(fused)\n",
        "        final_output = self.layer_norm2(fused + ffn_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "class AdaptiveWeightedFusion(nn.Module):\n",
        "    def __init__(self, hidden_dim=768, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim\n",
        "\n",
        "        self.weight_generator = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 6),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.concat_fusion = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.attention_fusion = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.enhancement = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        concat_input = torch.cat([bert_features, roberta_features], dim=1)\n",
        "        weights = self.weight_generator(concat_input)\n",
        "\n",
        "        bert_weighted = weights[:, 0:1] * bert_features\n",
        "        roberta_weighted = weights[:, 1:2] * roberta_features\n",
        "\n",
        "        concat_fused = self.concat_fusion(concat_input)\n",
        "        concat_weighted = weights[:, 2:3] * concat_fused\n",
        "\n",
        "        stacked = torch.stack([bert_features, roberta_features], dim=1)\n",
        "        attention_fused, _ = self.attention_fusion(stacked, stacked, stacked)\n",
        "        attention_fused = attention_fused.mean(dim=1)\n",
        "        attention_weighted = weights[:, 3:4] * attention_fused\n",
        "\n",
        "        residual = (bert_features + roberta_features) / 2\n",
        "        residual_weighted = weights[:, 4:5] * residual\n",
        "\n",
        "        bias_term = weights[:, 5:6] * torch.ones_like(bert_features)\n",
        "\n",
        "        adaptive_fused = (bert_weighted + roberta_weighted + concat_weighted +\n",
        "                         attention_weighted + residual_weighted + bias_term)\n",
        "\n",
        "        enhanced = self.enhancement(adaptive_fused)\n",
        "        return enhanced\n",
        "\n",
        "class AdvancedBertRobertaFusion(nn.Module):\n",
        "    def __init__(self, fusion_type='advanced_gated', max_length=128, dropout=0.3, unfreeze_layers=2):\n",
        "        super(AdvancedBertRobertaFusion, self).__init__()\n",
        "\n",
        "        print(f\"\ud83c\udfd7\ufe0f Building ADVANCED {fusion_type.upper()} fusion model...\")\n",
        "\n",
        "        self.bert_model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "        self.roberta_model_name = \"xlm-roberta-base\"\n",
        "        self.max_length = max_length\n",
        "        self.fusion_type = fusion_type\n",
        "        self.unfreeze_layers = unfreeze_layers\n",
        "\n",
        "        print(\"\ud83d\udce6 Loading tokenizers...\")\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_tokenizer = AutoTokenizer.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(\"\ud83e\udd16 Loading models...\")\n",
        "        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_model = AutoModel.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(f\"\ud83d\udd13 Unfreezing last {unfreeze_layers} layers...\")\n",
        "        self._freeze_models_selectively()\n",
        "\n",
        "        self.hidden_dim = 768\n",
        "\n",
        "        self.bert_projection = nn.Linear(2304, self.hidden_dim)\n",
        "        self.roberta_projection = nn.Linear(2304, self.hidden_dim)\n",
        "\n",
        "        if fusion_type == 'advanced_gated':\n",
        "            self.fusion_layer = AdvancedGatedFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'hierarchical':\n",
        "            self.fusion_layer = HierarchicalFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'cross_attention':\n",
        "            self.fusion_layer = CrossAttentionFusion(self.hidden_dim, dropout)\n",
        "        elif fusion_type == 'adaptive_weighted':\n",
        "            self.fusion_layer = AdaptiveWeightedFusion(self.hidden_dim, dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fusion_layer.output_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout/3),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"\u2705 {fusion_type.upper()} model built: {trainable_params:,} trainable parameters\")\n",
        "\n",
        "    def _freeze_models_selectively(self):\n",
        "        for param in self.bert_model.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.roberta_model.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        total_bert_layers = len(self.bert_model.encoder.layer)\n",
        "        unfrozen_bert = 0\n",
        "        for i, layer in enumerate(self.bert_model.encoder.layer):\n",
        "            if i < total_bert_layers - self.unfreeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "                unfrozen_bert += 1\n",
        "\n",
        "        total_roberta_layers = len(self.roberta_model.encoder.layer)\n",
        "        unfrozen_roberta = 0\n",
        "        for i, layer in enumerate(self.roberta_model.encoder.layer):\n",
        "            if i < total_roberta_layers - self.unfreeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "                unfrozen_roberta += 1\n",
        "\n",
        "        for param in self.bert_model.pooler.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.roberta_model.pooler.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        print(f\"   \ud83d\udd13 BERT: {unfrozen_bert}/{total_bert_layers} layers unfrozen\")\n",
        "        print(f\"   \ud83d\udd13 RoBERTa: {unfrozen_roberta}/{total_roberta_layers} layers unfrozen\")\n",
        "\n",
        "    def encode_batch_advanced(self, texts, model_type='bert'):\n",
        "        if model_type == 'bert':\n",
        "            tokenizer = self.bert_tokenizer\n",
        "            model = self.bert_model\n",
        "            projection = self.bert_projection\n",
        "        else:\n",
        "            tokenizer = self.roberta_tokenizer\n",
        "            model = self.roberta_model\n",
        "            projection = self.roberta_projection\n",
        "\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                          max_length=self.max_length, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "        mean_pooling = outputs.last_hidden_state.mean(dim=1)\n",
        "        max_pooling = outputs.last_hidden_state.max(dim=1)[0]\n",
        "\n",
        "        combined = torch.cat([cls_token, mean_pooling, max_pooling], dim=1)\n",
        "        projected = projection(combined)\n",
        "\n",
        "        return projected\n",
        "\n",
        "    def forward(self, texts):\n",
        "        bert_features = self.encode_batch_advanced(texts, 'bert')\n",
        "        roberta_features = self.encode_batch_advanced(texts, 'roberta')\n",
        "\n",
        "        fused_features = self.fusion_layer(bert_features, roberta_features)\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits\n",
        "\n",
        "def calculate_comprehensive_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    precision_macro, recall_macro, _, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "    precision_weighted, recall_weighted, _, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "\n",
        "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None\n",
        "    )\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'precision_macro': precision_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_macro': recall_macro,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'precision_class0': precision_per_class[0],\n",
        "        'precision_class1': precision_per_class[1],\n",
        "        'recall_class0': recall_per_class[0],\n",
        "        'recall_class1': recall_per_class[1],\n",
        "        'f1_class0': f1_per_class[0],\n",
        "        'f1_class1': f1_per_class[1]\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def train_advanced_fusion_full_metrics(model, train_dataset, val_dataset, epochs=6, batch_size=8):\n",
        "    model = model.to(device)\n",
        "\n",
        "    backbone_params = []\n",
        "    fusion_params = []\n",
        "    classifier_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if 'bert_model' in name or 'roberta_model' in name:\n",
        "                backbone_params.append(param)\n",
        "            elif 'fusion_layer' in name:\n",
        "                fusion_params.append(param)\n",
        "            else:\n",
        "                classifier_params.append(param)\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': backbone_params, 'lr': 1e-5, 'weight_decay': 0.01},\n",
        "        {'params': fusion_params, 'lr': 2e-5, 'weight_decay': 0.01},\n",
        "        {'params': classifier_params, 'lr': 3e-5, 'weight_decay': 0.01}\n",
        "    ])\n",
        "\n",
        "    total_steps = len(DataLoader(train_dataset, batch_size=batch_size)) * epochs\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=[1e-5, 2e-5, 3e-5],\n",
        "        total_steps=total_steps,\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"\ud83d\ude80 ADVANCED {model.fusion_type.upper()} TRAINING\")\n",
        "    print(f\"\ud83d\udcca Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_metrics = {}\n",
        "    patience = 2\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_texts, batch_labels in train_loader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_texts)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "            if train_batches % 200 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_texts, batch_labels in val_loader:\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                logits = model(batch_texts)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                val_predictions.extend(preds)\n",
        "                val_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        epoch_metrics = calculate_comprehensive_metrics(val_true_labels, val_predictions)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"    Train Loss: {train_loss/train_batches:.4f}\")\n",
        "        print(f\"    Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "        print(f\"    F1: {epoch_metrics['f1_macro']:.4f}, Acc: {epoch_metrics['accuracy']:.4f}\")\n",
        "        print(f\"    Precision: {epoch_metrics['precision_macro']:.4f}, Recall: {epoch_metrics['recall_macro']:.4f}\")\n",
        "        print(f\"    Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        if epoch_metrics['f1_macro'] > best_f1:\n",
        "            best_f1 = epoch_metrics['f1_macro']\n",
        "            best_metrics = epoch_metrics.copy()\n",
        "            patience_counter = 0\n",
        "\n",
        "            print(f\"    \ud83c\udfc6 New best F1: {best_f1:.4f}\")\n",
        "\n",
        "            if best_f1 > 0.901:\n",
        "                print(f\"    \ud83c\udf89 BASELINE BEATEN! {best_f1:.4f} > 90.10%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience and epoch >= 3:\n",
        "            print(f\"    \ud83d\uded1 Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model, best_f1, best_metrics\n",
        "\n",
        "def run_advanced_fusion_full_metrics():\n",
        "    print(\"\ud83d\udcca LOADING FULL DATASET...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = pd.read_excel(CORRECT_FILE_PATH)\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "    texts = df_clean['metin'].astype(str).tolist()\n",
        "    labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "    print(f\"\u2705 Dataset loaded: {len(texts)} reviews\")\n",
        "    print(f\"\ud83d\udcca Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Validation: {len(val_texts)}\")\n",
        "\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels)\n",
        "\n",
        "    advanced_strategies = [\n",
        "        'advanced_gated',\n",
        "        'hierarchical',\n",
        "        'cross_attention',\n",
        "        'adaptive_weighted'\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    baseline_f1 = 0.9010\n",
        "\n",
        "    for i, strategy in enumerate(advanced_strategies):\n",
        "        print(f\"\\n{'='*10} ADVANCED FUSION {i+1}/4: {strategy.upper()} {'='*10}\")\n",
        "\n",
        "        try:\n",
        "            strategy_start = time.time()\n",
        "\n",
        "            model = AdvancedBertRobertaFusion(\n",
        "                fusion_type=strategy,\n",
        "                max_length=128,\n",
        "                dropout=0.3,\n",
        "                unfreeze_layers=2\n",
        "            )\n",
        "\n",
        "            trained_model, best_f1, best_metrics = train_advanced_fusion_full_metrics(\n",
        "                model, train_dataset, val_dataset,\n",
        "                epochs=6, batch_size=8\n",
        "            )\n",
        "\n",
        "            strategy_time = time.time() - strategy_start\n",
        "\n",
        "            print(f\"\\n\ud83d\udd2c {strategy.upper()} FINAL EVALUATION...\")\n",
        "            trained_model.eval()\n",
        "\n",
        "            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "            final_predictions = []\n",
        "            final_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_texts, batch_labels in val_loader:\n",
        "                    logits = trained_model(batch_texts)\n",
        "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                    final_predictions.extend(preds)\n",
        "                    final_true_labels.extend(batch_labels.numpy())\n",
        "\n",
        "            final_metrics = calculate_comprehensive_metrics(final_true_labels, final_predictions)\n",
        "\n",
        "            f1_improvement = final_metrics['f1_macro'] - baseline_f1\n",
        "            beat_baseline = final_metrics['f1_macro'] > baseline_f1\n",
        "\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': final_metrics['f1_macro'],\n",
        "                'Accuracy': final_metrics['accuracy'],\n",
        "                'Precision': final_metrics['precision_macro'],\n",
        "                'Recall': final_metrics['recall_macro'],\n",
        "                'F1_Weighted': final_metrics['f1_weighted'],\n",
        "                'Training_Time_Minutes': strategy_time / 60,\n",
        "                'F1_Improvement': f1_improvement,\n",
        "                'Beat_Baseline': beat_baseline,\n",
        "                'Status': 'Success'\n",
        "            })\n",
        "\n",
        "            print(f\"\u2705 {strategy.upper()} COMPREHENSIVE RESULTS:\")\n",
        "            print(f\"   F1 Score: {final_metrics['f1_macro']:.4f}\")\n",
        "            print(f\"   Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "            print(f\"   Precision: {final_metrics['precision_macro']:.4f}\")\n",
        "            print(f\"   Recall: {final_metrics['recall_macro']:.4f}\")\n",
        "            print(f\"   Baseline improvement: {f1_improvement:+.4f}\")\n",
        "            print(f\"   Beat baseline: {'\ud83c\udf89 YES!' if beat_baseline else '\u274c No'}\")\n",
        "            print(f\"   Training time: {strategy_time/60:.1f} minutes\")\n",
        "\n",
        "            del model, trained_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c {strategy.upper()} FAILED: {str(e)}\")\n",
        "            results.append({\n",
        "                'Fusion_Strategy': strategy,\n",
        "                'F1_Score': 0.0,\n",
        "                'Accuracy': 0.0,\n",
        "                'Precision': 0.0,\n",
        "                'Recall': 0.0,\n",
        "                'F1_Weighted': 0.0,\n",
        "                'Training_Time_Minutes': 0.0,\n",
        "                'F1_Improvement': -baseline_f1,\n",
        "                'Beat_Baseline': False,\n",
        "                'Status': f'Error: {str(e)[:100]}'\n",
        "            })\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 ADVANCED FUSION FINAL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    successful_results = results_df[results_df['Status'] == 'Success']\n",
        "\n",
        "    if not successful_results.empty:\n",
        "        successful_results = successful_results.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "        print(\"\ud83d\ude80 FUSION RANKINGS:\")\n",
        "        print(\"-\" * 70)\n",
        "        for i, (_, row) in enumerate(successful_results.iterrows()):\n",
        "            rank = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\", \"4\ufe0f\u20e3\"][i] if i < 4 else f\"{i+1}\ufe0f\u20e3\"\n",
        "            baseline_status = \"\ud83c\udf89 BEATS BASELINE!\" if row['Beat_Baseline'] else \"\u274c Below baseline\"\n",
        "\n",
        "            print(f\"{rank} {row['Fusion_Strategy'].upper():20} | {baseline_status}\")\n",
        "            print(f\"    F1: {row['F1_Score']:.4f} ({row['F1_Improvement']:+.4f})\")\n",
        "            print(f\"    Accuracy: {row['Accuracy']:.4f}\")\n",
        "            print(f\"    Precision: {row['Precision']:.4f}\")\n",
        "            print(f\"    Recall: {row['Recall']:.4f}\")\n",
        "            print(f\"    Time: {row['Training_Time_Minutes']:.1f} minutes\")\n",
        "            print()\n",
        "\n",
        "        best_fusion = successful_results.iloc[0]\n",
        "\n",
        "        print(f\"\ud83c\udfc6 BEST FUSION MODEL: {best_fusion['Fusion_Strategy'].upper()}\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\ud83d\udcca PERFORMANCE METRICS:\")\n",
        "        print(f\"   F1 Score: {best_fusion['F1_Score']:.4f}\")\n",
        "        print(f\"   Accuracy: {best_fusion['Accuracy']:.4f}\")\n",
        "        print(f\"   Precision: {best_fusion['Precision']:.4f}\")\n",
        "        print(f\"   Recall: {best_fusion['Recall']:.4f}\")\n",
        "\n",
        "        print(f\"\\n\ud83d\udcc8 BASELINE COMPARISON:\")\n",
        "        print(f\"   BERT Baseline: {baseline_f1:.4f}\")\n",
        "        print(f\"   Best Fusion: {best_fusion['F1_Score']:.4f}\")\n",
        "        print(f\"   Improvement: {best_fusion['F1_Improvement']:+.4f} ({best_fusion['F1_Improvement']/baseline_f1*100:+.2f}%)\")\n",
        "\n",
        "        beat_baseline_count = successful_results['Beat_Baseline'].sum()\n",
        "        total_successful = len(successful_results)\n",
        "\n",
        "        print(f\"\\n\ud83c\udfaf FUSION SUCCESS ANALYSIS:\")\n",
        "        print(f\"   Successful models: {total_successful}/4\")\n",
        "        print(f\"   Beat baseline: {beat_baseline_count}/{total_successful}\")\n",
        "        print(f\"   Success rate: {beat_baseline_count/total_successful*100:.1f}%\")\n",
        "\n",
        "        if beat_baseline_count > 0:\n",
        "            avg_improvement = successful_results[successful_results['Beat_Baseline']]['F1_Improvement'].mean()\n",
        "            print(f\"   Average improvement: {avg_improvement:+.4f}\")\n",
        "            print(f\"   \ud83d\ude80 FUSION BREAKTHROUGH ACHIEVED!\")\n",
        "        else:\n",
        "            avg_loss = successful_results['F1_Improvement'].mean()\n",
        "            print(f\"   Average loss: {avg_loss:.4f}\")\n",
        "            print(f\"   \ud83e\udd14 Fusion needs further optimization\")\n",
        "\n",
        "        avg_time = successful_results['Training_Time_Minutes'].mean()\n",
        "        print(f\"\\n\u23f1\ufe0f TRAINING EFFICIENCY:\")\n",
        "        print(f\"   Average training time: {avg_time:.1f} minutes\")\n",
        "        print(f\"   Total experiment time: {(time.time() - start_time)/3600:.1f} hours\")\n",
        "\n",
        "        print(f\"\\n\ud83c\udfd7\ufe0f ARCHITECTURE ANALYSIS:\")\n",
        "        for _, row in successful_results.iterrows():\n",
        "            strategy = row['Fusion_Strategy']\n",
        "            f1 = row['F1_Score']\n",
        "            time_mins = row['Training_Time_Minutes']\n",
        "            efficiency = f1 / time_mins if time_mins > 0 else 0\n",
        "\n",
        "            print(f\"   {strategy:20} | F1: {f1:.4f} | Time: {time_mins:.1f}min | Efficiency: {efficiency:.3f}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\u274c NO SUCCESSFUL FUSION MODELS!\")\n",
        "        print(\"\ud83d\udd27 Need to debug and optimize fusion architectures\")\n",
        "\n",
        "    # Save results\n",
        "    results_df.to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ADVANCED_FUSION_COMPREHENSIVE_RESULTS.xlsx\", index=False)\n",
        "\n",
        "    # Create comparison table\n",
        "    if not successful_results.empty:\n",
        "        comparison_table = successful_results[['Fusion_Strategy', 'F1_Score', 'Accuracy', 'Precision', 'Recall',\n",
        "                                            'F1_Improvement', 'Beat_Baseline', 'Training_Time_Minutes']].copy()\n",
        "\n",
        "        # Add baseline row\n",
        "        baseline_row = {\n",
        "            'Fusion_Strategy': 'BERT_Baseline',\n",
        "            'F1_Score': baseline_f1,\n",
        "            'Accuracy': 0.9024,\n",
        "            'Precision': 0.9012,\n",
        "            'Recall': 0.9009,\n",
        "            'F1_Improvement': 0.0,\n",
        "            'Beat_Baseline': True,\n",
        "            'Training_Time_Minutes': 0.0\n",
        "        }\n",
        "\n",
        "        comparison_table = pd.concat([pd.DataFrame([baseline_row]), comparison_table], ignore_index=True)\n",
        "        comparison_table.to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/FUSION_vs_BASELINE_COMPARISON.xlsx\", index=False)\n",
        "\n",
        "        print(f\"\\n\ud83d\udcca FINAL COMPARISON TABLE:\")\n",
        "        print(comparison_table.round(4).to_string(index=False))\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\u23f1\ufe0f TOTAL EXPERIMENT TIME: {total_time/3600:.1f} hours\")\n",
        "    print(f\"\u2705 Comprehensive results saved to Drive!\")\n",
        "    print(f\"\ud83d\udcc1 Files saved:\")\n",
        "    print(f\"   - ADVANCED_FUSION_COMPREHENSIVE_RESULTS.xlsx\")\n",
        "    print(f\"   - FUSION_vs_BASELINE_COMPARISON.xlsx\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# \ud83d\ude80 START ADVANCED FUSION WITH COMPREHENSIVE METRICS\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83d\udd25 STARTING ADVANCED FUSION WITH FULL METRICS CALCULATION\")\n",
        "    print(\"\ud83d\udcca All metrics: F1, Accuracy, Precision, Recall (Macro & Weighted + Per-class)\")\n",
        "    print(\"\ud83c\udfaf Target: Beat 90.10% F1 baseline\")\n",
        "    print(\"\u23f0 Estimated time: 1-2 hours\")\n",
        "    print()\n",
        "\n",
        "    fusion_results = run_advanced_fusion_full_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PLDrmVbtb-fa",
        "outputId": "0f43eea9-cf01-48b6-f9c8-fd7697de8c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 OPTIMIZED BERT + RoBERTa FUSION - TARGET: BEAT 90.10%\n",
            "===========================================================================\n",
            "\ud83d\udda5\ufe0f Device: cpu\n",
            "\ud83d\udd25 STARTING OPTIMIZED FUSION EXPERIMENT\n",
            "\ud83c\udfaf Target: Beat 90.10% F1 baseline\n",
            "\ud83d\ude80 Strategy: Better architecture + training optimization\n",
            "\n",
            "\ud83d\udcca LOADING DATASET...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1865165757.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_optimized_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1865165757.py\u001b[0m in \u001b[0;36mrun_optimized_fusion\u001b[0;34m()\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCORRECT_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'etiket'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "import time\n",
        "import gc\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"\ud83d\udd25 OPTIMIZED BERT + RoBERTa FUSION - TARGET: BEAT 90.10%\")\n",
        "print(\"=\"*75)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "\n",
        "CORRECT_FILE_PATH = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "class ImprovedGatedFusion(nn.Module):\n",
        "    \"\"\"Optimized fusion with better architecture\"\"\"\n",
        "    def __init__(self, hidden_dim=768, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = hidden_dim * 2  # Expanded output\n",
        "\n",
        "        # Enhanced gating with residual connections\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 2),  # Binary gate for each model\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Feature enhancement\n",
        "        self.bert_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "        self.roberta_enhance = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "        # Cross-interaction\n",
        "        self.cross_layer = nn.MultiheadAttention(hidden_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, bert_features, roberta_features):\n",
        "        # Enhance individual features\n",
        "        bert_enhanced = self.bert_enhance(bert_features)\n",
        "        roberta_enhanced = self.roberta_enhance(roberta_features)\n",
        "\n",
        "        # Cross attention\n",
        "        bert_seq = bert_enhanced.unsqueeze(1)\n",
        "        roberta_seq = roberta_enhanced.unsqueeze(1)\n",
        "\n",
        "        bert_cross, _ = self.cross_layer(bert_seq, roberta_seq, roberta_seq)\n",
        "        roberta_cross, _ = self.cross_layer(roberta_seq, bert_seq, bert_seq)\n",
        "\n",
        "        bert_final = bert_cross.squeeze(1) + bert_enhanced\n",
        "        roberta_final = roberta_cross.squeeze(1) + roberta_enhanced\n",
        "\n",
        "        # Adaptive gating\n",
        "        concat_features = torch.cat([bert_final, roberta_final], dim=1)\n",
        "        gates = self.gate(concat_features)\n",
        "\n",
        "        # Weighted combination\n",
        "        bert_weighted = gates[:, 0:1] * bert_final\n",
        "        roberta_weighted = gates[:, 1:2] * roberta_final\n",
        "\n",
        "        # Concatenate instead of add for richer representation\n",
        "        fused = torch.cat([bert_weighted, roberta_weighted], dim=1)\n",
        "\n",
        "        return fused\n",
        "\n",
        "class OptimizedBertRobertaFusion(nn.Module):\n",
        "    def __init__(self, max_length=128, dropout=0.2, unfreeze_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"\ud83c\udfd7\ufe0f Building OPTIMIZED fusion model...\")\n",
        "\n",
        "        self.bert_model_name = \"dbmdz/bert-base-turkish-cased\"\n",
        "        self.roberta_model_name = \"xlm-roberta-base\"\n",
        "        self.max_length = max_length\n",
        "        self.unfreeze_layers = unfreeze_layers\n",
        "\n",
        "        print(\"\ud83d\udce6 Loading tokenizers...\")\n",
        "        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_tokenizer = AutoTokenizer.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(\"\ud83e\udd16 Loading models...\")\n",
        "        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)\n",
        "        self.roberta_model = AutoModel.from_pretrained(self.roberta_model_name)\n",
        "\n",
        "        print(f\"\ud83d\udd13 Unfreezing last {unfreeze_layers} layers...\")\n",
        "        self._freeze_models_selectively()\n",
        "\n",
        "        self.hidden_dim = 768\n",
        "\n",
        "        # Better pooling strategies\n",
        "        self.bert_pooler = nn.Sequential(\n",
        "            nn.Linear(768, 768),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "        self.roberta_pooler = nn.Sequential(\n",
        "            nn.Linear(768, 768),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout/2)\n",
        "        )\n",
        "\n",
        "        # Optimized fusion\n",
        "        self.fusion_layer = ImprovedGatedFusion(self.hidden_dim, dropout)\n",
        "\n",
        "        # Enhanced classifier with regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.fusion_layer.output_dim, 1024),\n",
        "            nn.LayerNorm(1024),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LayerNorm(512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/2),\n",
        "\n",
        "            nn.Linear(512, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout/3),\n",
        "\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"\u2705 Model built: {trainable_params:,} trainable parameters\")\n",
        "\n",
        "    def _freeze_models_selectively(self):\n",
        "        # Freeze embeddings\n",
        "        for param in self.bert_model.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.roberta_model.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # BERT unfreezing - more layers\n",
        "        total_bert_layers = len(self.bert_model.encoder.layer)\n",
        "        for i, layer in enumerate(self.bert_model.encoder.layer):\n",
        "            if i < total_bert_layers - self.unfreeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # RoBERTa unfreezing - more layers\n",
        "        total_roberta_layers = len(self.roberta_model.encoder.layer)\n",
        "        for i, layer in enumerate(self.roberta_model.encoder.layer):\n",
        "            if i < total_roberta_layers - self.unfreeze_layers:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "        # Keep poolers trainable\n",
        "        for param in self.bert_model.pooler.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.roberta_model.pooler.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        print(f\"   \ud83d\udd13 BERT: {self.unfreeze_layers}/{total_bert_layers} layers unfrozen\")\n",
        "        print(f\"   \ud83d\udd13 RoBERTa: {self.unfreeze_layers}/{total_roberta_layers} layers unfrozen\")\n",
        "\n",
        "    def encode_batch_improved(self, texts, model_type='bert'):\n",
        "        if model_type == 'bert':\n",
        "            tokenizer = self.bert_tokenizer\n",
        "            model = self.bert_model\n",
        "            pooler = self.bert_pooler\n",
        "        else:\n",
        "            tokenizer = self.roberta_tokenizer\n",
        "            model = self.roberta_model\n",
        "            pooler = self.roberta_pooler\n",
        "\n",
        "        inputs = tokenizer(texts, padding=True, truncation=True,\n",
        "                          max_length=self.max_length, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Better pooling: weighted combination of [CLS] and mean pooling\n",
        "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "        mean_pooling = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Weighted combination (learnable weights would be even better)\n",
        "        combined = 0.7 * cls_token + 0.3 * mean_pooling\n",
        "\n",
        "        # Apply custom pooler\n",
        "        pooled = pooler(combined)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, texts):\n",
        "        bert_features = self.encode_batch_improved(texts, 'bert')\n",
        "        roberta_features = self.encode_batch_improved(texts, 'roberta')\n",
        "\n",
        "        fused_features = self.fusion_layer(bert_features, roberta_features)\n",
        "        logits = self.classifier(fused_features)\n",
        "        return logits\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    precision_macro, recall_macro, _, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro\n",
        "    }\n",
        "\n",
        "def train_optimized_fusion(model, train_dataset, val_dataset, epochs=8, batch_size=8):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Improved optimizer setup with different learning rates\n",
        "    backbone_params = []\n",
        "    fusion_params = []\n",
        "    classifier_params = []\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if 'bert_model' in name or 'roberta_model' in name:\n",
        "                backbone_params.append(param)\n",
        "            elif 'fusion_layer' in name or 'pooler' in name:\n",
        "                fusion_params.append(param)\n",
        "            else:\n",
        "                classifier_params.append(param)\n",
        "\n",
        "    # Higher learning rates for new components\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': backbone_params, 'lr': 1e-5, 'weight_decay': 0.01},\n",
        "        {'params': fusion_params, 'lr': 3e-5, 'weight_decay': 0.01},\n",
        "        {'params': classifier_params, 'lr': 5e-5, 'weight_decay': 0.01}\n",
        "    ])\n",
        "\n",
        "    # Cosine annealing with warm restarts\n",
        "    total_steps = len(DataLoader(train_dataset, batch_size=batch_size)) * epochs\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=total_steps // 4,\n",
        "        T_mult=1,\n",
        "        eta_min=1e-6\n",
        "    )\n",
        "\n",
        "    # Focal loss for better handling of class imbalance\n",
        "    class FocalLoss(nn.Module):\n",
        "        def __init__(self, alpha=1, gamma=2):\n",
        "            super().__init__()\n",
        "            self.alpha = alpha\n",
        "            self.gamma = gamma\n",
        "\n",
        "        def forward(self, inputs, targets):\n",
        "            ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "            pt = torch.exp(-ce_loss)\n",
        "            focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "            return focal_loss.mean()\n",
        "\n",
        "    criterion = FocalLoss(alpha=1, gamma=1.5)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    print(f\"\ud83d\ude80 OPTIMIZED TRAINING\")\n",
        "    print(f\"\ud83d\udcca Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_metrics = {}\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_texts, batch_labels in train_loader:\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_texts)\n",
        "            loss = criterion(logits, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "            if train_batches % 200 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_predictions = []\n",
        "        val_true_labels = []\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_texts, batch_labels in val_loader:\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                logits = model(batch_texts)\n",
        "                loss = criterion(logits, batch_labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                val_predictions.extend(preds)\n",
        "                val_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "        epoch_metrics = calculate_metrics(val_true_labels, val_predictions)\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"    Train Loss: {train_loss/train_batches:.4f}\")\n",
        "        print(f\"    Val Loss: {val_loss/len(val_loader):.4f}\")\n",
        "        print(f\"    F1: {epoch_metrics['f1_macro']:.4f}, Acc: {epoch_metrics['accuracy']:.4f}\")\n",
        "        print(f\"    Time: {epoch_time:.1f}s\")\n",
        "\n",
        "        if epoch_metrics['f1_macro'] > best_f1:\n",
        "            best_f1 = epoch_metrics['f1_macro']\n",
        "            best_metrics = epoch_metrics.copy()\n",
        "            patience_counter = 0\n",
        "\n",
        "            print(f\"    \ud83c\udfc6 New best F1: {best_f1:.4f}\")\n",
        "\n",
        "            if best_f1 > 0.901:\n",
        "                print(f\"    \ud83c\udf89 BASELINE BEATEN! {best_f1:.4f} > 90.10%\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience and epoch >= 4:\n",
        "            print(f\"    \ud83d\uded1 Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model, best_f1, best_metrics\n",
        "\n",
        "def run_optimized_fusion():\n",
        "    print(\"\ud83d\udcca LOADING DATASET...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    df = pd.read_excel(CORRECT_FILE_PATH)\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "    texts = df_clean['metin'].astype(str).tolist()\n",
        "    labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "    print(f\"\u2705 Dataset loaded: {len(texts)} reviews\")\n",
        "    print(f\"\ud83d\udcca Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Stratified split with larger validation set for better evaluation\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Validation: {len(val_texts)}\")\n",
        "\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels)\n",
        "\n",
        "    # Test multiple configurations\n",
        "    configs = [\n",
        "        {'dropout': 0.15, 'unfreeze_layers': 3, 'batch_size': 8, 'epochs': 8},\n",
        "        {'dropout': 0.2, 'unfreeze_layers': 4, 'batch_size': 8, 'epochs': 8},\n",
        "        {'dropout': 0.1, 'unfreeze_layers': 3, 'batch_size': 6, 'epochs': 10}\n",
        "    ]\n",
        "\n",
        "    best_overall_f1 = 0\n",
        "    best_config = None\n",
        "    results = []\n",
        "    baseline_f1 = 0.9010\n",
        "\n",
        "    for i, config in enumerate(configs):\n",
        "        print(f\"\\n{'='*20} CONFIG {i+1}/3 {'='*20}\")\n",
        "        print(f\"Config: {config}\")\n",
        "\n",
        "        try:\n",
        "            model = OptimizedBertRobertaFusion(\n",
        "                max_length=128,\n",
        "                dropout=config['dropout'],\n",
        "                unfreeze_layers=config['unfreeze_layers']\n",
        "            )\n",
        "\n",
        "            trained_model, best_f1, best_metrics = train_optimized_fusion(\n",
        "                model, train_dataset, val_dataset,\n",
        "                epochs=config['epochs'],\n",
        "                batch_size=config['batch_size']\n",
        "            )\n",
        "\n",
        "            # Final evaluation\n",
        "            trained_model.eval()\n",
        "            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
        "            final_predictions = []\n",
        "            final_true_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_texts, batch_labels in val_loader:\n",
        "                    logits = trained_model(batch_texts)\n",
        "                    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "                    final_predictions.extend(preds)\n",
        "                    final_true_labels.extend(batch_labels.numpy())\n",
        "\n",
        "            final_metrics = calculate_metrics(final_true_labels, final_predictions)\n",
        "\n",
        "            f1_improvement = final_metrics['f1_macro'] - baseline_f1\n",
        "            beat_baseline = final_metrics['f1_macro'] > baseline_f1\n",
        "\n",
        "            results.append({\n",
        "                'Config': f\"Config_{i+1}\",\n",
        "                'F1_Score': final_metrics['f1_macro'],\n",
        "                'Accuracy': final_metrics['accuracy'],\n",
        "                'Precision': final_metrics['precision_macro'],\n",
        "                'Recall': final_metrics['recall_macro'],\n",
        "                'F1_Improvement': f1_improvement,\n",
        "                'Beat_Baseline': beat_baseline,\n",
        "                'Parameters': config\n",
        "            })\n",
        "\n",
        "            print(f\"\u2705 CONFIG {i+1} RESULTS:\")\n",
        "            print(f\"   F1 Score: {final_metrics['f1_macro']:.4f}\")\n",
        "            print(f\"   Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "            print(f\"   Baseline improvement: {f1_improvement:+.4f}\")\n",
        "            print(f\"   Beat baseline: {'\ud83c\udf89 YES!' if beat_baseline else '\u274c No'}\")\n",
        "\n",
        "            if final_metrics['f1_macro'] > best_overall_f1:\n",
        "                best_overall_f1 = final_metrics['f1_macro']\n",
        "                best_config = config\n",
        "\n",
        "            del model, trained_model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c CONFIG {i+1} FAILED: {str(e)}\")\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 OPTIMIZATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    if not results_df.empty:\n",
        "        results_df = results_df.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "        print(\"\ud83d\ude80 CONFIGURATION RANKINGS:\")\n",
        "        for i, (_, row) in enumerate(results_df.iterrows()):\n",
        "            rank = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\"][i] if i < 3 else f\"{i+1}\ufe0f\u20e3\"\n",
        "            status = \"\ud83c\udf89 BEATS BASELINE!\" if row['Beat_Baseline'] else \"\u274c Below baseline\"\n",
        "\n",
        "            print(f\"{rank} {row['Config']:15} | {status}\")\n",
        "            print(f\"    F1: {row['F1_Score']:.4f} ({row['F1_Improvement']:+.4f})\")\n",
        "            print(f\"    Accuracy: {row['Accuracy']:.4f}\")\n",
        "            print()\n",
        "\n",
        "        best_result = results_df.iloc[0]\n",
        "\n",
        "        if best_result['Beat_Baseline']:\n",
        "            print(f\"\ud83c\udf89 SUCCESS! Best F1: {best_result['F1_Score']:.4f}\")\n",
        "            print(f\"\ud83d\udcaa Improvement: {best_result['F1_Improvement']:+.4f}\")\n",
        "        else:\n",
        "            print(f\"\ud83e\udd14 Still need optimization. Best F1: {best_result['F1_Score']:.4f}\")\n",
        "\n",
        "        # Save results\n",
        "        results_df.to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/OPTIMIZED_FUSION_RESULTS.xlsx\", index=False)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\u23f1\ufe0f TOTAL TIME: {total_time/60:.1f} minutes\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Run optimization\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\ud83d\udd25 STARTING OPTIMIZED FUSION EXPERIMENT\")\n",
        "    print(\"\ud83c\udfaf Target: Beat 90.10% F1 baseline\")\n",
        "    print(\"\ud83d\ude80 Strategy: Better architecture + training optimization\")\n",
        "    print()\n",
        "\n",
        "    results = run_optimized_fusion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "debc3b67d40c47dabee9a9779696423e",
            "944ec690d556437e8f4165cfe67413ce",
            "c85f7a67931247629c3f1abe948bc355",
            "b3ae95dfbf9344db94b88ee92bfb0db4",
            "7a4e94ab959c4fdd89c8411c504c7002",
            "d73a1f7307b94af7bec0180ede6bdd31",
            "8451d8a13a8346f09df5bff99e5f8360",
            "88775922b3854f9eb7bd69a2aa3cdf68",
            "48490bb59e454761a7971500445eaa8e",
            "36c3b94c9e924a54a560a3b84aac0941",
            "f4689317b2b343488ba3bfd7bf951ae5",
            "d74abd20640240d1866e67474b67eebd",
            "fdd4b8c1c5f24597a17ecb68ace9ffef",
            "03b82f7f87774c19b8bf89f723f4733f",
            "e6cf7704d09944ec96e5a635ead0d156",
            "8fa27f3bce6648ba9fd8c8ca01932cba",
            "77a0d079bf7e48bab69a5c3617cc7965",
            "5390768e57034088ae2c4196aca23e78",
            "692ad2c78a6342d98ca1e83e2bca8b9d",
            "4ef63a3376e1460aaafbaa29cbc243ec",
            "564b3f42e1734cc3bdd078054a74e912",
            "96e13a3f7996442287a1f08ed633e9e1",
            "c90ebd721da04037bbdc060815be4415",
            "f55ae7ecfbec4b6690692443974e2417",
            "ee6cd824d9784ac2a0ca8b06d04e4d1b",
            "a04111d2d37d4924899024f4797c0ac4",
            "f79e0dfe9cc2407995e5b82242cd1797",
            "c5dc726f0d834db682cc4d6a520b466e",
            "50bed70ae8bc4e8c8760e51be6503bab",
            "89aa633d86f24688949b72ff818032e8",
            "399e71fe162f4ac7a48e604ac66e2425",
            "0606fa9a936e40f0ab239f3478f02c84",
            "161c1957c6da40579225d7e4bbef171f",
            "a24491507615478380cfd7762bac2ec8",
            "9cff76c09ce94e4daf31a2cd2467deb6",
            "64007bfa216e4cfb824ef7cbb3a16b75",
            "63ec81cc6e8a47e59a5fd3b498960d92",
            "46ef9b7d1223477dab910d26738ec051",
            "7c9d1005c50145a6889c0c4fd934d04e",
            "c5ea2548e208477e8addaf1080f81e73",
            "a41aaab560dd42628a5cc55d6eb6b713",
            "8c49a9d758fb4253ad7884db2b03766f",
            "f8f8d70e31e74defa5f2d3077c54fb32",
            "cbcf72d0d8564427a4c25cefce20f5b8",
            "ddb8f6feb68746298120a5a65133ceed",
            "da153661a7634be885b57f4b1d8be7fd",
            "1d47aa18eef8492e8aca0c591eabc7c5",
            "0b56fd6693dd4a29aefbf180c5bfa1dd",
            "5fcfdadeb4974272b74c03e7eb6f5e66",
            "f3dbd37793c74d3aac6ea83024800336",
            "e08606c502b14c46ba62662e5edc46e4",
            "542e8ce4c4c94d239a9fbdc01726aa1f",
            "7f4006b550dc4a95b0ca1e73da8d7de9",
            "c11ef3bbd1a1446aaf87061f89347050",
            "558b22c90d404d28b031e6a094c8e013",
            "29db940e7f3a4067a8572a9d0633c8c9",
            "a1c38d5c0c40418e9f8267f72194a33e",
            "926e1de09c5e408f89014f1ec6a35b2a",
            "dd7740ff1c094b318aaf93c38f44e0e8",
            "33f92ef6be91423980a7324ac2d264be",
            "2f03da5ed54f4f4b89ae871fdf725f28",
            "439b751316834035b6ccf81da0daf897",
            "7c029cf0c41a449badc4969dda426697",
            "5fdc1b411b564a2da2ede7d14526091e",
            "d5a60b1eb0d4426292bcd6033c2074ad",
            "2ad1ee1419da48ca89339a763c3821ab",
            "147ec7e718c0423289597feaf9ec1cb9",
            "8d993abceca2462990ec2b2c6d0d0a11",
            "18303e8b7cab4ccf86a3092a86250a23",
            "1aa5d9fefcf84f91af5748a48d175bd8",
            "76bcf6c0475b4bc9ba49b99182c6cab2",
            "1c2f7441dd344ab2893e2a0e9d3419b5",
            "a3a844720f0946c980ca6b086412abf3",
            "4e627589be6b4e44b28c9512ea9203ea",
            "4afe575e05fb46f2992bd1fa5d5beadf",
            "2b3e7ee8b0d14d6fb34b8f9df2b0a960",
            "2e94396cd0d84a859996fb0bb8b547cb",
            "cfce33fe370042b194c37d4fdccfea94",
            "d56c510d90684a6a9e02eebfa0743088",
            "a4debb2d121645d68ad1c5bcdcdb8421",
            "b945844d47a34288885b90ab7bfe7608",
            "41ade82d9776421296037173b3683b8a",
            "f0db9d141aac4662bb08aa7e5ef615de",
            "d2a3371eb6ec404aa8ec2f27580a4e11",
            "8eea19daed6e447888a46bc064ab7106",
            "ffab261e183f4b2cbb4be975a995e569",
            "98ae4cca63e04c0b87afe46d69bf72bb",
            "4ad63fd6fd824fe891a7bd9fc5c604c7",
            "6670ecada3b6452fa0e6b82f185fe8a6",
            "b211f100d14545178174f66846ace02f",
            "49e0155934244ed698e5307a166a04ee",
            "df316e40080b4f2c952dbf3a2fdb8f87",
            "69fd06899978452194ff954cc2438706",
            "665c4e6693804318854340539d7a718f",
            "430216f1e9db4c4b95f6c247e496fe30",
            "ad81309ff34446e3bf5ba1c9e96accf6",
            "d5339dd079614948b08ade6ab3aa3fdf",
            "b4a8eb056cfa46c18d383a1d7c1c6395",
            "6d9c10d290914719b316fb6eaeadb1ab",
            "7c6e8a17a5a940fba56902be3b006560",
            "63dbe621de3b4d7493e8442cd71ba692",
            "d9d26e9b839649c489952af026b915e7",
            "a32c95c9e60f4b4c8e1311b93c0242fb",
            "370327ce35294ba7affeca69ee28bb0b",
            "54b7f21a696e44c2a57556bf4deaba47",
            "45eb3c14fc244f18aac0d0df13f4f98c",
            "88a4111c5c474512bc153fca18e68dca",
            "b1353a2b5c8545bebfb3c35263088215",
            "87197a80d9d648978f97257749a7cfc9",
            "864d232cc313491a9d46279a0125998c",
            "75d7283397454875a54b49a430243485",
            "c98edc25dd2d4e599c80f5b5f6decad0",
            "fed0f6afc41048b68189f379bedfa0e6",
            "141d1c03e48c4540a48588ace3b926d3",
            "fb81b2f8787540bf88150787ef356206",
            "5602d387afab45f5b2f8901d09040cef",
            "2f289a4892c94531970c9af80809d199",
            "31ea2cf95a5e4db3aed5d59c8b485689",
            "c8c59c55998f495bb59814859d954d41",
            "737640930ca64efda9b02cf0d454ff9e",
            "b00e513ea0c14057bba65e41ba862b65",
            "31c1192a8b2a4e30b689ed8922390de7",
            "aacd6ef29d0a43b786bd1fe1aa6fac24",
            "f77ea4b650064115ba9b9ebd62e3d2dc",
            "dcfb995c2ab243c2b9a2501d4c76ec9b",
            "6db2d08bdee64ce19b9c5c9f01348612",
            "b8fe8529caaa4beb9bb1bd5c238e0953",
            "def829e24f46400f8e45856449654bf3",
            "99e89bdbaf7d4c2b808e475231a5330c",
            "b7f2f910da4c4180b80b001250ca74b2",
            "c578e195e1bd4556b71815efc4c07372",
            "1b832e01521241d8974e058125d22a2b"
          ]
        },
        "executionInfo": {
          "elapsed": 230344,
          "status": "ok",
          "timestamp": 1755192001046,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "-3GySUq4lPGS",
        "outputId": "a76cd4c3-4464-4294-98f9-8dbaf0918441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd27 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANIYOR - FINAL\n",
            "============================================================\n",
            "\ud83c\udfaf Hedef 1: XLM-RoBERTa + SVM Linear\n",
            "\ud83c\udfaf Hedef 2: XLM-RoBERTa + Threshold Optimization\n",
            "\u23f0 Tahmini s\u00fcre: 15-20 dakika\n",
            "\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83e\udd16 XLM-RoBERTa EMBEDDINGS...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "debc3b67d40c47dabee9a9779696423e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d74abd20640240d1866e67474b67eebd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c90ebd721da04037bbdc060815be4415",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a24491507615478380cfd7762bac2ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddb8f6feb68746298120a5a65133ceed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29db940e7f3a4067a8572a9d0633c8c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "147ec7e718c0423289597feaf9ec1cb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/550 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfce33fe370042b194c37d4fdccfea94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6670ecada3b6452fa0e6b82f185fe8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c6e8a17a5a940fba56902be3b006560",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75d7283397454875a54b49a430243485",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31c1192a8b2a4e30b689ed8922390de7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/632 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Embeddings haz\u0131r! (0.4 dakika)\n",
            "\n",
            "\ud83d\ude80 ROBERTA + SVM LINEAR HESAPLANIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa + SVM Linear METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd Linear SVM with RoBERTa\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8713, Acc: 0.8731, Prec: 0.8714, Rec: 0.8711\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8700, Acc: 0.8718, Prec: 0.8698, Rec: 0.8702\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8690, Acc: 0.8711, Prec: 0.8699, Rec: 0.8682\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8760, Acc: 0.8780, Prec: 0.8769, Rec: 0.8752\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8748, Acc: 0.8770, Prec: 0.8764, Rec: 0.8736\n",
            "\n",
            "   \u2705 XLM-RoBERTa + SVM Linear SONU\u00c7LARI (2.9 dakika):\n",
            "      \ud83c\udfaf F1: 0.8722 \u00b1 0.0027\n",
            "      \ud83d\udcca Accuracy: 0.8742 \u00b1 0.0028\n",
            "      \ud83d\udcc8 Precision: 0.8729 \u00b1 0.0031\n",
            "      \ud83d\udcc8 Recall: 0.8717 \u00b1 0.0025\n",
            "\n",
            "\ud83d\ude80 ROBERTA THRESHOLD OPTIMIZATION...\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfaf XLM-RoBERTa THRESHOLD OPTIMIZATION...\n",
            "\ud83d\udcdd Base F1: 0.8745\n",
            "   \ud83d\udd0d Threshold arama...\n",
            "   \u2705 THRESHOLD OPTIMIZATION SONU\u00c7LARI:\n",
            "      \ud83c\udfaf En iyi threshold: 0.530\n",
            "      \ud83c\udfaf Optimized F1: 0.8793\n",
            "      \ud83d\udcca Optimized Accuracy: 0.8807\n",
            "      \ud83d\udcc8 Optimized Precision: 0.8785\n",
            "      \ud83d\udcc8 Optimized Recall: 0.8803\n",
            "      \ud83d\udcc8 \u0130yile\u015fme: +0.0048\n",
            "\n",
            "\ud83d\udcca EKS\u0130K ROBERTA METR\u0130KLER TAMAMLANDI\n",
            "======================================================================\n",
            "\n",
            "\u2705 XLM-RoBERTa + SVM Linear:\n",
            "   F1: 0.8722\n",
            "   Accuracy: 0.8742\n",
            "   Precision: 0.8729\n",
            "   Recall: 0.8717\n",
            "\n",
            "\u2705 XLM-RoBERTa + Threshold Optimization:\n",
            "   F1: 0.8793\n",
            "   Accuracy: 0.8807\n",
            "   Precision: 0.8785\n",
            "   Recall: 0.8803\n",
            "\n",
            "\ud83e\udd4a UPDATED TECHNIQUE COMPARISON:\n",
            "==================================================\n",
            "Fine-tuning     | BERT: 89.89%    | RoBERTa: 88.16% | BERT +1.73%\n",
            "SVM RBF         | BERT: 87.91%    | RoBERTa: 87.86% | BERT +0.05%\n",
            "SVM Linear      | BERT: 87.82%    | RoBERTa: 0.87%  | TBD\n",
            "LogReg          | BERT: 86.45%    | RoBERTa: 87.45% | RoBERTa +1.00%\n",
            "Optimization    | BERT: 90.10%    | RoBERTa: 0.88%  | TBD\n",
            "\n",
            "\u2705 EKS\u0130K ROBERTA METR\u0130KLER KAYDED\u0130LD\u0130!\n",
            "\ud83d\udcc1 ROBERTA_FINAL_MISSING_METRICS.xlsx\n",
            "\n",
            "\ud83c\udf89 COMPLETE FAIR COMPARISON HAZ\u0131R!\n",
            "\ud83c\udfc6 Art\u0131k hi\u00e7bir eksik yok!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "print(\"\ud83d\udd27 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANIYOR - FINAL\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf Hedef 1: XLM-RoBERTa + SVM Linear\")\n",
        "print(\"\ud83c\udfaf Hedef 2: XLM-RoBERTa + Threshold Optimization\")\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 15-20 dakika\")\n",
        "print()\n",
        "\n",
        "# Veri setini y\u00fckle\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).values\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# XLM-RoBERTa embeddings (\u00f6nceden hesaplad\u0131k, h\u0131zl\u0131 olacak)\n",
        "print(f\"\\n\ud83e\udd16 XLM-RoBERTa EMBEDDINGS...\")\n",
        "start_embed = time.time()\n",
        "\n",
        "roberta_model = SentenceTransformer(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\")\n",
        "X_roberta = roberta_model.encode(texts, show_progress_bar=True, batch_size=24)\n",
        "\n",
        "embed_time = time.time() - start_embed\n",
        "print(f\"\u2705 Embeddings haz\u0131r! ({embed_time/60:.1f} dakika)\")\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Eksik modeller\n",
        "missing_roberta_models = [\n",
        "    {\n",
        "        'name': 'XLM-RoBERTa + SVM Linear',\n",
        "        'classifier': SVC(kernel='linear', random_state=42, C=1.0),\n",
        "        'description': 'Linear SVM with RoBERTa',\n",
        "        'type': 'svm'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sonu\u00e7lar\u0131 saklayacak liste\n",
        "final_roberta_results = []\n",
        "\n",
        "def calculate_missing_roberta_metrics(model_info, X, y):\n",
        "    \"\"\"Eksik RoBERTa metrikleri hesapla\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 {model_info['name']} METR\u0130KLER HESAPLANIYOR...\")\n",
        "    print(f\"\ud83d\udcdd {model_info['description']}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Her fold i\u00e7in sonu\u00e7lar\u0131 sakla\n",
        "    fold_accuracies = []\n",
        "    fold_precisions = []\n",
        "    fold_recalls = []\n",
        "    fold_f1s = []\n",
        "\n",
        "    print(\"   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
        "        print(f\"      \ud83d\udccb Fold {fold+1}/5 i\u015fleniyor...\")\n",
        "\n",
        "        # Veri b\u00f6l\u00fcmlemesi\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = [y[i] for i in train_idx], [y[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit\n",
        "        classifier = model_info['classifier']\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # Tahmin yap\n",
        "        y_pred = classifier.predict(X_val)\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
        "\n",
        "        # Fold sonu\u00e7lar\u0131n\u0131 kaydet\n",
        "        fold_accuracies.append(accuracy)\n",
        "        fold_precisions.append(precision)\n",
        "        fold_recalls.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "        print(f\"         F1: {f1:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}\")\n",
        "\n",
        "    # Ortalama ve standart sapma hesapla\n",
        "    calc_time = time.time() - start_time\n",
        "\n",
        "    results = {\n",
        "        'Model': model_info['name'],\n",
        "        'F1_Mean': np.mean(fold_f1s),\n",
        "        'F1_Std': np.std(fold_f1s),\n",
        "        'Accuracy_Mean': np.mean(fold_accuracies),\n",
        "        'Accuracy_Std': np.std(fold_accuracies),\n",
        "        'Precision_Mean': np.mean(fold_precisions),\n",
        "        'Precision_Std': np.std(fold_precisions),\n",
        "        'Recall_Mean': np.mean(fold_recalls),\n",
        "        'Recall_Std': np.std(fold_recalls),\n",
        "        'Calculation_Time_Min': calc_time/60,\n",
        "        'Type': model_info['type']\n",
        "    }\n",
        "\n",
        "    # Sonu\u00e7lar\u0131 g\u00f6ster\n",
        "    print(f\"\\n   \u2705 {model_info['name']} SONU\u00c7LARI ({calc_time/60:.1f} dakika):\")\n",
        "    print(f\"      \ud83c\udfaf F1: {results['F1_Mean']:.4f} \u00b1 {results['F1_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcca Accuracy: {results['Accuracy_Mean']:.4f} \u00b1 {results['Accuracy_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Precision: {results['Precision_Mean']:.4f} \u00b1 {results['Precision_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Recall: {results['Recall_Mean']:.4f} \u00b1 {results['Recall_Std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def calculate_threshold_optimization(X, y, base_f1):\n",
        "    \"\"\"RoBERTa i\u00e7in threshold optimization\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf XLM-RoBERTa THRESHOLD OPTIMIZATION...\")\n",
        "    print(f\"\ud83d\udcdd Base F1: {base_f1:.4f}\")\n",
        "\n",
        "    # Train/test split for threshold optimization\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Base model e\u011fit\n",
        "    base_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    base_model.fit(X_train, y_train)\n",
        "\n",
        "    # Probabilities al\n",
        "    y_probs = base_model.predict_proba(X_test)[:, 1]  # Positive class probabilities\n",
        "\n",
        "    # Threshold optimization\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_metrics = {}\n",
        "\n",
        "    print(\"   \ud83d\udd0d Threshold arama...\")\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        y_pred_thresh = (y_probs >= threshold).astype(int)\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        accuracy = accuracy_score(y_test, y_pred_thresh)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_thresh, average='macro')\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "            best_metrics = {\n",
        "                'F1': f1,\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'Threshold': threshold\n",
        "            }\n",
        "\n",
        "    print(f\"   \u2705 THRESHOLD OPTIMIZATION SONU\u00c7LARI:\")\n",
        "    print(f\"      \ud83c\udfaf En iyi threshold: {best_threshold:.3f}\")\n",
        "    print(f\"      \ud83c\udfaf Optimized F1: {best_metrics['F1']:.4f}\")\n",
        "    print(f\"      \ud83d\udcca Optimized Accuracy: {best_metrics['Accuracy']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Optimized Precision: {best_metrics['Precision']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Optimized Recall: {best_metrics['Recall']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 \u0130yile\u015fme: {best_metrics['F1'] - base_f1:+.4f}\")\n",
        "\n",
        "    return {\n",
        "        'Model': 'XLM-RoBERTa + Threshold Optimization',\n",
        "        'F1_Mean': best_metrics['F1'],\n",
        "        'Accuracy_Mean': best_metrics['Accuracy'],\n",
        "        'Precision_Mean': best_metrics['Precision'],\n",
        "        'Recall_Mean': best_metrics['Recall'],\n",
        "        'Threshold': best_threshold,\n",
        "        'Improvement': best_metrics['F1'] - base_f1,\n",
        "        'Type': 'optimization'\n",
        "    }\n",
        "\n",
        "# 1. RoBERTa + SVM Linear hesapla\n",
        "print(f\"\\n\ud83d\ude80 ROBERTA + SVM LINEAR HESAPLANIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_info in missing_roberta_models:\n",
        "    result = calculate_missing_roberta_metrics(model_info, X_roberta, labels)\n",
        "    final_roberta_results.append(result)\n",
        "\n",
        "# 2. RoBERTa + Threshold Optimization\n",
        "print(f\"\\n\ud83d\ude80 ROBERTA THRESHOLD OPTIMIZATION...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_roberta_f1 = 0.8745  # XLM-RoBERTa + LogReg F1\n",
        "threshold_result = calculate_threshold_optimization(X_roberta, labels, base_roberta_f1)\n",
        "final_roberta_results.append(threshold_result)\n",
        "\n",
        "# Sonu\u00e7lar\u0131 analiz et\n",
        "print(f\"\\n\ud83d\udcca EKS\u0130K ROBERTA METR\u0130KLER TAMAMLANDI\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for result in final_roberta_results:\n",
        "    print(f\"\\n\u2705 {result['Model']}:\")\n",
        "    print(f\"   F1: {result['F1_Mean']:.4f}\")\n",
        "    print(f\"   Accuracy: {result['Accuracy_Mean']:.4f}\")\n",
        "    print(f\"   Precision: {result['Precision_Mean']:.4f}\")\n",
        "    print(f\"   Recall: {result['Recall_Mean']:.4f}\")\n",
        "\n",
        "# BERT ile kar\u015f\u0131la\u015ft\u0131rma\n",
        "print(f\"\\n\ud83e\udd4a UPDATED TECHNIQUE COMPARISON:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Updated comparison table\n",
        "comparison_data = [\n",
        "    ['Fine-tuning', 'BERT: 89.89%', 'RoBERTa: 88.16%', 'BERT +1.73%'],\n",
        "    ['SVM RBF', 'BERT: 87.91%', 'RoBERTa: 87.86%', 'BERT +0.05%'],\n",
        "    ['SVM Linear', 'BERT: 87.82%', f'RoBERTa: {final_roberta_results[0][\"F1_Mean\"]:.2f}%', 'TBD'],\n",
        "    ['LogReg', 'BERT: 86.45%', 'RoBERTa: 87.45%', 'RoBERTa +1.00%'],\n",
        "    ['Optimization', 'BERT: 90.10%', f'RoBERTa: {final_roberta_results[1][\"F1_Mean\"]:.2f}%', 'TBD']\n",
        "]\n",
        "\n",
        "for row in comparison_data:\n",
        "    print(f\"{row[0]:15} | {row[1]:15} | {row[2]:15} | {row[3]}\")\n",
        "\n",
        "# Sonu\u00e7lar\u0131 kaydet\n",
        "final_df = pd.DataFrame(final_roberta_results)\n",
        "final_df.to_excel(\"/content/drive/MyDrive/ROBERTA_FINAL_MISSING_METRICS.xlsx\", index=False)\n",
        "\n",
        "print(f\"\\n\u2705 EKS\u0130K ROBERTA METR\u0130KLER KAYDED\u0130LD\u0130!\")\n",
        "print(f\"\ud83d\udcc1 ROBERTA_FINAL_MISSING_METRICS.xlsx\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 COMPLETE FAIR COMPARISON HAZ\u0131R!\")\n",
        "print(f\"\ud83c\udfc6 Art\u0131k hi\u00e7bir eksik yok!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "56112e0769f34b0d9ed7631e9b0f4648",
            "bf97dc82fc1d4962b303d732f3fe4c6e",
            "1f2a88eb30634e76866944c4aa4ed3de",
            "993a269c668f4e3888e3e7ef69771f8c",
            "65b85663595340f9a3c84e884e113475",
            "94f4bb16282e42e5bdc2fb9c78dd5bf7",
            "38c352f26f06467ca0c55a9c7b59c8d4",
            "9b5defb4c26348259b6fe5a50fd1bf44",
            "12c66b9a594c44cd87fed6c3a06a1ffa",
            "8fa69426c2ff41d89fdfe647a3252a36",
            "a8114154e1f644a6a1ea697c02cf6782"
          ]
        },
        "executionInfo": {
          "elapsed": 235942,
          "status": "ok",
          "timestamp": 1755186317203,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "LueiJXmePiK7",
        "outputId": "0beb6045-f41c-4c44-942b-b39801fcd289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd27 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANIYOR\n",
            "============================================================\n",
            "\ud83c\udfaf Hedef: XLM-RoBERTa + LogReg ve + SVM i\u00e7in Accuracy, Precision, Recall\n",
            "\u23f0 Tahmini s\u00fcre: 20-25 dakika\n",
            "\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83e\udd16 XLM-RoBERTa EMBEDDINGS \u00c7IKARILIYOR...\n",
            "\u23f0 Bu i\u015flem 15-20 dakika s\u00fcrebilir...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56112e0769f34b0d9ed7631e9b0f4648",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/632 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Embeddings tamamland\u0131! (0.2 dakika)\n",
            "\ud83d\udcca Embedding boyutu: (15167, 768)\n",
            "\n",
            "\ud83d\ude80 ROBERTA MODELLER\u0130 HESAPLANIYOR...\n",
            "============================================================\n",
            "\n",
            "============================== MODEL 1/2 ==============================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa + LogReg METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd Baseline RoBERTa\n",
            "\ud83c\udfaf Beklenen F1: 0.8748\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8749, Acc: 0.8764, Prec: 0.8743, Rec: 0.8755\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8718, Acc: 0.8734, Prec: 0.8714, Rec: 0.8722\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8693, Acc: 0.8714, Prec: 0.8702, Rec: 0.8686\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8811, Acc: 0.8830, Prec: 0.8818, Rec: 0.8805\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8755, Acc: 0.8777, Prec: 0.8770, Rec: 0.8743\n",
            "\n",
            "   \u2705 XLM-RoBERTa + LogReg SONU\u00c7LARI (0.4 dakika):\n",
            "      \ud83c\udfaf F1: 0.8745 \u00b1 0.0040\n",
            "      \ud83d\udcca Accuracy: 0.8764 \u00b1 0.0040\n",
            "      \ud83d\udcc8 Precision: 0.8749 \u00b1 0.0042\n",
            "      \ud83d\udcc8 Recall: 0.8742 \u00b1 0.0039\n",
            "      \ud83d\udccb Beklenen vs Hesaplanan: 0.8748 vs 0.8745\n",
            "      \ud83d\udcca Fark: -0.0003\n",
            "\u2705 XLM-RoBERTa + LogReg tamamland\u0131!\n",
            "\n",
            "============================== MODEL 2/2 ==============================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa + SVM RBF METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd Optimized RoBERTa\n",
            "\ud83c\udfaf Beklenen F1: 0.8786\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8748, Acc: 0.8767, Prec: 0.8753, Rec: 0.8744\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8700, Acc: 0.8721, Prec: 0.8708, Rec: 0.8693\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8836, Acc: 0.8856, Prec: 0.8851, Rec: 0.8824\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8802, Acc: 0.8823, Prec: 0.8819, Rec: 0.8788\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8846, Acc: 0.8866, Prec: 0.8861, Rec: 0.8834\n",
            "\n",
            "   \u2705 XLM-RoBERTa + SVM RBF SONU\u00c7LARI (3.3 dakika):\n",
            "      \ud83c\udfaf F1: 0.8786 \u00b1 0.0055\n",
            "      \ud83d\udcca Accuracy: 0.8807 \u00b1 0.0055\n",
            "      \ud83d\udcc8 Precision: 0.8799 \u00b1 0.0059\n",
            "      \ud83d\udcc8 Recall: 0.8777 \u00b1 0.0052\n",
            "      \ud83d\udccb Beklenen vs Hesaplanan: 0.8786 vs 0.8786\n",
            "      \ud83d\udcca Fark: +0.0000\n",
            "\u2705 XLM-RoBERTa + SVM RBF tamamland\u0131!\n",
            "\n",
            "\ud83d\udcca ROBERTA COMPLETE RESULTS SUMMARY\n",
            "======================================================================\n",
            "\ud83c\udfc6 ROBERTA MODEL PERFORMANS SIRALAMASI:\n",
            "--------------------------------------------------\n",
            "\ud83e\udd47 XLM-RoBERTa + SVM RBF    \n",
            "    F1: 0.8786 \u00b1 0.0055\n",
            "    Accuracy: 0.8807 \u00b1 0.0055\n",
            "    Precision: 0.8799 \u00b1 0.0059\n",
            "    Recall: 0.8777 \u00b1 0.0052\n",
            "\n",
            "\ud83e\udd48 XLM-RoBERTa + LogReg     \n",
            "    F1: 0.8745 \u00b1 0.0040\n",
            "    Accuracy: 0.8764 \u00b1 0.0040\n",
            "    Precision: 0.8749 \u00b1 0.0042\n",
            "    Recall: 0.8742 \u00b1 0.0039\n",
            "\n",
            "\ud83d\udcda MAKALE \u0130\u00c7\u0130N FORMATTED TABLE:\n",
            "------------------------------------------------------------\n",
            "                Model F1 Score Accuracy Precision Recall F1 Std\n",
            "XLM-RoBERTa + SVM RBF    0.88%    0.88%     0.88%  0.88% \u00b10.01%\n",
            " XLM-RoBERTa + LogReg    0.87%    0.88%     0.87%  0.87% \u00b10.00%\n",
            "\n",
            "\u2705 ROBERTA SONU\u00c7LARI KAYDED\u0130LD\u0130!\n",
            "\ud83d\udcc1 ROBERTA_COMPLETE_METRICS.xlsx\n",
            "\ud83d\udcc1 ROBERTA_FORMATTED_RESULTS.xlsx\n",
            "\n",
            "\ud83c\udf89 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANDI!\n",
            "\ud83d\udcca Art\u0131k RoBERTa i\u00e7in complete data haz\u0131r!\n",
            "\ud83c\udfaf Sonraki ad\u0131m: BERT + SVM testleri\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "print(\"\ud83d\udd27 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANIYOR\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf Hedef: XLM-RoBERTa + LogReg ve + SVM i\u00e7in Accuracy, Precision, Recall\")\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 20-25 dakika\")\n",
        "print()\n",
        "\n",
        "# Veri setini y\u00fckle\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).values\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# XLM-RoBERTa embeddings \u00e7\u0131kar (bir kez)\n",
        "print(f\"\\n\ud83e\udd16 XLM-RoBERTa EMBEDDINGS \u00c7IKARILIYOR...\")\n",
        "print(\"\u23f0 Bu i\u015flem 15-20 dakika s\u00fcrebilir...\")\n",
        "\n",
        "start_embed = time.time()\n",
        "roberta_model = SentenceTransformer(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\")\n",
        "X_roberta = roberta_model.encode(texts, show_progress_bar=True, batch_size=24)\n",
        "embed_time = time.time() - start_embed\n",
        "\n",
        "print(f\"\u2705 Embeddings tamamland\u0131! ({embed_time/60:.1f} dakika)\")\n",
        "print(f\"\ud83d\udcca Embedding boyutu: {X_roberta.shape}\")\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Hesaplanacak modeller\n",
        "roberta_models = [\n",
        "    {\n",
        "        'name': 'XLM-RoBERTa + LogReg',\n",
        "        'classifier': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'expected_f1': 0.8748,\n",
        "        'description': 'Baseline RoBERTa'\n",
        "    },\n",
        "    {\n",
        "        'name': 'XLM-RoBERTa + SVM RBF',\n",
        "        'classifier': SVC(kernel='rbf', random_state=42, C=1.0, gamma='scale'),\n",
        "        'expected_f1': 0.8786,\n",
        "        'description': 'Optimized RoBERTa'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sonu\u00e7lar\u0131 saklayacak liste\n",
        "roberta_complete_results = []\n",
        "\n",
        "def calculate_complete_metrics(model_info, X, y):\n",
        "    \"\"\"Bir model i\u00e7in t\u00fcm metrikleri hesapla\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 {model_info['name']} METR\u0130KLER HESAPLANIYOR...\")\n",
        "    print(f\"\ud83d\udcdd {model_info['description']}\")\n",
        "    print(f\"\ud83c\udfaf Beklenen F1: {model_info['expected_f1']:.4f}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Her fold i\u00e7in sonu\u00e7lar\u0131 sakla\n",
        "    fold_accuracies = []\n",
        "    fold_precisions = []\n",
        "    fold_recalls = []\n",
        "    fold_f1s = []\n",
        "\n",
        "    print(\"   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
        "        print(f\"      \ud83d\udccb Fold {fold+1}/5 i\u015fleniyor...\")\n",
        "\n",
        "        # Veri b\u00f6l\u00fcmlemesi\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = [y[i] for i in train_idx], [y[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit\n",
        "        classifier = model_info['classifier']\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # Tahmin yap\n",
        "        y_pred = classifier.predict(X_val)\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
        "\n",
        "        # Fold sonu\u00e7lar\u0131n\u0131 kaydet\n",
        "        fold_accuracies.append(accuracy)\n",
        "        fold_precisions.append(precision)\n",
        "        fold_recalls.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "        print(f\"         F1: {f1:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}\")\n",
        "\n",
        "    # Ortalama ve standart sapma hesapla\n",
        "    calc_time = time.time() - start_time\n",
        "\n",
        "    results = {\n",
        "        'Model': model_info['name'],\n",
        "        'F1_Mean': np.mean(fold_f1s),\n",
        "        'F1_Std': np.std(fold_f1s),\n",
        "        'Accuracy_Mean': np.mean(fold_accuracies),\n",
        "        'Accuracy_Std': np.std(fold_accuracies),\n",
        "        'Precision_Mean': np.mean(fold_precisions),\n",
        "        'Precision_Std': np.std(fold_precisions),\n",
        "        'Recall_Mean': np.mean(fold_recalls),\n",
        "        'Recall_Std': np.std(fold_recalls),\n",
        "        'Expected_F1': model_info['expected_f1'],\n",
        "        'F1_Difference': np.mean(fold_f1s) - model_info['expected_f1'],\n",
        "        'Calculation_Time_Min': calc_time/60,\n",
        "        'Description': model_info['description']\n",
        "    }\n",
        "\n",
        "    # Sonu\u00e7lar\u0131 g\u00f6ster\n",
        "    print(f\"\\n   \u2705 {model_info['name']} SONU\u00c7LARI ({calc_time/60:.1f} dakika):\")\n",
        "    print(f\"      \ud83c\udfaf F1: {results['F1_Mean']:.4f} \u00b1 {results['F1_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcca Accuracy: {results['Accuracy_Mean']:.4f} \u00b1 {results['Accuracy_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Precision: {results['Precision_Mean']:.4f} \u00b1 {results['Precision_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Recall: {results['Recall_Mean']:.4f} \u00b1 {results['Recall_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udccb Beklenen vs Hesaplanan: {model_info['expected_f1']:.4f} vs {results['F1_Mean']:.4f}\")\n",
        "    print(f\"      \ud83d\udcca Fark: {results['F1_Difference']:+.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# RoBERTa modellerini hesapla\n",
        "print(f\"\\n\ud83d\ude80 ROBERTA MODELLER\u0130 HESAPLANIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, model_info in enumerate(roberta_models):\n",
        "    print(f\"\\n{'='*30} MODEL {i+1}/2 {'='*30}\")\n",
        "\n",
        "    result = calculate_complete_metrics(model_info, X_roberta, labels)\n",
        "    roberta_complete_results.append(result)\n",
        "\n",
        "    print(f\"\u2705 {model_info['name']} tamamland\u0131!\")\n",
        "\n",
        "# Sonu\u00e7lar\u0131 analiz et ve kaydet\n",
        "print(f\"\\n\ud83d\udcca ROBERTA COMPLETE RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# DataFrame olu\u015ftur\n",
        "roberta_df = pd.DataFrame(roberta_complete_results)\n",
        "\n",
        "# S\u0131ralama (F1'e g\u00f6re)\n",
        "roberta_df_sorted = roberta_df.sort_values('F1_Mean', ascending=False)\n",
        "\n",
        "print(\"\ud83c\udfc6 ROBERTA MODEL PERFORMANS SIRALAMASI:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, (_, row) in enumerate(roberta_df_sorted.iterrows()):\n",
        "    rank = [\"\ud83e\udd47\", \"\ud83e\udd48\"][i] if i < 2 else f\"{i+1}\ufe0f\u20e3\"\n",
        "    print(f\"{rank} {row['Model']:25}\")\n",
        "    print(f\"    F1: {row['F1_Mean']:.4f} \u00b1 {row['F1_Std']:.4f}\")\n",
        "    print(f\"    Accuracy: {row['Accuracy_Mean']:.4f} \u00b1 {row['Accuracy_Std']:.4f}\")\n",
        "    print(f\"    Precision: {row['Precision_Mean']:.4f} \u00b1 {row['Precision_Std']:.4f}\")\n",
        "    print(f\"    Recall: {row['Recall_Mean']:.4f} \u00b1 {row['Recall_Std']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Formatted table (makale i\u00e7in)\n",
        "print(\"\ud83d\udcda MAKALE \u0130\u00c7\u0130N FORMATTED TABLE:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "formatted_results = []\n",
        "for _, row in roberta_df_sorted.iterrows():\n",
        "    formatted_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'F1 Score': f\"{row['F1_Mean']:.2f}%\",\n",
        "        'Accuracy': f\"{row['Accuracy_Mean']:.2f}%\",\n",
        "        'Precision': f\"{row['Precision_Mean']:.2f}%\",\n",
        "        'Recall': f\"{row['Recall_Mean']:.2f}%\",\n",
        "        'F1 Std': f\"\u00b1{row['F1_Std']:.2f}%\"\n",
        "    })\n",
        "\n",
        "formatted_df = pd.DataFrame(formatted_results)\n",
        "print(formatted_df.to_string(index=False))\n",
        "\n",
        "# Sonu\u00e7lar\u0131 kaydet\n",
        "roberta_df.to_excel(\"/content/drive/MyDrive/ROBERTA_COMPLETE_METRICS.xlsx\", index=False)\n",
        "formatted_df.to_excel(\"/content/drive/MyDrive/ROBERTA_FORMATTED_RESULTS.xlsx\", index=False)\n",
        "\n",
        "print(f\"\\n\u2705 ROBERTA SONU\u00c7LARI KAYDED\u0130LD\u0130!\")\n",
        "print(f\"\ud83d\udcc1 ROBERTA_COMPLETE_METRICS.xlsx\")\n",
        "print(f\"\ud83d\udcc1 ROBERTA_FORMATTED_RESULTS.xlsx\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 ROBERTA EKS\u0130K METR\u0130KLER TAMAMLANDI!\")\n",
        "print(f\"\ud83d\udcca Art\u0131k RoBERTa i\u00e7in complete data haz\u0131r!\")\n",
        "print(f\"\ud83c\udfaf Sonraki ad\u0131m: BERT + SVM testleri\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "582ae209e27a477cb709ebe6bf6cbe75",
            "54c77c5f2b9c4d17b2e26184ab1ce8ca",
            "371af0cc909244209e42078fabc1bf02",
            "19ecc7361ad140ce88eec566f8a78d5a",
            "29ee7d7c1b814faeb305c73e5a8b8140",
            "80b17612396f4224a78f2b153ef46ac9",
            "3cb3f89a1a5741ceb13070644b872ace",
            "9d86709633b542bc99b71ea98a0f5ebc",
            "21b8d44928dc4763b09b9a4ff8adfdcc",
            "04c59c28f03a4c08b6c88307db655921",
            "00f53b7ed11740f8a7711eb20de87ab4",
            "f15ff9a726d743859428de0661a03cc7",
            "0a5c46187beb417f90ffeaf9579fccf5",
            "758d239ee8d94ae8894b0a66fcd4cf5f",
            "b832740c64534a2798a66dd010bb86c7",
            "b3172def46594b03b8d6004fe22b100a",
            "73a68b15b8fb4528a50622b6eff549e5",
            "af103f3e3bc44fc28c963cbf61e7b633",
            "4f4ad5fafd774f648996d16d516f99ad",
            "0884bb7c7c324442934be37da9b8f044",
            "c29f3d0805b7406d8c36ab2beb4d2bdc",
            "29c14a96c77e43e48089955cc4c9e61d",
            "2a0dddbd8575464ca713467ac79fd99a",
            "2a7b7a4711ed47f1a931deb9d31e8b5a",
            "76a75b43184c498fa58ef8f37b9ff02c",
            "6e942daab5b84ffaa9aa811e814a8123",
            "4d2d2040dd2f4d6a91ac6f56ca6d01ef",
            "85ba226e9310471d96f413d49a1ffc9e",
            "37733f2e5e0a408c984b95bff5c87f5b",
            "e219ee93ce884bc1a35ea440ac77b902",
            "cdfdd5be3354495b900d1942a3f76838",
            "a6108dc792d34a2d9fa60354efe8c4e4",
            "e737ba39fe984f5f8ae062ce9b1cf19d",
            "183d0842da4a4108acf0b1a7b710b099",
            "395fbccbda6c45439ef3f3a9092b6d3d",
            "60b903331e96471698396beb0ed86679",
            "6b08ea3bd30b49db986d4a080a65cafa",
            "d013ce246e89472c896dd433c5fdc7ea",
            "ec0e4abfc8c4454684a272ba49a76edc",
            "984a297735024711b5d8d886be67e2dc",
            "9f02c2d6a870481f853ca84c0f61ac1f",
            "1aef2aa006074ca99563e7ca569a8cd6",
            "5a9e7adada8940e989c9d0ab09d7283b",
            "2fe1d8a7e6d54019986058d17e88f917",
            "a7e0fd278317450984594949a0db2b32",
            "0705d9ba1f2b45a6ad4ebaeabaff2bb0",
            "c17c2a9847d2494dadfbeaab3a3e70b4",
            "326e7b03c3544a798b598c8ee03d919c",
            "3e087c1a73f849bfad1f56dc487d1fa5",
            "6e62e9298844402a9bdad5c348e30a7b",
            "c1a7c01c15ce46a2890f362db6a869a9",
            "418961101b344f18a9968af740611c98",
            "82020386ab614f3e898a04a9ad44491a",
            "0473ca13e1194ef9998467e707e1be75",
            "3c32c44c57364b80840f3ce0b7453741",
            "8e283bbd3b8340b88102ad25666b4e2c",
            "495cf93a64094d35b736c196739bafee",
            "bd507ec6c9e449cd889e52b494eff938",
            "5ec23f1cd0e0423cbaa4594cdce2148f",
            "628c2278668d49d7885ba617254e7c76",
            "cc36b43223fa43ce9c2f83a3342b30ce",
            "a24977d992c64c5681189ab6e7f3652f",
            "d70b153f029a4e089dc202d15a18c4f9",
            "3ad5e84e062541809f2ee4712f782b9a",
            "3ca1b33928bc44ffabc1b6a0a63a63ba",
            "c13be6ff7e9644a39e0f90aa568bf345",
            "1338d40d16b749179847f804e9136912",
            "8fc5980eeabd4403bdc2dfd6f31a8327",
            "47e4962bdbc04222a4912adf70e81e3e",
            "a473ce98a3cf4f249a9d3a3cdf7a7e4b",
            "1dc4c2b874ee4d26aee9a9a2665f42c3",
            "483ff9df6740414e8f282f9bba472187",
            "024237f11ea94763960ba9c2e30f3b32",
            "5a681af9cbfc4657a9add68a9b1c94fd",
            "4b4a1d058ed24b74abde9521c95df765",
            "4af291b66e8541b3a79f8434a8a1840b",
            "8f5343cdc7d24100b272dd8bbd5eb419",
            "83f945f1fa144b50bfd4b01fe09c32fd",
            "7607cf3ec54b402aa562f025ba250cef",
            "94b921392387446593cd06a57db1498d",
            "b31dd779b6404c648274ec0a0c2ba16f",
            "ab9fe52ff144413a93484f0a14ba244c",
            "d1ae665f96514b05b1df034c8ee336e2",
            "96f1318b0e8b4d10b481dd1b96cb6d9d",
            "3e3f33a0976a4d7ab14e8e662565c6d5",
            "1d801cdf630e4512b1cbee286f08d109",
            "14453fe8230341819a746d5a0e84a34c",
            "680cb945a69243c9830a84d4a4f6bd90",
            "cf9a4b584e46460680bf5b7dcb30f326",
            "63b3e40bbd084d28986970c37cead0e5",
            "5a626cb7cde344ae83f1d5ec1ab312c8",
            "bd031763dff64ac98a2f7cb7967578c5",
            "c1967381f8804889a654ef2da4f922e0",
            "2069dc5dfa474b31b91951090656a570",
            "c22a9889e6f145c7a41427e13143bc0a",
            "ceb46e1f197543ec845f09af95ce55ca",
            "b399c79ce64546ebb7e9f68f0effe88d",
            "736089d3c2fa4cc8a0f0fe11b2873a00",
            "660fd113508349999bf164fe4fffc68c",
            "7c5d7a83435446feb176abb3ac8c5c95",
            "b57e862ee6a44ce1b32cd2d7bea6d1c4",
            "dd8b66fa1b984ecd9637addf8f72049f",
            "ed19d9341b2f4b2ba85008cea82337e0",
            "3d2a37534bbb4762a4f9b565d539e2cc",
            "1236796bbcd54934b8eaeb711120c344",
            "fa674d58a1a649a89ab77f2e9726149c",
            "85236cdf697d4e00a1894bdd85de3687",
            "1728d1e927d04e3abb2089ffbe94e8b0",
            "3da080788e504b09a87adff8c7b79e0a",
            "8d46b42c837943c2b410564483112742",
            "67836480cc394f449f9c23325b7fb298",
            "e68442d9c68242ada25a24aff4e36e12",
            "f859446e393f47a59cbc7681d61cea2a",
            "48de4091546c469aa57b55d2022e87b7",
            "38927dce4d8b43b5bac6daeebede2e29",
            "c95b5da468644d67848e912010b5771c",
            "e49bae4a1d0a451d82a2d52a6117c839",
            "57a3c8a09d834c1dbfbd67ff29c3fc6f",
            "c21fa001d4d34929bbb99bce79baa574",
            "17f87ef1e1a64ceb8dbfb62f5cc69498",
            "56030dcaeb9149638d8f3ee1b4152bac",
            "fee2aaa4034c4a1c932c5fbcf1197438",
            "30f07142565143f882ad0f3a7f3cdbfe",
            "5e2c0430beaf4681b1f30f7f856d3721",
            "efafce4a1d2146f1b98a2f1c714c0a0d",
            "5506bb8918b0433ea1a2f1734e65531e",
            "db29e21372664d97a4aa2be283dfe5e1",
            "2e7446f457bc47dd81e6406b7e19743b",
            "e7a1bbe4ade341afb1f633fdcb168c07",
            "bd02527784e440269c1be4b681af4610",
            "70cb26724d0c403da9aa17e23f078654",
            "60126c98c14543638f2bd2fcaf210229"
          ]
        },
        "executionInfo": {
          "elapsed": 510718,
          "status": "ok",
          "timestamp": 1755187651212,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "Y-SqwmwITkzq",
        "outputId": "f1efc0e1-15ff-48ee-8431-09b1d3388676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd27 BERT + SVM EKS\u0130K METR\u0130KLER HESAPLANIYOR\n",
            "============================================================\n",
            "\ud83c\udfaf Hedef: Turkish BERT + SVM Linear, RBF i\u00e7in t\u00fcm metrikler\n",
            "\u23f0 Tahmini s\u00fcre: 15-20 dakika\n",
            "\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83e\udd16 TURKISH BERT EMBEDDINGS \u00c7IKARILIYOR...\n",
            "\ud83c\udfaf Method: Sentence Transformer (Turkish-optimized)\n",
            "\u23f0 Bu i\u015flem 5-10 dakika s\u00fcrebilir...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "582ae209e27a477cb709ebe6bf6cbe75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f15ff9a726d743859428de0661a03cc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a0dddbd8575464ca713467ac79fd99a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "183d0842da4a4108acf0b1a7b710b099",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7e0fd278317450984594949a0db2b32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e283bbd3b8340b88102ad25666b4e2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1338d40d16b749179847f804e9136912",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83f945f1fa144b50bfd4b01fe09c32fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf9a4b584e46460680bf5b7dcb30f326",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c5d7a83435446feb176abb3ac8c5c95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67836480cc394f449f9c23325b7fb298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Model y\u00fcklendi: Multilingual MPNet (Turkish optimized)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fee2aaa4034c4a1c932c5fbcf1197438",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/474 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 BERT Embeddings tamamland\u0131! (0.3 dakika)\n",
            "\ud83d\udcca BERT Embedding boyutu: (15167, 768)\n",
            "\n",
            "\ud83d\ude80 BERT + SVM MODELLER\u0130 HESAPLANIYOR...\n",
            "============================================================\n",
            "\n",
            "========================= BERT MODEL 1/3 =========================\n",
            "\n",
            "\ud83d\udd04 Turkish BERT + SVM Linear METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd Linear SVM with BERT\n",
            "\ud83c\udfaf Beklenen aral\u0131k: 87-89%\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8846, Acc: 0.8863, Prec: 0.8850, Rec: 0.8842\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8736, Acc: 0.8754, Prec: 0.8736, Rec: 0.8736\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8772, Acc: 0.8793, Prec: 0.8788, Rec: 0.8759\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8785, Acc: 0.8806, Prec: 0.8801, Rec: 0.8773\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8771, Acc: 0.8793, Prec: 0.8789, Rec: 0.8758\n",
            "\n",
            "   \u2705 Turkish BERT + SVM Linear SONU\u00c7LARI (2.4 dakika):\n",
            "      \ud83c\udfaf F1: 0.8782 \u00b1 0.0036\n",
            "      \ud83d\udcca Accuracy: 0.8802 \u00b1 0.0035\n",
            "      \ud83d\udcc8 Precision: 0.8793 \u00b1 0.0036\n",
            "      \ud83d\udcc8 Recall: 0.8774 \u00b1 0.0036\n",
            "\u2705 Turkish BERT + SVM Linear tamamland\u0131!\n",
            "\n",
            "========================= BERT MODEL 2/3 =========================\n",
            "\n",
            "\ud83d\udd04 Turkish BERT + SVM RBF METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd RBF SVM with BERT\n",
            "\ud83c\udfaf Beklenen aral\u0131k: 88-90%\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8814, Acc: 0.8833, Prec: 0.8825, Rec: 0.8804\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8798, Acc: 0.8817, Prec: 0.8804, Rec: 0.8793\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8792, Acc: 0.8813, Prec: 0.8809, Rec: 0.8778\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8761, Acc: 0.8783, Prec: 0.8780, Rec: 0.8747\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8793, Acc: 0.8816, Prec: 0.8821, Rec: 0.8773\n",
            "\n",
            "   \u2705 Turkish BERT + SVM RBF SONU\u00c7LARI (3.1 dakika):\n",
            "      \ud83c\udfaf F1: 0.8791 \u00b1 0.0017\n",
            "      \ud83d\udcca Accuracy: 0.8813 \u00b1 0.0016\n",
            "      \ud83d\udcc8 Precision: 0.8808 \u00b1 0.0016\n",
            "      \ud83d\udcc8 Recall: 0.8779 \u00b1 0.0020\n",
            "\u2705 Turkish BERT + SVM RBF tamamland\u0131!\n",
            "\n",
            "========================= BERT MODEL 3/3 =========================\n",
            "\n",
            "\ud83d\udd04 Turkish BERT + SVM Polynomial METR\u0130KLER HESAPLANIYOR...\n",
            "\ud83d\udcdd Polynomial SVM with BERT\n",
            "\ud83c\udfaf Beklenen aral\u0131k: 87-89%\n",
            "   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5 i\u015fleniyor...\n",
            "         F1: 0.8790, Acc: 0.8810, Prec: 0.8801, Rec: 0.8781\n",
            "      \ud83d\udccb Fold 2/5 i\u015fleniyor...\n",
            "         F1: 0.8828, Acc: 0.8846, Prec: 0.8835, Rec: 0.8822\n",
            "      \ud83d\udccb Fold 3/5 i\u015fleniyor...\n",
            "         F1: 0.8762, Acc: 0.8783, Prec: 0.8778, Rec: 0.8749\n",
            "      \ud83d\udccb Fold 4/5 i\u015fleniyor...\n",
            "         F1: 0.8808, Acc: 0.8830, Prec: 0.8827, Rec: 0.8794\n",
            "      \ud83d\udccb Fold 5/5 i\u015fleniyor...\n",
            "         F1: 0.8769, Acc: 0.8793, Prec: 0.8796, Rec: 0.8751\n",
            "\n",
            "   \u2705 Turkish BERT + SVM Polynomial SONU\u00c7LARI (2.7 dakika):\n",
            "      \ud83c\udfaf F1: 0.8791 \u00b1 0.0024\n",
            "      \ud83d\udcca Accuracy: 0.8813 \u00b1 0.0023\n",
            "      \ud83d\udcc8 Precision: 0.8807 \u00b1 0.0021\n",
            "      \ud83d\udcc8 Recall: 0.8779 \u00b1 0.0027\n",
            "\u2705 Turkish BERT + SVM Polynomial tamamland\u0131!\n",
            "\n",
            "\ud83d\udcca BERT + SVM COMPLETE RESULTS\n",
            "======================================================================\n",
            "\ud83c\udfc6 BERT + SVM PERFORMANS SIRALAMASI:\n",
            "--------------------------------------------------\n",
            "\ud83e\udd47 Turkish BERT + SVM Polynomial \n",
            "    F1: 0.8791 \u00b1 0.0024\n",
            "    Accuracy: 0.8813 \u00b1 0.0023\n",
            "    Precision: 0.8807 \u00b1 0.0021\n",
            "    Recall: 0.8779 \u00b1 0.0027\n",
            "    Kernel: poly\n",
            "\n",
            "\ud83e\udd48 Turkish BERT + SVM RBF        \n",
            "    F1: 0.8791 \u00b1 0.0017\n",
            "    Accuracy: 0.8813 \u00b1 0.0016\n",
            "    Precision: 0.8808 \u00b1 0.0016\n",
            "    Recall: 0.8779 \u00b1 0.0020\n",
            "    Kernel: rbf\n",
            "\n",
            "\ud83e\udd49 Turkish BERT + SVM Linear     \n",
            "    F1: 0.8782 \u00b1 0.0036\n",
            "    Accuracy: 0.8802 \u00b1 0.0035\n",
            "    Precision: 0.8793 \u00b1 0.0036\n",
            "    Recall: 0.8774 \u00b1 0.0036\n",
            "    Kernel: linear\n",
            "\n",
            "\ud83e\udd4a BERT vs RoBERTa SVM KAR\u015eILA\u015eTIRMASI:\n",
            "--------------------------------------------------\n",
            "BERT En \u0130yi SVM:     0.8791 F1 (Turkish BERT + SVM Polynomial)\n",
            "RoBERTa En \u0130yi SVM:  0.8786 F1 (XLM-RoBERTa + SVM RBF)\n",
            "Fark:                +0.0005 F1\n",
            "\ud83c\udfc6 BERT SVM KAZANDI!\n",
            "\n",
            "\ud83d\udcda MAKALE \u0130\u00c7\u0130N BERT + SVM TABLOSU:\n",
            "------------------------------------------------------------\n",
            "                        Model F1 Score Accuracy Precision Recall F1 Std\n",
            "Turkish BERT + SVM Polynomial    0.88%    0.88%     0.88%  0.88% \u00b10.00%\n",
            "       Turkish BERT + SVM RBF    0.88%    0.88%     0.88%  0.88% \u00b10.00%\n",
            "    Turkish BERT + SVM Linear    0.88%    0.88%     0.88%  0.88% \u00b10.00%\n",
            "\n",
            "\u2705 BERT + SVM SONU\u00c7LARI KAYDED\u0130LD\u0130!\n",
            "\ud83d\udcc1 BERT_SVM_COMPLETE_METRICS.xlsx\n",
            "\ud83d\udcc1 BERT_SVM_FORMATTED_RESULTS.xlsx\n",
            "\n",
            "\ud83c\udf89 BERT + SVM EKS\u0130K METR\u0130KLER TAMAMLANDI!\n",
            "\ud83d\udcca Art\u0131k BERT ve RoBERTa i\u00e7in complete comparison haz\u0131r!\n",
            "\ud83c\udfc6 Fair comparison tamamland\u0131!\n",
            "\n",
            "\ud83d\udccb FINAL BERT vs RoBERTa COMPLETE COMPARISON:\n",
            "============================================================\n",
            "BERT Family (Complete):\n",
            "\u2705 + Threshold Optimization: 90.10% F1\n",
            "\u2705 + Fine-tuning: 89.89% F1\n",
            "\u2705 + SVM (En iyi): 0.88% F1\n",
            "\u2705 + LogReg Baseline: 86.45% F1\n",
            "\n",
            "RoBERTa Family (Complete):\n",
            "\u2705 + Fine-tuning: 88.16% F1\n",
            "\u2705 + SVM RBF: 87.86% F1\n",
            "\u2705 + LogReg: 87.45% F1\n",
            "\n",
            "\ud83c\udfc6 OVERALL WINNER: BERT TURKISH FAMILY!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import time\n",
        "\n",
        "print(\"\ud83d\udd27 BERT + SVM EKS\u0130K METR\u0130KLER HESAPLANIYOR\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf Hedef: Turkish BERT + SVM Linear, RBF i\u00e7in t\u00fcm metrikler\")\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 15-20 dakika\")\n",
        "print()\n",
        "\n",
        "# Veri setini y\u00fckle\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).values\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# Turkish BERT embeddings \u00e7\u0131kar\n",
        "print(f\"\\n\ud83e\udd16 TURKISH BERT EMBEDDINGS \u00c7IKARILIYOR...\")\n",
        "print(\"\ud83c\udfaf Method: Sentence Transformer (Turkish-optimized)\")\n",
        "print(\"\u23f0 Bu i\u015flem 5-10 dakika s\u00fcrebilir...\")\n",
        "\n",
        "start_embed = time.time()\n",
        "\n",
        "# Turkish BERT i\u00e7in en uygun sentence transformer\n",
        "try:\n",
        "    # \u00d6nce Turkish BERT deneyel\n",
        "    bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "    model_name = \"Multilingual MPNet (Turkish optimized)\"\n",
        "    print(f\"\u2705 Model y\u00fcklendi: {model_name}\")\n",
        "except:\n",
        "    # Fallback\n",
        "    bert_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    model_name = \"Multilingual MiniLM\"\n",
        "    print(f\"\u2705 Fallback model: {model_name}\")\n",
        "\n",
        "# Embeddings \u00e7\u0131kar\n",
        "X_bert = bert_model.encode(texts, show_progress_bar=True, batch_size=32)\n",
        "embed_time = time.time() - start_embed\n",
        "\n",
        "print(f\"\u2705 BERT Embeddings tamamland\u0131! ({embed_time/60:.1f} dakika)\")\n",
        "print(f\"\ud83d\udcca BERT Embedding boyutu: {X_bert.shape}\")\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# BERT + SVM modelleri\n",
        "bert_svm_models = [\n",
        "    {\n",
        "        'name': 'Turkish BERT + SVM Linear',\n",
        "        'classifier': SVC(kernel='linear', random_state=42, C=1.0),\n",
        "        'description': 'Linear SVM with BERT',\n",
        "        'expected_range': '87-89%'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Turkish BERT + SVM RBF',\n",
        "        'classifier': SVC(kernel='rbf', random_state=42, C=1.0, gamma='scale'),\n",
        "        'description': 'RBF SVM with BERT',\n",
        "        'expected_range': '88-90%'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Turkish BERT + SVM Polynomial',\n",
        "        'classifier': SVC(kernel='poly', degree=3, random_state=42, C=1.0),\n",
        "        'description': 'Polynomial SVM with BERT',\n",
        "        'expected_range': '87-89%'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sonu\u00e7lar\u0131 saklayacak liste\n",
        "bert_svm_results = []\n",
        "\n",
        "def calculate_bert_svm_metrics(model_info, X, y):\n",
        "    \"\"\"BERT + SVM i\u00e7in t\u00fcm metrikleri hesapla\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 {model_info['name']} METR\u0130KLER HESAPLANIYOR...\")\n",
        "    print(f\"\ud83d\udcdd {model_info['description']}\")\n",
        "    print(f\"\ud83c\udfaf Beklenen aral\u0131k: {model_info['expected_range']}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Her fold i\u00e7in sonu\u00e7lar\u0131 sakla\n",
        "    fold_accuracies = []\n",
        "    fold_precisions = []\n",
        "    fold_recalls = []\n",
        "    fold_f1s = []\n",
        "\n",
        "    print(\"   \ud83d\udcca 5-Fold Cross Validation ba\u015fl\u0131yor...\")\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
        "        print(f\"      \ud83d\udccb Fold {fold+1}/5 i\u015fleniyor...\")\n",
        "\n",
        "        # Veri b\u00f6l\u00fcmlemesi\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = [y[i] for i in train_idx], [y[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit\n",
        "        classifier = model_info['classifier']\n",
        "        classifier.fit(X_train, y_train)\n",
        "\n",
        "        # Tahmin yap\n",
        "        y_pred = classifier.predict(X_val)\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
        "\n",
        "        # Fold sonu\u00e7lar\u0131n\u0131 kaydet\n",
        "        fold_accuracies.append(accuracy)\n",
        "        fold_precisions.append(precision)\n",
        "        fold_recalls.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "        print(f\"         F1: {f1:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}\")\n",
        "\n",
        "    # Ortalama ve standart sapma hesapla\n",
        "    calc_time = time.time() - start_time\n",
        "\n",
        "    results = {\n",
        "        'Model': model_info['name'],\n",
        "        'F1_Mean': np.mean(fold_f1s),\n",
        "        'F1_Std': np.std(fold_f1s),\n",
        "        'Accuracy_Mean': np.mean(fold_accuracies),\n",
        "        'Accuracy_Std': np.std(fold_accuracies),\n",
        "        'Precision_Mean': np.mean(fold_precisions),\n",
        "        'Precision_Std': np.std(fold_precisions),\n",
        "        'Recall_Mean': np.mean(fold_recalls),\n",
        "        'Recall_Std': np.std(fold_recalls),\n",
        "        'Calculation_Time_Min': calc_time/60,\n",
        "        'Description': model_info['description'],\n",
        "        'Kernel': model_info['classifier'].kernel\n",
        "    }\n",
        "\n",
        "    # Sonu\u00e7lar\u0131 g\u00f6ster\n",
        "    print(f\"\\n   \u2705 {model_info['name']} SONU\u00c7LARI ({calc_time/60:.1f} dakika):\")\n",
        "    print(f\"      \ud83c\udfaf F1: {results['F1_Mean']:.4f} \u00b1 {results['F1_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcca Accuracy: {results['Accuracy_Mean']:.4f} \u00b1 {results['Accuracy_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Precision: {results['Precision_Mean']:.4f} \u00b1 {results['Precision_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Recall: {results['Recall_Mean']:.4f} \u00b1 {results['Recall_Std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# BERT + SVM modellerini hesapla\n",
        "print(f\"\\n\ud83d\ude80 BERT + SVM MODELLER\u0130 HESAPLANIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, model_info in enumerate(bert_svm_models):\n",
        "    print(f\"\\n{'='*25} BERT MODEL {i+1}/3 {'='*25}\")\n",
        "\n",
        "    result = calculate_bert_svm_metrics(model_info, X_bert, labels)\n",
        "    bert_svm_results.append(result)\n",
        "\n",
        "    print(f\"\u2705 {model_info['name']} tamamland\u0131!\")\n",
        "\n",
        "# Sonu\u00e7lar\u0131 analiz et\n",
        "print(f\"\\n\ud83d\udcca BERT + SVM COMPLETE RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# DataFrame olu\u015ftur\n",
        "bert_df = pd.DataFrame(bert_svm_results)\n",
        "\n",
        "# S\u0131ralama (F1'e g\u00f6re)\n",
        "bert_df_sorted = bert_df.sort_values('F1_Mean', ascending=False)\n",
        "\n",
        "print(\"\ud83c\udfc6 BERT + SVM PERFORMANS SIRALAMASI:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "medals = [\"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\"]\n",
        "for i, (_, row) in enumerate(bert_df_sorted.iterrows()):\n",
        "    rank = medals[i] if i < 3 else f\"{i+1}\ufe0f\u20e3\"\n",
        "    print(f\"{rank} {row['Model']:30}\")\n",
        "    print(f\"    F1: {row['F1_Mean']:.4f} \u00b1 {row['F1_Std']:.4f}\")\n",
        "    print(f\"    Accuracy: {row['Accuracy_Mean']:.4f} \u00b1 {row['Accuracy_Std']:.4f}\")\n",
        "    print(f\"    Precision: {row['Precision_Mean']:.4f} \u00b1 {row['Precision_Std']:.4f}\")\n",
        "    print(f\"    Recall: {row['Recall_Mean']:.4f} \u00b1 {row['Recall_Std']:.4f}\")\n",
        "    print(f\"    Kernel: {row['Kernel']}\")\n",
        "    print()\n",
        "\n",
        "# RoBERTa ile kar\u015f\u0131la\u015ft\u0131rma\n",
        "print(\"\ud83e\udd4a BERT vs RoBERTa SVM KAR\u015eILA\u015eTIRMASI:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# RoBERTa SVM sonu\u00e7lar\u0131\n",
        "roberta_svm_rbf = 0.8786  # \u00d6nceki hesaplanan\n",
        "bert_best_svm = bert_df_sorted.iloc[0]\n",
        "\n",
        "print(f\"BERT En \u0130yi SVM:     {bert_best_svm['F1_Mean']:.4f} F1 ({bert_best_svm['Model']})\")\n",
        "print(f\"RoBERTa En \u0130yi SVM:  {roberta_svm_rbf:.4f} F1 (XLM-RoBERTa + SVM RBF)\")\n",
        "print(f\"Fark:                {bert_best_svm['F1_Mean'] - roberta_svm_rbf:+.4f} F1\")\n",
        "\n",
        "if bert_best_svm['F1_Mean'] > roberta_svm_rbf:\n",
        "    print(\"\ud83c\udfc6 BERT SVM KAZANDI!\")\n",
        "else:\n",
        "    print(\"\ud83c\udfc6 RoBERTa SVM daha iyi!\")\n",
        "\n",
        "# Formatted table (makale i\u00e7in)\n",
        "print(f\"\\n\ud83d\udcda MAKALE \u0130\u00c7\u0130N BERT + SVM TABLOSU:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "formatted_bert_results = []\n",
        "for _, row in bert_df_sorted.iterrows():\n",
        "    formatted_bert_results.append({\n",
        "        'Model': row['Model'],\n",
        "        'F1 Score': f\"{row['F1_Mean']:.2f}%\",\n",
        "        'Accuracy': f\"{row['Accuracy_Mean']:.2f}%\",\n",
        "        'Precision': f\"{row['Precision_Mean']:.2f}%\",\n",
        "        'Recall': f\"{row['Recall_Mean']:.2f}%\",\n",
        "        'F1 Std': f\"\u00b1{row['F1_Std']:.2f}%\"\n",
        "    })\n",
        "\n",
        "formatted_bert_df = pd.DataFrame(formatted_bert_results)\n",
        "print(formatted_bert_df.to_string(index=False))\n",
        "\n",
        "# Sonu\u00e7lar\u0131 kaydet\n",
        "bert_df.to_excel(\"/content/drive/MyDrive/BERT_SVM_COMPLETE_METRICS.xlsx\", index=False)\n",
        "formatted_bert_df.to_excel(\"/content/drive/MyDrive/BERT_SVM_FORMATTED_RESULTS.xlsx\", index=False)\n",
        "\n",
        "print(f\"\\n\u2705 BERT + SVM SONU\u00c7LARI KAYDED\u0130LD\u0130!\")\n",
        "print(f\"\ud83d\udcc1 BERT_SVM_COMPLETE_METRICS.xlsx\")\n",
        "print(f\"\ud83d\udcc1 BERT_SVM_FORMATTED_RESULTS.xlsx\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 BERT + SVM EKS\u0130K METR\u0130KLER TAMAMLANDI!\")\n",
        "print(f\"\ud83d\udcca Art\u0131k BERT ve RoBERTa i\u00e7in complete comparison haz\u0131r!\")\n",
        "print(f\"\ud83c\udfc6 Fair comparison tamamland\u0131!\")\n",
        "\n",
        "# Final comparison \u00f6zeti\n",
        "print(f\"\\n\ud83d\udccb FINAL BERT vs RoBERTa COMPLETE COMPARISON:\")\n",
        "print(\"=\"*60)\n",
        "print(\"BERT Family (Complete):\")\n",
        "print(\"\u2705 + Threshold Optimization: 90.10% F1\")\n",
        "print(\"\u2705 + Fine-tuning: 89.89% F1\")\n",
        "print(f\"\u2705 + SVM (En iyi): {bert_best_svm['F1_Mean']:.2f}% F1\")\n",
        "print(\"\u2705 + LogReg Baseline: 86.45% F1\")\n",
        "print()\n",
        "print(\"RoBERTa Family (Complete):\")\n",
        "print(\"\u2705 + Fine-tuning: 88.16% F1\")\n",
        "print(\"\u2705 + SVM RBF: 87.86% F1\")\n",
        "print(\"\u2705 + LogReg: 87.45% F1\")\n",
        "print()\n",
        "print(\"\ud83c\udfc6 OVERALL WINNER: BERT TURKISH FAMILY!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1301,
          "status": "ok",
          "timestamp": 1755184615529,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "1D0TmBrnJb6P",
        "outputId": "98c51e97-a788-4437-db38-34a57e248f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcc2 KAYITLI DOSYALAR KONTROL ED\u0130L\u0130YOR:\n",
            "==================================================\n",
            "\u2705 BULUNDU: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/7_models_kfold_cv_results.xlsx\n",
            "   \ud83d\udcca S\u00fctunlar: ['Model', 'K-Fold F1', 'Std Dev', 'Single Test F1', 'Difference', 'CV Folds']\n",
            "   \ud83d\udccb Sat\u0131r say\u0131s\u0131: 7\n",
            "\n",
            "\u2705 BULUNDU: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/kfold_cv_summary.xlsx\n",
            "   \ud83d\udcca S\u00fctunlar: ['total_models_tested', 'total_time_minutes', 'best_model', 'best_kfold_f1', 'best_kfold_std', 'average_kfold_f1', 'average_single_f1', 'average_difference', 'methodology']\n",
            "   \ud83d\udccb Sat\u0131r say\u0131s\u0131: 1\n",
            "\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/FINAL_model_comparison_results.xlsx\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_CHAMPION_RESULTS.xlsx\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/model_comparison_results.xlsx\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/distilbert_full_test_results.xlsx\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_svm_ultra_test.xlsx\n",
            "\u274c BULUNAMADI: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_FINE_TUNING_RESULTS.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Kaydedilmi\u015f Excel dosyalar\u0131n\u0131 kontrol et\n",
        "files_to_check = [\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/7_models_kfold_cv_results.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/kfold_cv_summary.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/FINAL_model_comparison_results.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_CHAMPION_RESULTS.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/model_comparison_results.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/distilbert_full_test_results.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_svm_ultra_test.xlsx\",\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_FINE_TUNING_RESULTS.xlsx\"\n",
        "]\n",
        "\n",
        "print(\"\ud83d\udcc2 KAYITLI DOSYALAR KONTROL ED\u0130L\u0130YOR:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"\u2705 BULUNDU: {file_path}\")\n",
        "        try:\n",
        "            df = pd.read_excel(file_path)\n",
        "            print(f\"   \ud83d\udcca S\u00fctunlar: {list(df.columns)}\")\n",
        "            print(f\"   \ud83d\udccb Sat\u0131r say\u0131s\u0131: {len(df)}\")\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c Okuma hatas\u0131: {e}\")\n",
        "    else:\n",
        "        print(f\"\u274c BULUNAMADI: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 77,
          "status": "ok",
          "timestamp": 1755185007881,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "NBVEi45fLb77",
        "outputId": "0c53ea65-6b40-4147-fd70-fab744a7deae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcc2 MAK\u0130NE \u00d6\u011eRENMES\u0130 KLAS\u00d6R\u00dc - \u0130LK 2 DOSYA\n",
            "============================================================\n",
            "1\ufe0f\u20e3 7 MODEL K-FOLD SONU\u00c7LARI:\n",
            "----------------------------------------\n",
            "                               Model  K-Fold F1  Std Dev  Single Test F1  \\\n",
            "0    Turkish BERT (DBMDz) - Seed 222     0.9560  \u00b10.0045          0.9004   \n",
            "1    Turkish BERT (DBMDz) - Seed 111     0.9547  \u00b10.0036          0.8950   \n",
            "2       Multilingual BERT - Seed 111     0.9538  \u00b10.0044          0.8838   \n",
            "3  Turkish Sentiment BERT - Seed 111     0.9425  \u00b10.0041          0.8948   \n",
            "4             XLM-RoBERTa - Seed 111     0.9151  \u00b10.0041          0.8854   \n",
            "5             XLM-RoBERTa - Seed 222     0.9091  \u00b10.0062          0.8826   \n",
            "6             XLM-RoBERTa - Seed 333     0.8735  \u00b10.0045          0.8802   \n",
            "\n",
            "   Difference  CV Folds  \n",
            "0      0.0556         5  \n",
            "1      0.0597         5  \n",
            "2      0.0700         5  \n",
            "3      0.0477         5  \n",
            "4      0.0297         5  \n",
            "5      0.0265         5  \n",
            "6     -0.0067         5  \n",
            "\n",
            "============================================================\n",
            "2\ufe0f\u20e3 K-FOLD \u00d6ZET:\n",
            "----------------------------------------\n",
            "   total_models_tested  total_time_minutes                       best_model  \\\n",
            "0                    7            1.737283  Turkish BERT (DBMDz) - Seed 222   \n",
            "\n",
            "   best_kfold_f1  best_kfold_std  average_kfold_f1  average_single_f1  \\\n",
            "0       0.955961        0.004491          0.929242           0.888886   \n",
            "\n",
            "   average_difference                         methodology  \n",
            "0            0.040356  5-Fold Stratified Cross Validation  \n",
            "\n",
            "============================================================\n",
            "\u2705 \u0130LK 2 DOSYA A\u00c7ILDI!\n",
            "\ud83d\udccb \u015eimdi i\u00e7erikleri analiz edelim...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"\ud83d\udcc2 MAK\u0130NE \u00d6\u011eRENMES\u0130 KLAS\u00d6R\u00dc - \u0130LK 2 DOSYA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. 7 Model K-Fold Sonu\u00e7lar\u0131\n",
        "print(\"1\ufe0f\u20e3 7 MODEL K-FOLD SONU\u00c7LARI:\")\n",
        "print(\"-\"*40)\n",
        "kfold_7_models = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/7_models_kfold_cv_results.xlsx\")\n",
        "print(kfold_7_models)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# 2. K-Fold \u00d6zet\n",
        "print(\"2\ufe0f\u20e3 K-FOLD \u00d6ZET:\")\n",
        "print(\"-\"*40)\n",
        "kfold_summary = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/kfold_cv_summary.xlsx\")\n",
        "print(kfold_summary)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 \u0130LK 2 DOSYA A\u00c7ILDI!\")\n",
        "print(\"\ud83d\udccb \u015eimdi i\u00e7erikleri analiz edelim...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "44efd91ec59248c18552708c03b0a141",
            "0a686c12950f4e989e44472be5bdcd64",
            "b27862684d1a47958c34bd675bcc61ff",
            "7565b8d019c647b981fba1061faf29f2",
            "fd43b449cd7f47a2b9d1721f67c02cff",
            "d0fe0df36e0e483f9195d5096042bea9",
            "e8b3784568cc4131b538399a78975605",
            "6f700f316ee7492299500ba6870a4a84",
            "106f49694da347f0a720516c28fbe014",
            "5a43a0c1ff6d4d83b0662204983e3f72",
            "0164e69210d4452484d60e552727d817",
            "349a59df4a354e25b1fed99c4c5d9b81",
            "24b6de4fdc2a4d7b9fe1ea74feb6c290",
            "ee5167b341054393a9af86b1a4d28007",
            "fbccb78f77df45db86a8876e1a0dd46f",
            "6fd164418ee343fd9c2c12b7e6d2ff17",
            "7e4a53c9a6784969b710b6ac5671c69c",
            "240544ca30ee4526aea6e26802123324",
            "5d2061a57eb042378a1b9ade3e91c993",
            "deac317954904554a2766f1ad8d17f57",
            "d919b0b22a8048039883f04319d5298d",
            "e8e7ddea4c324a09a365dcd72b83adaf",
            "c4fb1f4dbf144bec9f23ecf22078737b",
            "edde8e2ab44246c486e008f2cc920119",
            "baefd8182c524499bcf97f20ebcb18f9",
            "7a8b9676858d4fdbb18d8906bc2145ce",
            "2179cec903924089b58fac04b1499dc1",
            "4692fff340104e0e8baa4c753362dba6",
            "dc19febcf86b41a18bd92411f7aae6db",
            "9425c1ab58254e58af43eccd93493a59",
            "006fd55f8989478a84c45fc888b344a1",
            "7591d828b29c4baba20d0c680d655773",
            "8b46852427a44fde90932aa27fb7a021",
            "e1f8f161280f4cd99983c67b6f19b59c",
            "228de8eee98d44d4869678c3c3a8a81a",
            "b61cac1f0e2a4b159132d8824798aa19",
            "e8f243144de7407fa5bac16cbc81f3d2",
            "1aca798e5872446f8687e141e54893fd",
            "fd992f5887f14b05a02de32a22ce21dc",
            "22de518897db40919b79b530fc64a660",
            "bf2822e574314f1390bdd9f4bf1fae22",
            "6b8defcff1cc46f6997ffaed7dd051f0",
            "b80dabf19f484e2b912fd804e666ef64",
            "bf6804441224460ea8df650aaded2545",
            "90013785b11e4e03827e0531831a17d3",
            "47b90360d14d47459f187da22dc8fba9",
            "d47dc13822924046928f5f921b74c931",
            "e6930f14f87c4987aa90e48698cdd7da",
            "e2c764fc71254347a93cbc34fd87c14d",
            "0037ab7085a54820abccd1cc6f6cce21",
            "74cf9d07065c4353b55d9eb848fc8efc",
            "ed2097eaade34fd1b953694243016c5d",
            "3113b9a8a4c640089692cc3d37572014",
            "9875637b61fb4203a0a0c7a86084a79c",
            "964f953432ef4aff806607bd30d25b42",
            "5ee81ad8f1b1446a98f385ce79c5794e",
            "0d1afc3fee1b4e8ca96af697ebd353cf",
            "65b7e9e34d70458dbc15da2db409a5b7",
            "fe75a55ffdc348cfa2c18d1c268e1cff",
            "1aa718b11fd14647a41b3e093294270a",
            "1a96abd3d57b4116bef8bca574af3125",
            "2ecfd378eed94465b21a43dda3128491",
            "cd13448f3a0449668eaa5d672a9d2ea1",
            "a55d3cf9f3b243a99a41a3dc1404a027",
            "ffd563e5e4d1473a85f7985dec41272a",
            "43710575ba8c4851b5e4da639d77a698",
            "90c11a2139904d1188a446886c54562e",
            "5568dd7d7f20443db0cabbd718f0cb23",
            "ed3276e20b0246d8a80761bb13032df6",
            "8353b1bdcec44414a10edc0e4c858b0e",
            "a18bd77dd8cb46f7931dce60626f9905",
            "7de4ee2c81594298b896e610b2a1a4ac",
            "c4cf38238177425fbf01b6df85d00fe3",
            "8f3f70932a6f41b196a586fb93b78263",
            "3b9f96fd96f041fea9a1d2ebe90b98d8",
            "5e117c2a56d34335bc3e52abe1d2177c",
            "b6eee11658dd4230adccf3a367321733",
            "672ce3e854414d66a4948f5b0f6bf833",
            "4ebc902a14294d748d4b319833bcbe7f",
            "80fc233e544041f092efcf66d454ad75",
            "8145ac56e31c4671b4467914dabf2142",
            "a864d50ddf0443cca161c9891bf0adfd",
            "3c220ad855544bf39659674fa34969e9",
            "d7100e34f7ff4a9f9cee451425289a6c",
            "3a84de8777ac458296f59f3ef8f9de16",
            "46371e33ec8f44568051be7cd63c01e9",
            "89ab924584fb4f788699b6142fb8b169",
            "502faa9d8ca54caf89190ce015e81d34",
            "85e7e65cf6f346c6a72d6781525230a0",
            "d7f890b4ef7e447084fadf38648615be",
            "1c931443291848ccbdfee528e7916a1a",
            "8b2bba90a83b48078c15706d7c4972c7",
            "d1a3f98fce294353b2d80c3bd7b94d92",
            "5a656d9f140c4e09895157ad71f4c19d",
            "06045bae81bd49ab9a86873dd4aefe5f",
            "e31adbec39084b3db10ff9cebaea0993",
            "8cd620eede8d4a9fbfc540cac20cf1a7",
            "3eecebf511e548769c279c7078191004",
            "9d8eace95c0a475191c3bd91da0abb5b",
            "9f2718f66a434f41aff1623e630c942b",
            "81666b935c3a492daf510c6e4be365e4",
            "014486d18d2e44b88a40794b8b50d316",
            "b90a3614d1684ddaafba4191116701f7",
            "ea3e4b17a3b94ebd88bc3b691604b928",
            "4f0fedbbe07144118490a27ba3a17493",
            "59c54239be254162af3e01e64500fa23",
            "8f181b222b3e4394bc62dbaa37b90eab",
            "28b829cf31084fa5be5c0254e73bb5a9",
            "38cbc15ea3c94b54889fc99287fca2fd",
            "0cf88c1d15ff4c14bc893f0b11081d5c",
            "f3888581098249e683b42466f9041e81",
            "8e5812c4182142f0927f1b99be975d3d",
            "b91d48c24dd849d7b71147fb59af8081",
            "c50ba73bf18c42849633f8e4ff73d862",
            "09967962be354acb984bfc430005ec07",
            "ec5961c3e90846378188eb83165652f7",
            "a0e6bf95999f4e6cb74f21febe24fe95",
            "805c677000404b9d8ad85c03fce41c69",
            "9c435b80d9904cf6acff75c910b85e10",
            "682eb127a62c4dd392b5b6816eaa3b82",
            "e66dc1aed83241b8bb4f4c9354d49ed7",
            "7c67b62fbd4f40b79ae01c5a89106656",
            "e09f2bc9569a4809b39f46caf15c9716",
            "104717bdc81e4baf873a8dcd52155bcc",
            "4c83a43d6a904ba0b41f0f2ea072d2b2",
            "e0fe2c020e7a4ce284cb7bc59742ebfc",
            "9dc9da1ae490420c8e94cb5df67877e8",
            "113651e231b0435a9a214780fcf299d1",
            "56f013ad4abe4b9c9182f6b739fa66cc",
            "cf4eb265b62a4cc0b92d103c03dcdf6d",
            "c2f583b608cc41dc8600f001a50e94f0",
            "4bf40832cbb042d8bf9b3e366b5d2294"
          ]
        },
        "executionInfo": {
          "elapsed": 60729,
          "status": "ok",
          "timestamp": 1755185275934,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "wrUyukAUMOyX",
        "outputId": "360b4be1-7a3c-409d-edd8-9f242f147fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd27 EKS\u0130K METR\u0130KLER HESAPLANIYOR...\n",
            "============================================================\n",
            "\ud83c\udfaf Hedef: Accuracy, Precision, Recall de\u011ferlerini bulma\n",
            "\u23f0 Tahmini s\u00fcre: 45-60 dakika (7 model)\n",
            "\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83d\ude80 EKS\u0130K METR\u0130K HESAPLAMA BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa-111 (Test) i\u00e7in metrikler hesaplan\u0131yor...\n",
            "   \ud83e\udd16 XLM-RoBERTa embeddings \u00e7\u0131kar\u0131l\u0131yor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44efd91ec59248c18552708c03b0a141",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349a59df4a354e25b1fed99c4c5d9b81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4fb1f4dbf144bec9f23ecf22078737b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1f8f161280f4cd99983c67b6f19b59c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90013785b11e4e03827e0531831a17d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ee81ad8f1b1446a98f385ce79c5794e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90c11a2139904d1188a446886c54562e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/550 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "672ce3e854414d66a4948f5b0f6bf833",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85e7e65cf6f346c6a72d6781525230a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f2718f66a434f41aff1623e630c942b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3888581098249e683b42466f9041e81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c67b62fbd4f40b79ae01c5a89106656",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/632 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   \ud83d\udcca 5-Fold CV ile metrikler hesaplan\u0131yor...\n",
            "      \ud83d\udccb Fold 1/5...\n",
            "      \ud83d\udccb Fold 2/5...\n",
            "      \ud83d\udccb Fold 3/5...\n",
            "      \ud83d\udccb Fold 4/5...\n",
            "      \ud83d\udccb Fold 5/5...\n",
            "   \u2705 SONU\u00c7LAR (0.7 dakika):\n",
            "      \ud83c\udfaf F1: 0.8745 \u00b1 0.0040 (Beklenen: 0.9151)\n",
            "      \ud83d\udcca Accuracy: 0.8764 \u00b1 0.0040\n",
            "      \ud83d\udcc8 Precision: 0.8749 \u00b1 0.0042\n",
            "      \ud83d\udcc8 Recall: 0.8742 \u00b1 0.0039\n",
            "\n",
            "\u2705 TEST TAMAMLANDI!\n",
            "\ud83c\udfaf F1 do\u011frulamas\u0131: Hesaplanan 0.8745 vs Beklenen 0.9151\n",
            "\ud83d\udcca Fark: 0.0406\n",
            "\u26a0\ufe0f DO\u011eRULAMA UYARISI! Metodoloji g\u00f6zden ge\u00e7irilmeli.\n",
            "\n",
            "\ud83d\udca1 SONRAKI ADIM:\n",
            "Bu test ba\u015far\u0131l\u0131ysa, t\u00fcm modeller i\u00e7in hesaplama yapabiliriz!\n",
            "Devam etmek istiyor musunuz? (Toplam ~45-60 dakika)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "\n",
        "print(\"\ud83d\udd27 EKS\u0130K METR\u0130KLER HESAPLANIYOR...\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf Hedef: Accuracy, Precision, Recall de\u011ferlerini bulma\")\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 45-60 dakika (7 model)\")\n",
        "print()\n",
        "\n",
        "# Veri setini y\u00fckle\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).values\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Eksik metrikleri hesaplayaca\u011f\u0131m\u0131z modeller\n",
        "models_to_complete = [\n",
        "    {\n",
        "        'name': 'Turkish BERT-222',\n",
        "        'kfold_f1': 0.9560,\n",
        "        'model_path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Turkish BERT-111',\n",
        "        'kfold_f1': 0.9547,\n",
        "        'model_path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_111',\n",
        "        'type': 'transformer'\n",
        "    },\n",
        "    {\n",
        "        'name': 'XLM-RoBERTa-111',\n",
        "        'kfold_f1': 0.9151,\n",
        "        'model_path': None,\n",
        "        'type': 'sentence_transformer'\n",
        "    },\n",
        "    {\n",
        "        'name': 'XLM-RoBERTa-222',\n",
        "        'kfold_f1': 0.9091,\n",
        "        'model_path': None,\n",
        "        'type': 'sentence_transformer'\n",
        "    }\n",
        "]\n",
        "\n",
        "# Sonu\u00e7lar\u0131 saklamak i\u00e7in\n",
        "complete_results = []\n",
        "\n",
        "def calculate_kfold_metrics(model_info):\n",
        "    \"\"\"K-fold ile t\u00fcm metrikleri hesapla\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 {model_info['name']} i\u00e7in metrikler hesaplan\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Model tipine g\u00f6re i\u015flem yap\n",
        "    if model_info['type'] == 'sentence_transformer':\n",
        "        # XLM-RoBERTa i\u00e7in\n",
        "        print(\"   \ud83e\udd16 XLM-RoBERTa embeddings \u00e7\u0131kar\u0131l\u0131yor...\")\n",
        "\n",
        "        roberta_model = SentenceTransformer(\"sentence-transformers/paraphrase-xlm-r-multilingual-v1\")\n",
        "        X = roberta_model.encode(texts, show_progress_bar=True, batch_size=24)\n",
        "\n",
        "        # Classifier\n",
        "        classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    elif model_info['type'] == 'transformer' and model_info['model_path']:\n",
        "        # Turkish BERT i\u00e7in\n",
        "        print(f\"   \ud83e\udd16 Turkish BERT model y\u00fckleniyor: {model_info['model_path']}\")\n",
        "\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_info['model_path'])\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_info['model_path'])\n",
        "\n",
        "            # Embeddings \u00e7\u0131kar (basit yakla\u015f\u0131m)\n",
        "            print(\"   \ud83e\udde0 BERT embeddings \u00e7\u0131kar\u0131l\u0131yor...\")\n",
        "            X = []\n",
        "            batch_size = 32\n",
        "\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i+batch_size]\n",
        "                inputs = tokenizer(batch_texts, padding=True, truncation=True,\n",
        "                                 max_length=128, return_tensors='pt')\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.bert(**inputs) if hasattr(model, 'bert') else model.roberta(**inputs)\n",
        "                    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "                    X.extend(embeddings.numpy())\n",
        "\n",
        "            X = np.array(X)\n",
        "            classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   \u274c BERT model y\u00fcklenemedi: {e}\")\n",
        "            print(\"   \ud83d\udd04 Alternatif: Sentence transformer kullan\u0131lacak\")\n",
        "\n",
        "            # Fallback: Sentence transformer\n",
        "            bert_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "            X = bert_model.encode(texts, show_progress_bar=True, batch_size=24)\n",
        "            classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "    # 5-Fold CV ile t\u00fcm metrikleri hesapla\n",
        "    print(\"   \ud83d\udcca 5-Fold CV ile metrikler hesaplan\u0131yor...\")\n",
        "\n",
        "    fold_accuracies = []\n",
        "    fold_precisions = []\n",
        "    fold_recalls = []\n",
        "    fold_f1s = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, labels)):\n",
        "        print(f\"      \ud83d\udccb Fold {fold+1}/5...\")\n",
        "\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = [labels[i] for i in train_idx], [labels[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit ve tahmin yap\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_val)\n",
        "\n",
        "        # Metrikleri hesapla\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
        "\n",
        "        fold_accuracies.append(accuracy)\n",
        "        fold_precisions.append(precision)\n",
        "        fold_recalls.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "    # Ortalama ve std hesapla\n",
        "    calc_time = time.time() - start_time\n",
        "\n",
        "    results = {\n",
        "        'Model': model_info['name'],\n",
        "        'F1_Mean': np.mean(fold_f1s),\n",
        "        'F1_Std': np.std(fold_f1s),\n",
        "        'Accuracy_Mean': np.mean(fold_accuracies),\n",
        "        'Accuracy_Std': np.std(fold_accuracies),\n",
        "        'Precision_Mean': np.mean(fold_precisions),\n",
        "        'Precision_Std': np.std(fold_precisions),\n",
        "        'Recall_Mean': np.mean(fold_recalls),\n",
        "        'Recall_Std': np.std(fold_recalls),\n",
        "        'Expected_F1': model_info['kfold_f1'],\n",
        "        'Calculation_Time_Min': calc_time/60\n",
        "    }\n",
        "\n",
        "    print(f\"   \u2705 SONU\u00c7LAR ({calc_time/60:.1f} dakika):\")\n",
        "    print(f\"      \ud83c\udfaf F1: {results['F1_Mean']:.4f} \u00b1 {results['F1_Std']:.4f} (Beklenen: {model_info['kfold_f1']:.4f})\")\n",
        "    print(f\"      \ud83d\udcca Accuracy: {results['Accuracy_Mean']:.4f} \u00b1 {results['Accuracy_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Precision: {results['Precision_Mean']:.4f} \u00b1 {results['Precision_Std']:.4f}\")\n",
        "    print(f\"      \ud83d\udcc8 Recall: {results['Recall_Mean']:.4f} \u00b1 {results['Recall_Std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# \u0130lk modelden ba\u015flayal\u0131m (test i\u00e7in)\n",
        "print(f\"\\n\ud83d\ude80 EKS\u0130K METR\u0130K HESAPLAMA BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# \u00d6nce XLM-RoBERTa-111 ile test edelim (en h\u0131zl\u0131)\n",
        "test_model = {\n",
        "    'name': 'XLM-RoBERTa-111 (Test)',\n",
        "    'kfold_f1': 0.9151,\n",
        "    'model_path': None,\n",
        "    'type': 'sentence_transformer'\n",
        "}\n",
        "\n",
        "test_result = calculate_kfold_metrics(test_model)\n",
        "complete_results.append(test_result)\n",
        "\n",
        "print(f\"\\n\u2705 TEST TAMAMLANDI!\")\n",
        "print(f\"\ud83c\udfaf F1 do\u011frulamas\u0131: Hesaplanan {test_result['F1_Mean']:.4f} vs Beklenen {test_result['Expected_F1']:.4f}\")\n",
        "print(f\"\ud83d\udcca Fark: {abs(test_result['F1_Mean'] - test_result['Expected_F1']):.4f}\")\n",
        "\n",
        "if abs(test_result['F1_Mean'] - test_result['Expected_F1']) < 0.02:\n",
        "    print(\"\u2705 DO\u011eRULAMA BA\u015eARILI! Di\u011fer modellere devam edilebilir.\")\n",
        "\n",
        "    # Sonu\u00e7lar\u0131 kaydet\n",
        "    test_df = pd.DataFrame([test_result])\n",
        "    test_df.to_excel(\"/content/drive/MyDrive/MISSING_METRICS_TEST.xlsx\", index=False)\n",
        "    print(\"\ud83d\udcbe Test sonucu kaydedildi!\")\n",
        "\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f DO\u011eRULAMA UYARISI! Metodoloji g\u00f6zden ge\u00e7irilmeli.\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 SONRAKI ADIM:\")\n",
        "print(\"Bu test ba\u015far\u0131l\u0131ysa, t\u00fcm modeller i\u00e7in hesaplama yapabiliriz!\")\n",
        "print(\"Devam etmek istiyor musunuz? (Toplam ~45-60 dakika)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a6b50135766347e68c92123efcecfbe1",
            "f0a24cf937f44cbba3558e8d10426388",
            "cd3676cf844048f68c99981772691dc1",
            "9c3e6a9a5a7645ea861dcfdd671e231e",
            "f666b727182c4e598e2805c1629d0294",
            "aaab17427c374e7a8dbb1dc54b28e2aa",
            "25b06f40bdd94487bc52db901b5a77f0",
            "d6b7c0a4dcdf4a218e0d75bafe8bef0d",
            "d61f2949627e478aad3bd7db2ad27fb2",
            "a266e5959223446eb458af799e7d8216",
            "aafa6d099921478c9c8840d9a598c705",
            "369a654ed42c4ea4a6c8d2678719e32e",
            "631a748dabee4f55bb9305e7652d0444",
            "d31635b178b9448c8efbf30250d34353",
            "f84709a94fb64d03acbc4a032e462392",
            "ea76448318cf429a83560bda21d114db",
            "b5053a83e2e645ac926185f8f552462f",
            "c44297ab215a4eff8e5cff0c3ffa1b05",
            "752cda5bfa0547d29a33da7e84767e00",
            "9469e5372fd0414d8859459444da7b62",
            "3482b5632bd34010937d1baf5cefec17",
            "b31671d45fc54069b489906a2abd5c8d",
            "54f76eec958f4940aec443967b89f7e5",
            "6666bba3a147466fabc500991c913ea1",
            "f6db56cd7b70466d9d56c5e91fb1d53d",
            "6bb7d1f141564f5388d81bb41656b350",
            "7a8fb250535744719cf26189d6d75f31",
            "05a9a80833b644ca81988af58fdfb538",
            "6d90cd2736b34d2cbda695cc563e90dc",
            "32a2c8864b9e4323b53a6b0a7853940a",
            "b4bf5fcb935441eab701297db06cf340",
            "54e85822f25c4e44973afebe811ad064",
            "5a9cec239dbc4d7897bc6b5c65f006ec",
            "a47acbcec9164efb937f0a9be9e367b5",
            "73f8a83b6a234f6b8aa85e7efff5c5eb",
            "d866786cc54341c197d83b6089d2f23d",
            "3107742601fc444b91b4d23852b5348d",
            "b0eb09cdf9754db6b6d7ef7577ea2601",
            "01af04e13b094cebbd5c805c625c427e",
            "88ad558d7be745048a0009dcc7cd0473",
            "1118fabcbfc349188f9e9f41cf031c14",
            "07ea44e4f4cf4b38aa9f46d61bd36e17",
            "90ea717027744d43bf63d2e43fb9e26d",
            "7864cc92bd36494baee7773862f9841d",
            "6910542448bd4391b8a851cf3a342870",
            "c841fbeeaa894d06804fb277310e3a76",
            "b9ea57a861244e79aed74cb4ab99c49c",
            "cef1bc1d780a40ad96716c1ef4318dd1",
            "6983bcae458c474bbe0cea278df06a2e",
            "918a77a7083e4d0aa000f5a7ae5bf3a0",
            "d386cd38cdb94c3390e4d1c5fc980493",
            "5fa921b4be0943da87f984787fee2c71",
            "d2d8962dfe8944c3b50df56df4e10374",
            "1675e4bb6a3e4cfba59b328b919743a3",
            "f6c19b50d2254826b83a5b5292feb090"
          ]
        },
        "executionInfo": {
          "elapsed": 215765,
          "status": "ok",
          "timestamp": 1749976201173,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "DKuVtzqhdjmL",
        "outputId": "993992bd-d782-4d62-cd10-a930f4d0d47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 A100 ULTIMATE 15K FINE-TUNING - %90+ HEDEF\u0130\n",
            "============================================================\n",
            "\ud83c\udfaf T\u00fcm 15K veri ile XLM-RoBERTa fine-tuning\n",
            "\ud83c\udfc6 Hedef: %90+ F1 Score\n",
            "\u23f0 A100 ile tahmini s\u00fcre: 30-45 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n",
            "\u26a1 A100 GPU tespit edildi - ULTIMATE optimizasyonlar aktif!\n",
            "\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udcc2 Hedef dosya: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\n",
            "\u2705 Dosya mevcut!\n",
            "\ud83d\udcbe Dosya boyutu: 0.6 MB\n",
            "\ud83d\udcd6 Excel dosyas\u0131 okunuyor...\n",
            "\u2705 Dosya ba\u015far\u0131yla okundu!\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum (2.6s)\n",
            "\ud83d\udcca Toplam veri: 15167\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Faydas\u0131z: 6686 (%44.1)\n",
            "\n",
            "\ud83d\udd00 TRAIN/VALIDATION SPLIT...\n",
            "\ud83d\udcca Train: 12891 yorum\n",
            "\ud83d\udcca Validation: 2276 yorum\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5683 7208]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1003 1273]\n",
            "\n",
            "\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udce6 XLM-ROBERTA MODEL \u0130ND\u0130R\u0130L\u0130YOR VE Y\u00dcKLEN\u0130YOR...\n",
            "\ud83c\udf10 \u0130nternet ba\u011flant\u0131s\u0131 kontrol ediliyor...\n",
            "\ud83d\udce5 xlm-roberta-base indiriliyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6b50135766347e68c92123efcecfbe1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "369a654ed42c4ea4a6c8d2678719e32e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54f76eec958f4940aec443967b89f7e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a47acbcec9164efb937f0a9be9e367b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6910542448bd4391b8a851cf3a342870",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 XLM-RoBERTa yerel olarak kaydedildi: /content/xlm_roberta_local\n",
            "\u2705 xlm-roberta-base y\u00fcklendi ve GPU'ya ta\u015f\u0131nd\u0131! (17.2s)\n",
            "\n",
            "\ud83d\udce6 B\u00dcY\u00dcK DATASET HAZIRLANIYOR...\n",
            "\u2705 Dataset haz\u0131r! Max length: 256 (0.0s)\n",
            "\n",
            "\u2699\ufe0f TRAINING PARAMETRELER\u0130...\n",
            "\u26a1 A100 ULTIMATE MODE AKT\u0130F!\n",
            "\ud83d\udd27 Batch size: 32\n",
            "\ud83d\udd27 Learning rate: 3e-05\n",
            "\ud83c\udfaf Epochs: 4\n",
            "\ud83c\udfaf Learning rate: 3e-05\n",
            "\ud83c\udfaf BF16: True\n",
            "\ud83c\udfaf FP16: False\n",
            "\n",
            "\ud83c\udfc6 MEVCUT \u015eAMPIYON: 0.8786 F1\n",
            "\ud83c\udfaf HEDEF: 0.9000+ F1 (%90+)\n",
            "\n",
            "\ud83d\ude80 FINE-TUNING BA\u015eLIYOR...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1612' max='1612' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1612/1612 02:49, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.433900</td>\n",
              "      <td>0.403140</td>\n",
              "      <td>0.872144</td>\n",
              "      <td>0.868882</td>\n",
              "      <td>0.874998</td>\n",
              "      <td>0.865614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.406200</td>\n",
              "      <td>0.395974</td>\n",
              "      <td>0.879613</td>\n",
              "      <td>0.877186</td>\n",
              "      <td>0.880014</td>\n",
              "      <td>0.875252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.344400</td>\n",
              "      <td>0.387860</td>\n",
              "      <td>0.892355</td>\n",
              "      <td>0.890541</td>\n",
              "      <td>0.891670</td>\n",
              "      <td>0.889603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.304600</td>\n",
              "      <td>0.398218</td>\n",
              "      <td>0.896309</td>\n",
              "      <td>0.894807</td>\n",
              "      <td>0.894892</td>\n",
              "      <td>0.894724</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 FINE-TUNING TAMAMLANDI! (2.9 dakika)\n",
            "\n",
            "\ud83d\udcca MODEL DE\u011eERLEND\u0130RME:\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfc6 F1 Score: 0.8948\n",
            "\ud83d\udcca Accuracy: 0.8963\n",
            "\ud83d\udcc8 Precision: 0.8949\n",
            "\ud83d\udcc8 Recall: 0.8947\n",
            "\n",
            "\ud83c\udf89 SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\n",
            "==================================================\n",
            "Mevcut \u015fampiyon: 0.8786 F1\n",
            "Fine-tuned model: 0.8948 F1\n",
            "\u0130yile\u015fme: +0.0162 F1 (+1.84%)\n",
            "\n",
            "\u2705 \u015eAMPIYON DE\u011e\u0130\u015eT\u0130!\n",
            "\n",
            "\ud83d\udcbe XLM-ROBERTA MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 XLM-RoBERTa model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model\n",
            "\n",
            "\ud83d\udcda FINE-TUNING \u00d6ZET\u0130:\n",
            "========================================\n",
            "\u2022 Model: xlm-roberta-base\n",
            "\u2022 Dataset: 15,167 yorumlar\n",
            "\u2022 Train/Val: 12891/2276\n",
            "\u2022 Epochs: 4\n",
            "\u2022 Batch size: 32\n",
            "\u2022 F1 Score: 0.8948\n",
            "\u2022 Achievement: CHAMPION\n",
            "\u2022 Training time: 2.9 dakika\n",
            "\u2022 Total time: 3.3 dakika\n",
            "\n",
            "\ud83e\uddea \u00d6RNEK TEST:\n",
            "Metin: 'Bu \u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel ve kaliteli!'\n",
            "Tahmin: Faydas\u0131z (G\u00fcven: %95.2)\n",
            "\n",
            "\ud83c\udf8a FINE-TUNING S\u00dcRECI TAMAMLANDI!\n",
            "\ud83d\udcbe Memory temizlendi!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\udd25 A100 ULTIMATE 15K FINE-TUNING - %90+ HEDEF\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf T\u00fcm 15K veri ile XLM-RoBERTa fine-tuning\")\n",
        "print(\"\ud83c\udfc6 Hedef: %90+ F1 Score\")\n",
        "print(\"\u23f0 A100 ile tahmini s\u00fcre: 30-45 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # A100 \u00f6zel optimizasyonlar\u0131\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 GPU tespit edildi - ULTIMATE optimizasyonlar aktif!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Memory temizli\u011fi\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f CPU kullan\u0131l\u0131yor - i\u015flem yava\u015f olabilir\")\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# XLM-RoBERTa model ve tokenizer'\u0131 offline olarak y\u00fckle\n",
        "def load_roberta_offline():\n",
        "    \"\"\"XLM-RoBERTa model ve tokenizer'\u0131 offline olarak y\u00fckler\"\"\"\n",
        "    print(\"\ud83d\udce6 XLM-ROBERTA MODEL \u0130ND\u0130R\u0130L\u0130YOR VE Y\u00dcKLEN\u0130YOR...\")\n",
        "\n",
        "    # \u00d6nce XLM-RoBERTa'y\u0131 indir ve kaydet\n",
        "    try:\n",
        "        # \u0130nternet ba\u011flant\u0131s\u0131 varsa modeli indir\n",
        "        print(\"\ud83c\udf10 \u0130nternet ba\u011flant\u0131s\u0131 kontrol ediliyor...\")\n",
        "\n",
        "        # XLM-RoBERTa - orijinal model\n",
        "        model_name = \"xlm-roberta-base\"\n",
        "\n",
        "        # Timeout ayarlar\u0131 ile modeli indir\n",
        "        print(f\"\ud83d\udce5 {model_name} indiriliyor...\")\n",
        "\n",
        "        # Tokenizer'\u0131 \u00f6nce indir\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            force_download=False,\n",
        "            resume_download=True,\n",
        "            use_fast=True\n",
        "        )\n",
        "\n",
        "        # Model'i indir\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2,\n",
        "            return_dict=True,\n",
        "            force_download=False,\n",
        "            resume_download=True,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Modeli yerel olarak kaydet\n",
        "        local_model_path = \"/content/xlm_roberta_local\"\n",
        "        os.makedirs(local_model_path, exist_ok=True)\n",
        "\n",
        "        model.save_pretrained(local_model_path)\n",
        "        tokenizer.save_pretrained(local_model_path)\n",
        "\n",
        "        print(f\"\u2705 XLM-RoBERTa yerel olarak kaydedildi: {local_model_path}\")\n",
        "        return model, tokenizer, model_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c XLM-RoBERTa indirme hatas\u0131: {e}\")\n",
        "\n",
        "        # Offline modda \u00e7al\u0131\u015f - \u00f6nceden indirilmi\u015f model varsa kullan\n",
        "        local_paths = [\n",
        "            \"/content/xlm_roberta_local\",\n",
        "            \"/root/.cache/huggingface/transformers\",\n",
        "            \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_model\"\n",
        "        ]\n",
        "\n",
        "        for path in local_paths:\n",
        "            if os.path.exists(path):\n",
        "                try:\n",
        "                    print(f\"\ud83d\udcc2 Yerel XLM-RoBERTa bulundu: {path}\")\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                        path,\n",
        "                        num_labels=2,\n",
        "                        return_dict=True,\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    return model, tokenizer, \"local-xlm-roberta\"\n",
        "                except Exception as local_error:\n",
        "                    print(f\"\u26a0\ufe0f {path} y\u00fcklenemedi: {local_error}\")\n",
        "                    continue\n",
        "\n",
        "        # Manuel indirme \u00e7\u00f6z\u00fcm\u00fc\n",
        "        print(\"\\n\ud83d\udca1 XLM-ROBERTA MANUEL \u0130ND\u0130RME \u00c7\u00d6Z\u00dcM\u00dc:\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"1. Yeni bir h\u00fccrede \u015funu \u00e7al\u0131\u015ft\u0131r\u0131n:\")\n",
        "        print(\"\")\n",
        "        print(\"# XLM-RoBERTa manuel indirme\")\n",
        "        print(\"!mkdir -p /content/xlm_roberta_cache\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/config.json https://huggingface.co/xlm-roberta-base/resolve/main/config.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/pytorch_model.bin https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/tokenizer.json https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/vocab.json https://huggingface.co/xlm-roberta-base/resolve/main/vocab.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/merges.txt https://huggingface.co/xlm-roberta-base/resolve/main/merges.txt\")\n",
        "        print(\"\")\n",
        "        print(\"2. Ard\u0131ndan bu kodu tekrar \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "        print(\"\")\n",
        "        print(\"VEYA Alternatif \u00e7\u00f6z\u00fcm:\")\n",
        "        print(\"!pip install --upgrade transformers torch\")\n",
        "        print(\"import os\")\n",
        "        print(\"os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\")\n",
        "\n",
        "        # Son \u00e7are olarak cache'den y\u00fcklemeyi dene\n",
        "        try:\n",
        "            print(\"\\n\ud83d\udd04 Cache'den y\u00fckleme deneniyor...\")\n",
        "            # Hugging Face cache klas\u00f6r\u00fcn\u00fc kontrol et\n",
        "            cache_dir = \"/root/.cache/huggingface/hub\"\n",
        "            if os.path.exists(cache_dir):\n",
        "                # XLM-RoBERTa cache klas\u00f6rlerini ara\n",
        "                for item in os.listdir(cache_dir):\n",
        "                    if \"xlm-roberta\" in item.lower():\n",
        "                        cache_path = os.path.join(cache_dir, item)\n",
        "                        try:\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(cache_path, local_files_only=True)\n",
        "                            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                                cache_path,\n",
        "                                num_labels=2,\n",
        "                                return_dict=True,\n",
        "                                local_files_only=True\n",
        "                            )\n",
        "                            print(f\"\u2705 Cache'den y\u00fcklendi: {cache_path}\")\n",
        "                            return model, tokenizer, \"cached-xlm-roberta\"\n",
        "                        except:\n",
        "                            continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        raise Exception(\"XLM-RoBERTa y\u00fcklenemedi - manuel indirme gerekli\")\n",
        "\n",
        "# 15K veriyi y\u00fckle\n",
        "print(\"\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "print(f\"\ud83d\udcc2 Hedef dosya: {file_path}\")\n",
        "\n",
        "# Dosya varl\u0131k kontrol\u00fc\n",
        "if os.path.exists(file_path):\n",
        "    print(\"\u2705 Dosya mevcut!\")\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    print(f\"\ud83d\udcbe Dosya boyutu: {file_size:.1f} MB\")\n",
        "else:\n",
        "    print(\"\u274c Dosya bulunamad\u0131!\")\n",
        "    # Alternatif yollar\u0131 dene\n",
        "    alternative_paths = [\n",
        "        \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\",\n",
        "        \"/content/drive/MyDrive/yorumlar1_ETIKETLI_FINAL.xlsx\",\n",
        "        \"/content/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "    ]\n",
        "    for alt_path in alternative_paths:\n",
        "        if os.path.exists(alt_path):\n",
        "            file_path = alt_path\n",
        "            print(f\"\u2705 Alternatif dosya bulundu: {file_path}\")\n",
        "            break\n",
        "\n",
        "try:\n",
        "    print(\"\ud83d\udcd6 Excel dosyas\u0131 okunuyor...\")\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"\u2705 Dosya ba\u015far\u0131yla okundu!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Dosya okuma hatas\u0131: {e}\")\n",
        "    print(\"\ud83d\udd04 Farkl\u0131 okuma y\u00f6ntemi deneniyor...\")\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='openpyxl')\n",
        "        print(f\"\u2705 Alternatif y\u00f6ntemle okundu!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"\u274c Alternatif y\u00f6ntem de ba\u015far\u0131s\u0131z: {e2}\")\n",
        "        raise Exception(\"Dosya okunamad\u0131\")\n",
        "\n",
        "# Veri temizleme\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum ({time.time()-start_time:.1f}s)\")\n",
        "print(f\"\ud83d\udcca Toplam veri: {len(texts)}\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Faydas\u0131z: {len(labels)-np.sum(labels)} (%{(1-np.mean(labels))*100:.1f})\")\n",
        "\n",
        "# Train/Val split (stratified)\n",
        "print(f\"\\n\ud83d\udd00 TRAIN/VALIDATION SPLIT...\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Validation: {len(val_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: {np.bincount(train_labels)}\")\n",
        "print(f\"\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "# Model y\u00fckleme\n",
        "print(f\"\\n\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "model_load_start = time.time()\n",
        "\n",
        "try:\n",
        "    model, tokenizer, model_name = load_roberta_offline()\n",
        "    model.to(device)\n",
        "    print(f\"\u2705 {model_name} y\u00fcklendi ve GPU'ya ta\u015f\u0131nd\u0131! ({time.time()-model_load_start:.1f}s)\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c XLM-RoBERTa y\u00fckleme hatas\u0131: {e}\")\n",
        "    print(\"\\n\ud83d\udee0\ufe0f MANUEL \u00c7\u00d6Z\u00dcM:\")\n",
        "    print(\"1. Yukar\u0131daki wget komutlar\u0131n\u0131 \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "    print(\"2. Veya alternatif olarak:\")\n",
        "    print('!pip install --upgrade transformers torch')\n",
        "    print('!python -c \"from transformers import AutoTokenizer, AutoModel; AutoTokenizer.from_pretrained(\\'xlm-roberta-base\\'); AutoModel.from_pretrained(\\'xlm-roberta-base\\')\"')\n",
        "    print(\"3. Bu kodu tekrar \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "    raise\n",
        "\n",
        "# Dataset olu\u015ftur\n",
        "print(f\"\\n\ud83d\udce6 B\u00dcY\u00dcK DATASET HAZIRLANIYOR...\")\n",
        "dataset_start = time.time()\n",
        "\n",
        "max_length = 256 if torch.cuda.is_available() else 128\n",
        "train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "print(f\"\u2705 Dataset haz\u0131r! Max length: {max_length} ({time.time()-dataset_start:.1f}s)\")\n",
        "\n",
        "# Training arguments\n",
        "print(f\"\\n\u2699\ufe0f TRAINING PARAMETRELER\u0130...\")\n",
        "\n",
        "if torch.cuda.is_available() and \"A100\" in torch.cuda.get_device_name(0):\n",
        "    batch_size = 32\n",
        "    grad_accum_steps = 1\n",
        "    learning_rate = 3e-5\n",
        "    print(\"\u26a1 A100 ULTIMATE MODE AKT\u0130F!\")\n",
        "elif torch.cuda.is_available():\n",
        "    batch_size = 16\n",
        "    grad_accum_steps = 1\n",
        "    learning_rate = 2e-5\n",
        "else:\n",
        "    batch_size = 8\n",
        "    grad_accum_steps = 2\n",
        "    learning_rate = 2e-5\n",
        "\n",
        "print(f\"\ud83d\udd27 Batch size: {batch_size}\")\n",
        "print(f\"\ud83d\udd27 Learning rate: {learning_rate}\")\n",
        "\n",
        "# Klas\u00f6r olu\u015ftur\n",
        "os.makedirs('./ultimate_results', exist_ok=True)\n",
        "os.makedirs('./ultimate_logs', exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./ultimate_results',\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=grad_accum_steps,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=learning_rate,\n",
        "    logging_dir='./ultimate_logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=2,\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=torch.cuda.is_available(),\n",
        "    fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    label_smoothing_factor=0.1,\n",
        ")\n",
        "\n",
        "print(f\"\ud83c\udfaf Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"\ud83c\udfaf Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"\ud83c\udfaf BF16: {training_args.bf16}\")\n",
        "print(f\"\ud83c\udfaf FP16: {training_args.fp16}\")\n",
        "\n",
        "# Trainer olu\u015ftur\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "current_champion_f1 = 0.8786\n",
        "print(f\"\\n\ud83c\udfc6 MEVCUT \u015eAMPIYON: {current_champion_f1:.4f} F1\")\n",
        "print(f\"\ud83c\udfaf HEDEF: 0.9000+ F1 (%90+)\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 FINE-TUNING BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "fine_tuning_start = time.time()\n",
        "\n",
        "try:\n",
        "    # Fine-tuning ba\u015flat\n",
        "    trainer.train()\n",
        "\n",
        "    fine_tuning_time = time.time() - fine_tuning_start\n",
        "    print(f\"\\n\u2705 FINE-TUNING TAMAMLANDI! ({fine_tuning_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(f\"\\n\ud83d\udcca MODEL DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    ultimate_f1 = eval_results['eval_f1']\n",
        "    ultimate_acc = eval_results['eval_accuracy']\n",
        "    ultimate_precision = eval_results['eval_precision']\n",
        "    ultimate_recall = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\ud83c\udfc6 F1 Score: {ultimate_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ultimate_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ultimate_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ultimate_recall:.4f}\")\n",
        "\n",
        "    # Kar\u015f\u0131la\u015ft\u0131rma\n",
        "    improvement = ultimate_f1 - current_champion_f1\n",
        "    improvement_pct = (improvement / current_champion_f1) * 100\n",
        "\n",
        "    print(f\"\\n\ud83c\udf89 SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Mevcut \u015fampiyon: {current_champion_f1:.4f} F1\")\n",
        "    print(f\"Fine-tuned model: {ultimate_f1:.4f} F1\")\n",
        "    print(f\"\u0130yile\u015fme: {improvement:+.4f} F1 ({improvement_pct:+.2f}%)\")\n",
        "\n",
        "    # Hedef de\u011ferlendirme\n",
        "    if ultimate_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF ULA\u015eILDI! %90+ F1 SCORE!\")\n",
        "        achievement = \"LEGENDARY\"\n",
        "    elif ultimate_f1 >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 NEREDEYSE HEDEF! %89.5+ F1!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    elif ultimate_f1 > current_champion_f1:\n",
        "        print(f\"\\n\u2705 \u015eAMPIYON DE\u011e\u0130\u015eT\u0130!\")\n",
        "        achievement = \"CHAMPION\"\n",
        "    else:\n",
        "        print(f\"\\n\ud83d\ude10 Beklenen iyile\u015fme sa\u011flanamad\u0131\")\n",
        "        achievement = \"COMPARABLE\"\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe XLM-ROBERTA MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"\u2705 XLM-RoBERTa model kaydedildi: {save_path}\")\n",
        "\n",
        "    # Sonu\u00e7 \u00f6zeti\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83d\udcda FINE-TUNING \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\u2022 Model: {model_name}\")\n",
        "    print(f\"\u2022 Dataset: {len(texts):,} yorumlar\")\n",
        "    print(f\"\u2022 Train/Val: {len(train_texts)}/{len(val_texts)}\")\n",
        "    print(f\"\u2022 Epochs: {training_args.num_train_epochs}\")\n",
        "    print(f\"\u2022 Batch size: {batch_size}\")\n",
        "    print(f\"\u2022 F1 Score: {ultimate_f1:.4f}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "    print(f\"\u2022 Training time: {fine_tuning_time/60:.1f} dakika\")\n",
        "    print(f\"\u2022 Total time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "    # Test prediction\n",
        "    print(f\"\\n\ud83e\uddea \u00d6RNEK TEST:\")\n",
        "    test_text = \"Bu \u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel ve kaliteli!\"\n",
        "    inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "        confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "    result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "    print(f\"Metin: '{test_text}'\")\n",
        "    print(f\"Tahmin: {result} (G\u00fcven: %{confidence*100:.1f})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c FINE-TUNING HATASI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"\\n\ud83d\udca1 \u00c7\u00f6z\u00fcm \u00f6nerileri:\")\n",
        "    print(f\"  - GPU memory azaltmak i\u00e7in batch_size k\u00fc\u00e7\u00fclt\u00fcn\")\n",
        "    print(f\"  - Max length 128'e d\u00fc\u015f\u00fcr\u00fcn\")\n",
        "    print(f\"  - Epoch say\u0131s\u0131n\u0131 2'ye d\u00fc\u015f\u00fcr\u00fcn\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a FINE-TUNING S\u00dcRECI TAMAMLANDI!\")\n",
        "\n",
        "# Memory temizli\u011fi\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 114203,
          "status": "ok",
          "timestamp": 1749976632721,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "uzcdE7q2fLVw",
        "outputId": "b5b4a9a5-1234-43b8-8f95-4b9d9c89a602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 XLM-ROBERTA %92+ F1 SCORE ULTIMATE OPTIMIZATION\n",
            "======================================================================\n",
            "\ud83c\udfaf Mevcut: %89.48 F1 \u2192 Hedef: %92+ F1\n",
            "\ud83d\ude80 Advanced hyperparameter tuning ve optimizasyonlar\n",
            "\u26a1 A100 POWER: Maximum performance mode\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n",
            "\u26a1 A100 ULTIMATE %92+ MODE AKT\u0130F!\n",
            "\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 15167 yorum y\u00fcklendi\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83d\udd00 ADVANCED TRAIN/VALIDATION SPLIT...\n",
            "\ud83d\udcca Train: 13346 yorum (%88.0)\n",
            "\ud83d\udcca Validation: 1821 yorum (%12.0)\n",
            "\n",
            "\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udd04 \u00d6nceki fine-tuned model bulundu - devam ediliyor...\n",
            "\u2705 Fine-tuned model'den devam ediliyor!\n",
            "\u2705 Model GPU'ya ta\u015f\u0131nd\u0131! (1.6s)\n",
            "\n",
            "\ud83d\udce6 ADVANCED DATASET HAZIRLANIYOR...\n",
            "\u2705 Advanced Dataset haz\u0131r! Max length: 512\n",
            "\ud83d\udcca Data augmentation: Train'de aktif\n",
            "\n",
            "\u2699\ufe0f %92+ F1 \u0130\u00c7\u0130N ULTIMATE PARAMETRELER\u0130...\n",
            "\u26a1 A100 %92+ ULTIMATE MODE!\n",
            "\ud83d\udd27 Batch size: 16 (effective: 32)\n",
            "\ud83d\udd27 Learning rate: 1e-05\n",
            "\ud83d\udd27 Epochs: 6\n",
            "\ud83d\udd27 Max length: 512\n",
            "\ud83c\udfaf Scheduler: SchedulerType.COSINE\n",
            "\ud83c\udfaf Warmup ratio: 0.1\n",
            "\ud83c\udfaf Label smoothing: 0.1\n",
            "\n",
            "\ud83c\udfc6 MEVCUT EN \u0130Y\u0130: 0.8948 F1\n",
            "\ud83c\udfaf YEN\u0130 HEDEF: 0.9200+ F1 (%92+)\n",
            "\ud83d\udcc8 Gereken iyile\u015fme: +0.0252\n",
            "\n",
            "\ud83d\ude80 %92+ F1 \u0130\u00c7\u0130N ULTIMATE FINE-TUNING BA\u015eLIYOR...\n",
            "======================================================================\n",
            "\u23f0 A100 ile tahmini s\u00fcre: 45-60 dakika\n",
            "\ud83d\udd25 Advanced optimizasyonlar aktif...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='2508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 400/2508 01:44 < 09:13, 3.81 it/s, Epoch 0/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.384484</td>\n",
              "      <td>0.893465</td>\n",
              "      <td>0.891692</td>\n",
              "      <td>0.892791</td>\n",
              "      <td>0.890776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.282500</td>\n",
              "      <td>0.410666</td>\n",
              "      <td>0.887424</td>\n",
              "      <td>0.885082</td>\n",
              "      <td>0.888434</td>\n",
              "      <td>0.882874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.313500</td>\n",
              "      <td>0.393167</td>\n",
              "      <td>0.890719</td>\n",
              "      <td>0.889465</td>\n",
              "      <td>0.888619</td>\n",
              "      <td>0.890555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.334300</td>\n",
              "      <td>0.390789</td>\n",
              "      <td>0.889621</td>\n",
              "      <td>0.888406</td>\n",
              "      <td>0.887453</td>\n",
              "      <td>0.889704</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 ULTIMATE FINE-TUNING TAMAMLANDI! (1.8 dakika)\n",
            "\n",
            "\ud83d\udcca %92+ HEDEF \u0130\u00c7\u0130N FINAL DE\u011eERLEND\u0130RME:\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [57/57 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfc6 ULTIMATE F1: 0.8917\n",
            "\ud83d\udcca Accuracy: 0.8935\n",
            "\ud83d\udcc8 Precision: 0.8928\n",
            "\ud83d\udcc8 Recall: 0.8908\n",
            "\n",
            "\ud83c\udf89 %92+ HEDEF DE\u011eERLEND\u0130RMES\u0130:\n",
            "================================================================================\n",
            "\u00d6nceki en iyi:     0.8948 F1\n",
            "ULTIMATE result:   0.8917 F1\n",
            "\u0130yile\u015fme:          -0.0031 F1 (-0.35%)\n",
            "Hedefe mesafe:     +0.0283\n",
            "\n",
            "\ud83e\udd14 Bu denemede iyile\u015fme olmad\u0131\n",
            "\n",
            "\ud83d\udcbe ULTIMATE %92+ MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Ultimate model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_ultimate_92_model\n",
            "\n",
            "\ud83d\udcda ULTIMATE %92+ FINE-TUNING \u00d6ZET\u0130:\n",
            "============================================================\n",
            "\u2022 Model: fine-tuned-xlm-roberta-continued\n",
            "\u2022 Dataset: 15,167 yorumlar\n",
            "\u2022 Train/Val: 13346/1821\n",
            "\u2022 Epochs: 6\n",
            "\u2022 Effective batch size: 32\n",
            "\u2022 Max length: 512\n",
            "\u2022 Learning rate: 1e-05\n",
            "\u2022 Scheduler: SchedulerType.COSINE\n",
            "\u2022 Data augmentation: \u2705\n",
            "\u2022 BF16: True\n",
            "\u2022 ULTIMATE F1: 0.8917\n",
            "\u2022 Achievement: STABLE\n",
            "\u2022 Training time: 1.8 dakika\n",
            "\u2022 Total time: 1.9 dakika\n",
            "\n",
            "\u2705 Ultimate sonu\u00e7lar kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_92_PLUS_RESULTS.xlsx\n",
            "\n",
            "\ud83e\uddea ADVANCED MODEL TEST\u0130:\n",
            "========================================\n",
            "1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!'\n",
            "   \u2192 Faydas\u0131z (%94.5 g\u00fcven)\n",
            "2. 'Berbat bir deneyimdi, hi\u00e7 tavsiye etmem.'\n",
            "   \u2192 Faydas\u0131z (%94.3 g\u00fcven)\n",
            "3. 'Fiyat\u0131na g\u00f6re idare eder.'\n",
            "   \u2192 Faydas\u0131z (%94.7 g\u00fcven)\n",
            "4. 'Muhte\u015fem kalite, herkese tavsiye ederim!'\n",
            "   \u2192 Faydas\u0131z (%95.0 g\u00fcven)\n",
            "\n",
            "\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\n",
            "========================================\n",
            "\ud83d\udd04 Daha fazla iyile\u015fme i\u00e7in:\n",
            "  \u2022 Daha fazla epoch (8-10)\n",
            "  \u2022 Cross-validation ensemble\n",
            "  \u2022 Advanced data augmentation\n",
            "  \u2022 xlm-roberta-large model\n",
            "  \u2022 Focal loss for imbalanced data\n",
            "  \u2022 Learning rate scheduling fine-tuning\n",
            "\n",
            "\ud83c\udf8a ULTIMATE %92+ F1 OPTIMIZATION TAMAMLANDI!\n",
            "\n",
            "\ud83d\udcc8 GREAT PROGRESS! \ud83d\udcc8\n",
            "\ud83d\udcaa 0.8917 F1 - \u0130yile\u015fme devam ediyor!\n",
            "\n",
            "\ud83d\udcbe GPU memory temizlendi!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback,\n",
        "    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from torch.optim import AdamW\n",
        "import random\n",
        "\n",
        "print(\"\ud83d\udd25 XLM-ROBERTA %92+ F1 SCORE ULTIMATE OPTIMIZATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf Mevcut: %89.48 F1 \u2192 Hedef: %92+ F1\")\n",
        "print(\"\ud83d\ude80 Advanced hyperparameter tuning ve optimizasyonlar\")\n",
        "print(\"\u26a1 A100 POWER: Maximum performance mode\")\n",
        "print()\n",
        "\n",
        "# Reproducibility i\u00e7in seed sabitleme\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # A100 ultimate optimizasyonlar\u0131\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 ULTIMATE %92+ MODE AKT\u0130F!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "        torch.backends.cudnn.benchmark = True  # A100 i\u00e7in eklendi\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Advanced Dataset with data augmentation\n",
        "class AdvancedReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512, augment=False):  # Longer sequences\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        \"\"\"Simple text augmentation\"\"\"\n",
        "        if not self.augment or random.random() > 0.3:\n",
        "            return text\n",
        "\n",
        "        # Random word dropout (5% of words)\n",
        "        words = text.split()\n",
        "        if len(words) > 5:\n",
        "            keep_ratio = 0.95\n",
        "            keep_count = max(1, int(len(words) * keep_ratio))\n",
        "            indices = random.sample(range(len(words)), keep_count)\n",
        "            words = [words[i] for i in sorted(indices)]\n",
        "            return ' '.join(words)\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply augmentation\n",
        "        if self.augment:\n",
        "            text = self.augment_text(text)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 {len(texts)} yorum y\u00fcklendi\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# Advanced train/val split with stratification\n",
        "print(f\"\\n\ud83d\udd00 ADVANCED TRAIN/VALIDATION SPLIT...\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.12,  # Biraz daha fazla train verisi\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)} yorum (%{len(train_texts)/len(texts)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Validation: {len(val_texts)} yorum (%{len(val_texts)/len(texts)*100:.1f})\")\n",
        "\n",
        "# Model y\u00fckleme (\u00f6nceki fine-tuned model varsa kullan)\n",
        "print(f\"\\n\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "model_load_start = time.time()\n",
        "\n",
        "# \u00d6nceki fine-tuned model'i kullan\n",
        "pretrained_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model\"\n",
        "base_model = \"xlm-roberta-base\"\n",
        "\n",
        "if os.path.exists(pretrained_path):\n",
        "    print(\"\ud83d\udd04 \u00d6nceki fine-tuned model bulundu - devam ediliyor...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(pretrained_path)\n",
        "        model_name = \"fine-tuned-xlm-roberta-continued\"\n",
        "        print(\"\u2705 Fine-tuned model'den devam ediliyor!\")\n",
        "    except:\n",
        "        print(\"\u26a0\ufe0f Fine-tuned model y\u00fcklenemedi, base model kullan\u0131l\u0131yor...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)\n",
        "        model_name = base_model\n",
        "else:\n",
        "    print(\"\ud83d\udce6 Base model y\u00fckleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)\n",
        "    model_name = base_model\n",
        "\n",
        "model.to(device)\n",
        "print(f\"\u2705 Model GPU'ya ta\u015f\u0131nd\u0131! ({time.time()-model_load_start:.1f}s)\")\n",
        "\n",
        "# Advanced dataset creation\n",
        "print(f\"\\n\ud83d\udce6 ADVANCED DATASET HAZIRLANIYOR...\")\n",
        "dataset_start = time.time()\n",
        "\n",
        "# A100 i\u00e7in uzun sequence length\n",
        "max_length = 512 if \"A100\" in torch.cuda.get_device_name(0) else 384\n",
        "\n",
        "# Augmentation ile train dataset\n",
        "train_dataset = AdvancedReviewDataset(\n",
        "    train_texts, train_labels, tokenizer,\n",
        "    max_length=max_length, augment=True\n",
        ")\n",
        "val_dataset = AdvancedReviewDataset(\n",
        "    val_texts, val_labels, tokenizer,\n",
        "    max_length=max_length, augment=False\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Advanced Dataset haz\u0131r! Max length: {max_length}\")\n",
        "print(f\"\ud83d\udcca Data augmentation: Train'de aktif\")\n",
        "\n",
        "# ULTIMATE A100 training arguments for %92+ F1\n",
        "print(f\"\\n\u2699\ufe0f %92+ F1 \u0130\u00c7\u0130N ULTIMATE PARAMETRELER\u0130...\")\n",
        "\n",
        "if \"A100\" in torch.cuda.get_device_name(0):\n",
        "    batch_size = 16  # Uzun sequence i\u00e7in azalt\u0131ld\u0131\n",
        "    grad_accum_steps = 2  # Effective batch = 32\n",
        "    learning_rate = 1e-5  # Daha d\u00fc\u015f\u00fck LR for fine-tuning\n",
        "    epochs = 6  # Daha fazla epoch\n",
        "    warmup_ratio = 0.1\n",
        "    print(\"\u26a1 A100 %92+ ULTIMATE MODE!\")\n",
        "else:\n",
        "    batch_size = 8\n",
        "    grad_accum_steps = 4\n",
        "    learning_rate = 1.5e-5\n",
        "    epochs = 5\n",
        "    warmup_ratio = 0.1\n",
        "\n",
        "print(f\"\ud83d\udd27 Batch size: {batch_size} (effective: {batch_size * grad_accum_steps})\")\n",
        "print(f\"\ud83d\udd27 Learning rate: {learning_rate}\")\n",
        "print(f\"\ud83d\udd27 Epochs: {epochs}\")\n",
        "print(f\"\ud83d\udd27 Max length: {max_length}\")\n",
        "\n",
        "# Klas\u00f6r olu\u015ftur\n",
        "os.makedirs('./ultimate_92_results', exist_ok=True)\n",
        "os.makedirs('./ultimate_92_logs', exist_ok=True)\n",
        "\n",
        "# Advanced training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./ultimate_92_results',\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=grad_accum_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=\"cosine\",  # Cosine annealing\n",
        "    logging_dir='./ultimate_92_logs',\n",
        "    logging_steps=25,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,  # Daha s\u0131k evaluation\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=True,\n",
        "    fp16=False,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    dataloader_num_workers=4,  # Daha fazla worker\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    label_smoothing_factor=0.1,\n",
        "    # Advanced optimization\n",
        "    adam_epsilon=1e-6,\n",
        "    max_grad_norm=1.0,\n",
        "    prediction_loss_only=False,\n",
        ")\n",
        "\n",
        "print(f\"\ud83c\udfaf Scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"\ud83c\udfaf Warmup ratio: {training_args.warmup_ratio}\")\n",
        "print(f\"\ud83c\udfaf Label smoothing: {training_args.label_smoothing_factor}\")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "# Trainer olu\u015ftur\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "# Baseline comparison\n",
        "current_best_f1 = 0.8948\n",
        "target_f1 = 0.92\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 MEVCUT EN \u0130Y\u0130: {current_best_f1:.4f} F1\")\n",
        "print(f\"\ud83c\udfaf YEN\u0130 HEDEF: {target_f1:.4f}+ F1 (%92+)\")\n",
        "print(f\"\ud83d\udcc8 Gereken iyile\u015fme: {target_f1 - current_best_f1:+.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 %92+ F1 \u0130\u00c7\u0130N ULTIMATE FINE-TUNING BA\u015eLIYOR...\")\n",
        "print(\"=\"*70)\n",
        "print(\"\u23f0 A100 ile tahmini s\u00fcre: 45-60 dakika\")\n",
        "print(\"\ud83d\udd25 Advanced optimizasyonlar aktif...\")\n",
        "\n",
        "fine_tuning_start = time.time()\n",
        "\n",
        "try:\n",
        "    # ULTIMATE FINE-TUNING FOR %92+!\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    fine_tuning_time = time.time() - fine_tuning_start\n",
        "    print(f\"\\n\u2705 ULTIMATE FINE-TUNING TAMAMLANDI! ({fine_tuning_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Final comprehensive evaluation\n",
        "    print(f\"\\n\ud83d\udcca %92+ HEDEF \u0130\u00c7\u0130N FINAL DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    ultimate_f1 = eval_results['eval_f1']\n",
        "    ultimate_acc = eval_results['eval_accuracy']\n",
        "    ultimate_precision = eval_results['eval_precision']\n",
        "    ultimate_recall = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\ud83c\udfc6 ULTIMATE F1: {ultimate_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ultimate_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ultimate_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ultimate_recall:.4f}\")\n",
        "\n",
        "    # MAJOR COMPARISON\n",
        "    print(f\"\\n\ud83c\udf89 %92+ HEDEF DE\u011eERLEND\u0130RMES\u0130:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    improvement = ultimate_f1 - current_best_f1\n",
        "    improvement_pct = (improvement / current_best_f1) * 100\n",
        "\n",
        "    print(f\"\u00d6nceki en iyi:     {current_best_f1:.4f} F1\")\n",
        "    print(f\"ULTIMATE result:   {ultimate_f1:.4f} F1\")\n",
        "    print(f\"\u0130yile\u015fme:          {improvement:+.4f} F1 ({improvement_pct:+.2f}%)\")\n",
        "    print(f\"Hedefe mesafe:     {target_f1 - ultimate_f1:+.4f}\")\n",
        "\n",
        "    # TARGET EVALUATION\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF ULA\u015eILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "        print(f\"\ud83c\udf1f WORLD-CLASS PERFORMANCE!\")\n",
        "        print(f\"\ud83d\ude80 XLM-RoBERTa ULTIMATE CHAMPION!\")\n",
        "        achievement = \"LEGENDARY %92+\"\n",
        "    elif ultimate_f1 >= 0.915:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKLA\u015eTINIZ! %91.5+!\")\n",
        "        print(f\"\u2728 Sadece {0.92 - ultimate_f1:.3f} kald\u0131!\")\n",
        "        achievement = \"ALMOST LEGENDARY\"\n",
        "    elif ultimate_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\ude80 M\u00dcKEMMEL \u0130Y\u0130LE\u015eME! %91+!\")\n",
        "        print(f\"\ud83d\udcaa %92 hedefine do\u011fru g\u00fc\u00e7l\u00fc ad\u0131m!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    elif ultimate_f1 > current_best_f1:\n",
        "        print(f\"\\n\u2705 S\u00dcREKL\u0130 \u0130Y\u0130LE\u015eME!\")\n",
        "        print(f\"\ud83d\udcc8 Do\u011fru y\u00f6nde ilerliyoruz!\")\n",
        "        achievement = \"IMPROVED\"\n",
        "    else:\n",
        "        print(f\"\\n\ud83e\udd14 Bu denemede iyile\u015fme olmad\u0131\")\n",
        "        achievement = \"STABLE\"\n",
        "\n",
        "    # Model kaydet - Ultimate version\n",
        "    print(f\"\\n\ud83d\udcbe ULTIMATE %92+ MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    ultimate_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_ultimate_92_model\"\n",
        "    os.makedirs(ultimate_save_path, exist_ok=True)\n",
        "    model.save_pretrained(ultimate_save_path)\n",
        "    tokenizer.save_pretrained(ultimate_save_path)\n",
        "    print(f\"\u2705 Ultimate model kaydedildi: {ultimate_save_path}\")\n",
        "\n",
        "    # Comprehensive results\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83d\udcda ULTIMATE %92+ FINE-TUNING \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\u2022 Model: {model_name}\")\n",
        "    print(f\"\u2022 Dataset: {len(texts):,} yorumlar\")\n",
        "    print(f\"\u2022 Train/Val: {len(train_texts)}/{len(val_texts)}\")\n",
        "    print(f\"\u2022 Epochs: {training_args.num_train_epochs}\")\n",
        "    print(f\"\u2022 Effective batch size: {batch_size * grad_accum_steps}\")\n",
        "    print(f\"\u2022 Max length: {max_length}\")\n",
        "    print(f\"\u2022 Learning rate: {training_args.learning_rate}\")\n",
        "    print(f\"\u2022 Scheduler: {training_args.lr_scheduler_type}\")\n",
        "    print(f\"\u2022 Data augmentation: \u2705\")\n",
        "    print(f\"\u2022 BF16: {training_args.bf16}\")\n",
        "    print(f\"\u2022 ULTIMATE F1: {ultimate_f1:.4f}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "    print(f\"\u2022 Training time: {fine_tuning_time/60:.1f} dakika\")\n",
        "    print(f\"\u2022 Total time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "    # Detailed results for analysis\n",
        "    ultimate_results = {\n",
        "        'Model': f'Ultimate-{model_name}',\n",
        "        'Dataset_Size': len(texts),\n",
        "        'Max_Length': max_length,\n",
        "        'Epochs': training_args.num_train_epochs,\n",
        "        'Effective_Batch_Size': batch_size * grad_accum_steps,\n",
        "        'Learning_Rate': training_args.learning_rate,\n",
        "        'Scheduler': training_args.lr_scheduler_type,\n",
        "        'Data_Augmentation': True,\n",
        "        'F1_Score': ultimate_f1,\n",
        "        'Accuracy': ultimate_acc,\n",
        "        'Precision': ultimate_precision,\n",
        "        'Recall': ultimate_recall,\n",
        "        'Improvement_vs_Previous': improvement,\n",
        "        'Target_Distance': target_f1 - ultimate_f1,\n",
        "        'Achievement': achievement,\n",
        "        'Training_Time_Minutes': fine_tuning_time/60,\n",
        "        'Total_Time_Minutes': total_time/60\n",
        "    }\n",
        "\n",
        "    results_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ULTIMATE_92_PLUS_RESULTS.xlsx\"\n",
        "    pd.DataFrame([ultimate_results]).to_excel(results_path, index=False)\n",
        "    print(f\"\\n\u2705 Ultimate sonu\u00e7lar kaydedildi: {results_path}\")\n",
        "\n",
        "    # Advanced test samples\n",
        "    print(f\"\\n\ud83e\uddea ADVANCED MODEL TEST\u0130:\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    test_samples = [\n",
        "        \"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\",\n",
        "        \"Berbat bir deneyimdi, hi\u00e7 tavsiye etmem.\",\n",
        "        \"Fiyat\u0131na g\u00f6re idare eder.\",\n",
        "        \"Muhte\u015fem kalite, herkese tavsiye ederim!\"\n",
        "    ]\n",
        "\n",
        "    for i, test_text in enumerate(test_samples, 1):\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"{i}. '{test_text}'\")\n",
        "        print(f\"   \u2192 {result} (%{confidence*100:.1f} g\u00fcven)\")\n",
        "\n",
        "    # Final recommendations if target not reached\n",
        "    if ultimate_f1 < 0.92:\n",
        "        print(f\"\\n\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\")\n",
        "        print(\"=\"*40)\n",
        "        print(\"\ud83d\udd04 Daha fazla iyile\u015fme i\u00e7in:\")\n",
        "        print(\"  \u2022 Daha fazla epoch (8-10)\")\n",
        "        print(\"  \u2022 Cross-validation ensemble\")\n",
        "        print(\"  \u2022 Advanced data augmentation\")\n",
        "        print(\"  \u2022 xlm-roberta-large model\")\n",
        "        print(\"  \u2022 Focal loss for imbalanced data\")\n",
        "        print(\"  \u2022 Learning rate scheduling fine-tuning\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c ULTIMATE FINE-TUNING HATASI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"\\n\ud83d\udca1 %92+ hedef i\u00e7in \u00e7\u00f6z\u00fcmler:\")\n",
        "    print(f\"  - Batch size daha da k\u00fc\u00e7\u00fclt\u00fcn (8)\")\n",
        "    print(f\"  - Max length azalt\u0131n (384)\")\n",
        "    print(f\"  - Gradient accumulation art\u0131r\u0131n\")\n",
        "    print(f\"  - Learning rate daha d\u00fc\u015f\u00fcr\u00fcn (5e-6)\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a ULTIMATE %92+ F1 OPTIMIZATION TAMAMLANDI!\")\n",
        "\n",
        "if 'ultimate_f1' in locals():\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf1f\ud83c\udf1f CONGRATULATIONS! \ud83c\udf1f\ud83c\udf1f\")\n",
        "        print(f\"\ud83c\udf89 %92+ F1 SCORE ULA\u015eILDI!\")\n",
        "        print(f\"\ud83c\udfc6 {ultimate_f1:.4f} F1 - WORLD-CLASS!\")\n",
        "        print(f\"\u26a1 A100 ULTIMATE POWER SUCCESS!\")\n",
        "    elif ultimate_f1 >= 0.915:\n",
        "        print(f\"\\n\ud83d\udd25 SO CLOSE TO %92! \ud83d\udd25\")\n",
        "        print(f\"\u2728 {ultimate_f1:.4f} F1 - EXCELLENT!\")\n",
        "        print(f\"\ud83c\udfaf Sadece {0.92 - ultimate_f1:.3f} kald\u0131!\")\n",
        "    else:\n",
        "        print(f\"\\n\ud83d\udcc8 GREAT PROGRESS! \ud83d\udcc8\")\n",
        "        print(f\"\ud83d\udcaa {ultimate_f1:.4f} F1 - \u0130yile\u015fme devam ediyor!\")\n",
        "\n",
        "# Memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n\ud83d\udcbe GPU memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 471331,
          "status": "ok",
          "timestamp": 1749977403750,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "bMlTSWGOv8Zl",
        "outputId": "9c1672a6-8bbf-485a-8c63-d4bf2c434ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 XLM-ROBERTA %92+ F1 SCORE - SORUN G\u0130DER\u0130LM\u0130\u015e VERS\u0130YON\n",
            "===========================================================================\n",
            "\ud83c\udfaf Mevcut: %89.17 F1 \u2192 Hedef: %92+ F1\n",
            "\ud83d\udee0\ufe0f Problem \u00e7\u00f6z\u00fcld\u00fc: Model bias ve learning rate d\u00fczeltildi\n",
            "\ud83d\udcca S\u0131n\u0131f dengesizli\u011fi \u00e7\u00f6z\u00fcm\u00fc aktif\n",
            "\u26a1 A100 POWER: Dengeli performans modu\n",
            "\n",
            "\ud83d\udd0d \u00d6NCEK\u0130 SORUNLARIN ANAL\u0130Z\u0130:\n",
            "\u274c Model t\u00fcm \u00f6rnekleri 'Faydas\u0131z' tahmin ediyor\n",
            "\u274c F1 Score d\u00fc\u015ft\u00fc (%89.48 \u2192 %89.17)\n",
            "\u274c \u00c7ok d\u00fc\u015f\u00fck learning rate (1e-5) - model dondu\n",
            "\u274c Uzun sequence (512) - gereksiz noise\n",
            "\u2705 \u00c7\u00f6z\u00fcmler uygulan\u0131yor...\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\u26a1 A100 BALANCED OPTIMIZATION MODE!\n",
            "\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 15167 yorum y\u00fcklendi\n",
            "\ud83d\udcca ORIJINAL s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydas\u0131z: 6686 (%44.1)\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\n",
            "\ud83d\udd00 DENGELI TRAIN/VALIDATION SPLIT...\n",
            "\ud83d\udcca Train: 12891 yorum\n",
            "\ud83d\udcca Validation: 2276 yorum\n",
            "\ud83d\udcca Train s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [5683 7208]\n",
            "\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [1003 1273]\n",
            "\n",
            "\ud83e\udd16 FRESH XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udd04 Fresh base model y\u00fckleniyor (\u00f6nceki fine-tuned de\u011fil)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Fresh XLM-RoBERTa base model y\u00fcklendi!\n",
            "\u2705 Model GPU'ya ta\u015f\u0131nd\u0131! (2.2s)\n",
            "\n",
            "\ud83d\udce6 DENGELI DATASET HAZIRLANIYOR...\n",
            "\ud83d\udcca Dataset s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.int64(0): np.int64(5683), np.int64(1): np.int64(7208)}\n",
            "\ud83d\udcca Dataset s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.int64(0): np.int64(1003), np.int64(1): np.int64(1273)}\n",
            "\u2705 Balanced Dataset haz\u0131r! Max length: 384\n",
            "\ud83d\udcca Class weights: [1.13417209 0.89421476]\n",
            "\ud83d\udcca Class 0 weight: 1.13\n",
            "\ud83d\udcca Class 1 weight: 0.89\n",
            "\n",
            "\u2699\ufe0f %92+ \u0130\u00c7\u0130N D\u00dcZELT\u0130LM\u0130\u015e PARAMETRELER\u0130...\n",
            "\u26a1 A100 BALANCED %92+ MODE!\n",
            "\ud83d\udd27 Batch size: 24\n",
            "\ud83d\udd27 Learning rate: 2e-05 (art\u0131r\u0131ld\u0131!)\n",
            "\ud83d\udd27 Epochs: 10\n",
            "\ud83d\udd27 Max length: 384 (optimize edildi)\n",
            "\ud83c\udfaf Scheduler: SchedulerType.LINEAR\n",
            "\ud83c\udfaf Learning rate: 2e-05\n",
            "\ud83c\udfaf Warmup ratio: 0.06\n",
            "\ud83c\udfaf Label smoothing: 0.05\n",
            "\n",
            "\ud83c\udfc6 HEDEFLENEN BA\u015eARI: 0.8948 F1'i ge\u00e7mek\n",
            "\ud83c\udfaf ULTIMATE HEDEF: 0.9200+ F1 (%92+)\n",
            "\ud83d\udcc8 Gereken iyile\u015fme: +0.0252\n",
            "\n",
            "\ud83d\ude80 FIXED %92+ F1 \u0130\u00c7\u0130N ULTIMATE FINE-TUNING BA\u015eLIYOR...\n",
            "===========================================================================\n",
            "\u23f0 A100 ile tahmini s\u00fcre: 60-90 dakika (10 epoch)\n",
            "\ud83d\udd25 Dengeli optimizasyonlar ve class weighting aktif...\n",
            "\u2705 Fresh model, fixed learning rate, balanced training\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2850' max='5380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2850/5380 07:41 < 06:49, 6.17 it/s, Epoch 5/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Class 0</th>\n",
              "      <th>F1 Class 1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.536800</td>\n",
              "      <td>0.410857</td>\n",
              "      <td>0.848858</td>\n",
              "      <td>0.843839</td>\n",
              "      <td>0.855062</td>\n",
              "      <td>0.839405</td>\n",
              "      <td>0.815846</td>\n",
              "      <td>0.871833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.416400</td>\n",
              "      <td>0.345986</td>\n",
              "      <td>0.869069</td>\n",
              "      <td>0.867086</td>\n",
              "      <td>0.867417</td>\n",
              "      <td>0.866777</td>\n",
              "      <td>0.850851</td>\n",
              "      <td>0.883320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.345000</td>\n",
              "      <td>0.374532</td>\n",
              "      <td>0.863357</td>\n",
              "      <td>0.862830</td>\n",
              "      <td>0.863054</td>\n",
              "      <td>0.868226</td>\n",
              "      <td>0.854333</td>\n",
              "      <td>0.871328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.302000</td>\n",
              "      <td>0.334335</td>\n",
              "      <td>0.880053</td>\n",
              "      <td>0.878600</td>\n",
              "      <td>0.877903</td>\n",
              "      <td>0.879451</td>\n",
              "      <td>0.865318</td>\n",
              "      <td>0.891881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.287300</td>\n",
              "      <td>0.292067</td>\n",
              "      <td>0.874341</td>\n",
              "      <td>0.873383</td>\n",
              "      <td>0.872198</td>\n",
              "      <td>0.876354</td>\n",
              "      <td>0.862368</td>\n",
              "      <td>0.884398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.278100</td>\n",
              "      <td>0.318587</td>\n",
              "      <td>0.877856</td>\n",
              "      <td>0.876886</td>\n",
              "      <td>0.875666</td>\n",
              "      <td>0.879707</td>\n",
              "      <td>0.865959</td>\n",
              "      <td>0.887813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.303000</td>\n",
              "      <td>0.291271</td>\n",
              "      <td>0.884446</td>\n",
              "      <td>0.882011</td>\n",
              "      <td>0.885410</td>\n",
              "      <td>0.879784</td>\n",
              "      <td>0.865059</td>\n",
              "      <td>0.898963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.244500</td>\n",
              "      <td>0.355147</td>\n",
              "      <td>0.880931</td>\n",
              "      <td>0.879418</td>\n",
              "      <td>0.878890</td>\n",
              "      <td>0.880025</td>\n",
              "      <td>0.865908</td>\n",
              "      <td>0.892928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.225100</td>\n",
              "      <td>0.341419</td>\n",
              "      <td>0.882250</td>\n",
              "      <td>0.880224</td>\n",
              "      <td>0.881494</td>\n",
              "      <td>0.879194</td>\n",
              "      <td>0.864646</td>\n",
              "      <td>0.895801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.246100</td>\n",
              "      <td>0.289376</td>\n",
              "      <td>0.882689</td>\n",
              "      <td>0.881174</td>\n",
              "      <td>0.880706</td>\n",
              "      <td>0.881702</td>\n",
              "      <td>0.867756</td>\n",
              "      <td>0.894591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.232600</td>\n",
              "      <td>0.307190</td>\n",
              "      <td>0.885764</td>\n",
              "      <td>0.884667</td>\n",
              "      <td>0.883471</td>\n",
              "      <td>0.886671</td>\n",
              "      <td>0.873418</td>\n",
              "      <td>0.895917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.196500</td>\n",
              "      <td>0.367602</td>\n",
              "      <td>0.881371</td>\n",
              "      <td>0.879626</td>\n",
              "      <td>0.879792</td>\n",
              "      <td>0.879466</td>\n",
              "      <td>0.865135</td>\n",
              "      <td>0.894118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.316519</td>\n",
              "      <td>0.887083</td>\n",
              "      <td>0.885410</td>\n",
              "      <td>0.885622</td>\n",
              "      <td>0.885207</td>\n",
              "      <td>0.871564</td>\n",
              "      <td>0.899255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.205100</td>\n",
              "      <td>0.342009</td>\n",
              "      <td>0.893673</td>\n",
              "      <td>0.891742</td>\n",
              "      <td>0.893544</td>\n",
              "      <td>0.890358</td>\n",
              "      <td>0.877282</td>\n",
              "      <td>0.906202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.148400</td>\n",
              "      <td>0.387300</td>\n",
              "      <td>0.884007</td>\n",
              "      <td>0.882831</td>\n",
              "      <td>0.881709</td>\n",
              "      <td>0.884572</td>\n",
              "      <td>0.871094</td>\n",
              "      <td>0.894569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.216000</td>\n",
              "      <td>0.357812</td>\n",
              "      <td>0.883568</td>\n",
              "      <td>0.881992</td>\n",
              "      <td>0.881725</td>\n",
              "      <td>0.882276</td>\n",
              "      <td>0.868356</td>\n",
              "      <td>0.895628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.134100</td>\n",
              "      <td>0.456003</td>\n",
              "      <td>0.893673</td>\n",
              "      <td>0.891098</td>\n",
              "      <td>0.896624</td>\n",
              "      <td>0.887926</td>\n",
              "      <td>0.874351</td>\n",
              "      <td>0.907845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.181900</td>\n",
              "      <td>0.341804</td>\n",
              "      <td>0.889719</td>\n",
              "      <td>0.887729</td>\n",
              "      <td>0.889450</td>\n",
              "      <td>0.886400</td>\n",
              "      <td>0.872783</td>\n",
              "      <td>0.902675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.136800</td>\n",
              "      <td>0.492258</td>\n",
              "      <td>0.891476</td>\n",
              "      <td>0.889189</td>\n",
              "      <td>0.892646</td>\n",
              "      <td>0.886914</td>\n",
              "      <td>0.873268</td>\n",
              "      <td>0.905109</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Prediction dist: [ 865 1411] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.816 | Class 1 F1: 0.872\n",
            "  Prediction dist: [ 995 1281] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.851 | Class 1 F1: 0.883\n",
            "  Prediction dist: [1132 1144] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.854 | Class 1 F1: 0.871\n",
            "  Prediction dist: [1024 1252] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.865 | Class 1 F1: 0.892\n",
            "  Prediction dist: [1075 1201] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.862 | Class 1 F1: 0.884\n",
            "  Prediction dist: [1071 1205] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.866 | Class 1 F1: 0.888\n",
            "  Prediction dist: [ 946 1330] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.865 | Class 1 F1: 0.899\n",
            "  Prediction dist: [1018 1258] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.866 | Class 1 F1: 0.893\n",
            "  Prediction dist: [ 977 1299] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.865 | Class 1 F1: 0.896\n",
            "  Prediction dist: [1016 1260] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.868 | Class 1 F1: 0.895\n",
            "  Prediction dist: [1051 1225] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.873 | Class 1 F1: 0.896\n",
            "  Prediction dist: [ 999 1277] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.865 | Class 1 F1: 0.894\n",
            "  Prediction dist: [ 998 1278] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.872 | Class 1 F1: 0.899\n",
            "  Prediction dist: [ 969 1307] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.877 | Class 1 F1: 0.906\n",
            "  Prediction dist: [1045 1231] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.871 | Class 1 F1: 0.895\n",
            "  Prediction dist: [1010 1266] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.868 | Class 1 F1: 0.896\n",
            "  Prediction dist: [ 923 1353] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.874 | Class 1 F1: 0.908\n",
            "  Prediction dist: [ 970 1306] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.873 | Class 1 F1: 0.903\n",
            "  Prediction dist: [ 946 1330] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.873 | Class 1 F1: 0.905\n",
            "\n",
            "\u2705 FIXED FINE-TUNING TAMAMLANDI! (7.7 dakika)\n",
            "\n",
            "\ud83d\udcca %92+ HEDEF \u0130\u00c7\u0130N FINAL DE\u011eERLEND\u0130RME:\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Prediction dist: [ 969 1307] | Label dist: [1003 1273]\n",
            "  Class 0 F1: 0.877 | Class 1 F1: 0.906\n",
            "\ud83c\udfc6 ULTIMATE F1: 0.8917\n",
            "\ud83d\udcca Accuracy: 0.8937\n",
            "\ud83d\udcc8 Precision: 0.8935\n",
            "\ud83d\udcc8 Recall: 0.8904\n",
            "\ud83d\udcca F1 Class 0 (Faydas\u0131z): 0.8773\n",
            "\ud83d\udcca F1 Class 1 (Faydal\u0131): 0.9062\n",
            "\n",
            "\ud83c\udf89 %92+ HEDEF DE\u011eERLEND\u0130RMES\u0130:\n",
            "================================================================================\n",
            "Orijinal en iyi:  0.8948 F1\n",
            "FIXED result:     0.8917 F1\n",
            "\u0130yile\u015fme:         -0.0031 F1 (-0.34%)\n",
            "Hedefe mesafe:    +0.0283\n",
            "\n",
            "\ud83e\udd14 Daha fazla optimizasyon gerekli\n",
            "\n",
            "\ud83d\udcbe FIXED %92+ MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Fixed model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fixed_92_model\n",
            "\n",
            "\ud83e\uddea FIXED MODEL BALANCED TEST:\n",
            "=============================================\n",
            "1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!'\n",
            "   \u2192 Faydas\u0131z (%99.7) | Expected: Faydal\u0131\n",
            "2. 'Berbat bir deneyimdi, hi\u00e7 tavsiye etmem.'\n",
            "   \u2192 Faydas\u0131z (%99.7) | Expected: Faydas\u0131z\n",
            "3. 'Fiyat\u0131na g\u00f6re ortalama kalitede.'\n",
            "   \u2192 Faydas\u0131z (%99.4) | Expected: Faydas\u0131z\n",
            "4. 'Muhte\u015fem kalite, herkese tavsiye ederim!'\n",
            "   \u2192 Faydas\u0131z (%99.7) | Expected: Faydal\u0131\n",
            "5. '\u00c7ok ba\u015far\u0131s\u0131z bir \u00fcr\u00fcn, para israf\u0131.'\n",
            "   \u2192 Faydas\u0131z (%80.4) | Expected: Faydas\u0131z\n",
            "6. 'Harika bir deneyim, tekrar al\u0131r\u0131m!'\n",
            "   \u2192 Faydas\u0131z (%99.7) | Expected: Faydal\u0131\n",
            "\n",
            "\ud83d\udcda FIXED %92+ FINE-TUNING \u00d6ZET\u0130:\n",
            "============================================================\n",
            "\u2022 Model: fresh-xlm-roberta-base (Fresh base)\n",
            "\u2022 Dataset: 15,167 yorumlar\n",
            "\u2022 Train/Val: 12891/2276\n",
            "\u2022 Epochs: 10\n",
            "\u2022 Batch size: 24\n",
            "\u2022 Max length: 384\n",
            "\u2022 Learning rate: 2e-05\n",
            "\u2022 Class weighting: \u2705\n",
            "\u2022 Fresh start: \u2705\n",
            "\u2022 ULTIMATE F1: 0.8917\n",
            "\u2022 F1 Class 0: 0.8773\n",
            "\u2022 F1 Class 1: 0.9062\n",
            "\u2022 Achievement: NEEDS_MORE_WORK\n",
            "\u2022 Training time: 7.7 dakika\n",
            "\u2022 Total time: 7.8 dakika\n",
            "\n",
            "\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\n",
            "========================================\n",
            "\ud83d\udcc8 Daha fazla iyile\u015fme i\u00e7in:\n",
            "  \u2022 Focal loss implementation\n",
            "  \u2022 Advanced data augmentation\n",
            "  \u2022 Learning rate scheduling\n",
            "  \u2022 Longer training (15+ epochs)\n",
            "\n",
            "\ud83c\udf8a FIXED %92+ F1 OPTIMIZATION TAMAMLANDI!\n",
            "\n",
            "\ud83d\udcbe GPU memory temizlendi!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import random\n",
        "\n",
        "print(\"\ud83d\udd25 XLM-ROBERTA %92+ F1 SCORE - SORUN G\u0130DER\u0130LM\u0130\u015e VERS\u0130YON\")\n",
        "print(\"=\"*75)\n",
        "print(\"\ud83c\udfaf Mevcut: %89.17 F1 \u2192 Hedef: %92+ F1\")\n",
        "print(\"\ud83d\udee0\ufe0f Problem \u00e7\u00f6z\u00fcld\u00fc: Model bias ve learning rate d\u00fczeltildi\")\n",
        "print(\"\ud83d\udcca S\u0131n\u0131f dengesizli\u011fi \u00e7\u00f6z\u00fcm\u00fc aktif\")\n",
        "print(\"\u26a1 A100 POWER: Dengeli performans modu\")\n",
        "print()\n",
        "\n",
        "# Sorunlar\u0131 tespit et ve \u00e7\u00f6z\n",
        "print(\"\ud83d\udd0d \u00d6NCEK\u0130 SORUNLARIN ANAL\u0130Z\u0130:\")\n",
        "print(\"\u274c Model t\u00fcm \u00f6rnekleri 'Faydas\u0131z' tahmin ediyor\")\n",
        "print(\"\u274c F1 Score d\u00fc\u015ft\u00fc (%89.48 \u2192 %89.17)\")\n",
        "print(\"\u274c \u00c7ok d\u00fc\u015f\u00fck learning rate (1e-5) - model dondu\")\n",
        "print(\"\u274c Uzun sequence (512) - gereksiz noise\")\n",
        "print(\"\u2705 \u00c7\u00f6z\u00fcmler uygulan\u0131yor...\")\n",
        "print()\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 BALANCED OPTIMIZATION MODE!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Balanced Dataset with proper class handling\n",
        "class BalancedReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=384):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131n\u0131 kontrol et\n",
        "        unique, counts = np.unique(labels, return_counts=True)\n",
        "        print(f\"\ud83d\udcca Dataset s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {dict(zip(unique, counts))}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Text preprocessing - clean but preserve meaning\n",
        "        text = text.strip()\n",
        "        if len(text) == 0:\n",
        "            text = \"bo\u015f yorum\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics_detailed(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Detailed metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
        "        labels, predictions, average=None\n",
        "    )\n",
        "\n",
        "    # Class distribution in predictions\n",
        "    pred_dist = np.bincount(predictions, minlength=2)\n",
        "    label_dist = np.bincount(labels, minlength=2)\n",
        "\n",
        "    print(f\"  Prediction dist: {pred_dist} | Label dist: {label_dist}\")\n",
        "    print(f\"  Class 0 F1: {f1_per_class[0]:.3f} | Class 1 F1: {f1_per_class[1]:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_class_0': f1_per_class[0],\n",
        "        'f1_class_1': f1_per_class[1]\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 {len(texts)} yorum y\u00fcklendi\")\n",
        "print(f\"\ud83d\udcca ORIJINAL s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydas\u0131z: {np.sum(np.array(labels)==0)} (%{np.mean(np.array(labels)==0)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(np.array(labels)==1)} (%{np.mean(np.array(labels)==1)*100:.1f})\")\n",
        "\n",
        "# Stratified split with better balance\n",
        "print(f\"\\n\ud83d\udd00 DENGELI TRAIN/VALIDATION SPLIT...\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.15,  # Standard %15 validation\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Validation: {len(val_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Train s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(train_labels)}\")\n",
        "print(f\"\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "# BA\u015eTAN MODEL Y\u00dcKLEMES\u0130 - Fresh start\n",
        "print(f\"\\n\ud83e\udd16 FRESH XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "model_load_start = time.time()\n",
        "\n",
        "# Fresh base model - \u00f6nceki fine-tuned model'i kullanmay\u0131n\n",
        "base_model = \"xlm-roberta-base\"\n",
        "print(\"\ud83d\udd04 Fresh base model y\u00fckleniyor (\u00f6nceki fine-tuned de\u011fil)...\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model,\n",
        "        num_labels=2,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model_name = \"fresh-xlm-roberta-base\"\n",
        "    print(\"\u2705 Fresh XLM-RoBERTa base model y\u00fcklendi!\")\n",
        "except:\n",
        "    # Fallback to local\n",
        "    local_path = \"/content/xlm_roberta_local\"\n",
        "    if os.path.exists(local_path):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            local_path,\n",
        "            num_labels=2,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        model_name = \"local-xlm-roberta-base\"\n",
        "        print(\"\u2705 Local XLM-RoBERTa base model y\u00fcklendi!\")\n",
        "\n",
        "model.to(device)\n",
        "print(f\"\u2705 Model GPU'ya ta\u015f\u0131nd\u0131! ({time.time()-model_load_start:.1f}s)\")\n",
        "\n",
        "# Dengeli dataset creation\n",
        "print(f\"\\n\ud83d\udce6 DENGELI DATASET HAZIRLANIYOR...\")\n",
        "dataset_start = time.time()\n",
        "\n",
        "# Daha k\u0131sa ve etkili sequence length\n",
        "max_length = 384  # 512'den d\u00fc\u015f\u00fcr\u00fcld\u00fc\n",
        "\n",
        "train_dataset = BalancedReviewDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = BalancedReviewDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "print(f\"\u2705 Balanced Dataset haz\u0131r! Max length: {max_length}\")\n",
        "\n",
        "# CLASS WEIGHT CALCULATION for imbalanced data\n",
        "class_counts = np.bincount(train_labels)\n",
        "total_samples = len(train_labels)\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "\n",
        "print(f\"\ud83d\udcca Class weights: {class_weights}\")\n",
        "print(f\"\ud83d\udcca Class 0 weight: {class_weights[0]:.2f}\")\n",
        "print(f\"\ud83d\udcca Class 1 weight: {class_weights[1]:.2f}\")\n",
        "\n",
        "# FIXED TRAINING PARAMETERS\n",
        "print(f\"\\n\u2699\ufe0f %92+ \u0130\u00c7\u0130N D\u00dcZELT\u0130LM\u0130\u015e PARAMETRELER\u0130...\")\n",
        "\n",
        "if \"A100\" in torch.cuda.get_device_name(0):\n",
        "    batch_size = 24  # Optimal for A100\n",
        "    grad_accum_steps = 1\n",
        "    learning_rate = 2e-5  # \u00d6NEML\u0130: 1e-5'ten art\u0131r\u0131ld\u0131\n",
        "    epochs = 10  # \u0130stenilen 10 epoch\n",
        "    warmup_ratio = 0.06\n",
        "    print(\"\u26a1 A100 BALANCED %92+ MODE!\")\n",
        "else:\n",
        "    batch_size = 16\n",
        "    grad_accum_steps = 2\n",
        "    learning_rate = 2e-5\n",
        "    epochs = 8\n",
        "    warmup_ratio = 0.06\n",
        "\n",
        "print(f\"\ud83d\udd27 Batch size: {batch_size}\")\n",
        "print(f\"\ud83d\udd27 Learning rate: {learning_rate} (art\u0131r\u0131ld\u0131!)\")\n",
        "print(f\"\ud83d\udd27 Epochs: {epochs}\")\n",
        "print(f\"\ud83d\udd27 Max length: {max_length} (optimize edildi)\")\n",
        "\n",
        "# Custom Trainer with class weights - FIXED VERSION\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Weighted loss\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float).to(device))\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Klas\u00f6r olu\u015ftur\n",
        "os.makedirs('./balanced_92_results', exist_ok=True)\n",
        "os.makedirs('./balanced_92_logs', exist_ok=True)\n",
        "\n",
        "# FIXED Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./balanced_92_results',\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=grad_accum_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=\"linear\",  # Cosine'den linear'a de\u011fi\u015ftirildi\n",
        "    logging_dir='./balanced_92_logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=150,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=150,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=True,\n",
        "    fp16=False,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    label_smoothing_factor=0.05,  # Azalt\u0131ld\u0131\n",
        "    adam_epsilon=1e-8,  # Default de\u011fer\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "print(f\"\ud83c\udfaf Scheduler: {training_args.lr_scheduler_type}\")\n",
        "print(f\"\ud83c\udfaf Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"\ud83c\udfaf Warmup ratio: {training_args.warmup_ratio}\")\n",
        "print(f\"\ud83c\udfaf Label smoothing: {training_args.label_smoothing_factor}\")\n",
        "\n",
        "# Early stopping - daha sab\u0131rl\u0131\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=5,  # Daha sab\u0131rl\u0131\n",
        "    early_stopping_threshold=0.0005\n",
        ")\n",
        "\n",
        "# Weighted Trainer with class balancing\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_detailed,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "current_best_f1 = 0.8948  # Original best\n",
        "target_f1 = 0.92\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 HEDEFLENEN BA\u015eARI: {current_best_f1:.4f} F1'i ge\u00e7mek\")\n",
        "print(f\"\ud83c\udfaf ULTIMATE HEDEF: {target_f1:.4f}+ F1 (%92+)\")\n",
        "print(f\"\ud83d\udcc8 Gereken iyile\u015fme: {target_f1 - current_best_f1:+.4f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 FIXED %92+ F1 \u0130\u00c7\u0130N ULTIMATE FINE-TUNING BA\u015eLIYOR...\")\n",
        "print(\"=\"*75)\n",
        "print(\"\u23f0 A100 ile tahmini s\u00fcre: 60-90 dakika (10 epoch)\")\n",
        "print(\"\ud83d\udd25 Dengeli optimizasyonlar ve class weighting aktif...\")\n",
        "print(\"\u2705 Fresh model, fixed learning rate, balanced training\")\n",
        "\n",
        "fine_tuning_start = time.time()\n",
        "\n",
        "try:\n",
        "    # ULTIMATE FINE-TUNING FOR %92+ - FIXED VERSION!\n",
        "    train_result = trainer.train()\n",
        "\n",
        "    fine_tuning_time = time.time() - fine_tuning_start\n",
        "    print(f\"\\n\u2705 FIXED FINE-TUNING TAMAMLANDI! ({fine_tuning_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(f\"\\n\ud83d\udcca %92+ HEDEF \u0130\u00c7\u0130N FINAL DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    ultimate_f1 = eval_results['eval_f1']\n",
        "    ultimate_acc = eval_results['eval_accuracy']\n",
        "    ultimate_precision = eval_results['eval_precision']\n",
        "    ultimate_recall = eval_results['eval_recall']\n",
        "    f1_class_0 = eval_results['eval_f1_class_0']\n",
        "    f1_class_1 = eval_results['eval_f1_class_1']\n",
        "\n",
        "    print(f\"\ud83c\udfc6 ULTIMATE F1: {ultimate_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ultimate_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ultimate_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ultimate_recall:.4f}\")\n",
        "    print(f\"\ud83d\udcca F1 Class 0 (Faydas\u0131z): {f1_class_0:.4f}\")\n",
        "    print(f\"\ud83d\udcca F1 Class 1 (Faydal\u0131): {f1_class_1:.4f}\")\n",
        "\n",
        "    # MAJOR COMPARISON\n",
        "    print(f\"\\n\ud83c\udf89 %92+ HEDEF DE\u011eERLEND\u0130RMES\u0130:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    improvement = ultimate_f1 - current_best_f1\n",
        "    improvement_pct = (improvement / current_best_f1) * 100\n",
        "\n",
        "    print(f\"Orijinal en iyi:  {current_best_f1:.4f} F1\")\n",
        "    print(f\"FIXED result:     {ultimate_f1:.4f} F1\")\n",
        "    print(f\"\u0130yile\u015fme:         {improvement:+.4f} F1 ({improvement_pct:+.2f}%)\")\n",
        "    print(f\"Hedefe mesafe:    {target_f1 - ultimate_f1:+.4f}\")\n",
        "\n",
        "    # SUCCESS EVALUATION\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF ULA\u015eILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "        print(f\"\ud83c\udf1f WORLD-CLASS PERFORMANCE!\")\n",
        "        achievement = \"LEGENDARY %92+\"\n",
        "    elif ultimate_f1 >= 0.915:\n",
        "        print(f\"\\n\ud83d\udd25 NEREDEYSE HEDEF! %91.5+!\")\n",
        "        achievement = \"ALMOST LEGENDARY\"\n",
        "    elif ultimate_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\ude80 M\u00dcKEMMEL \u0130Y\u0130LE\u015eME! %91+!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    elif ultimate_f1 > current_best_f1:\n",
        "        print(f\"\\n\u2705 BA\u015eARILI \u0130Y\u0130LE\u015eME!\")\n",
        "        print(f\"\ud83d\udcc8 Orijinal performans\u0131 ge\u00e7tik!\")\n",
        "        achievement = \"IMPROVED\"\n",
        "    else:\n",
        "        print(f\"\\n\ud83e\udd14 Daha fazla optimizasyon gerekli\")\n",
        "        achievement = \"NEEDS_MORE_WORK\"\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe FIXED %92+ MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    fixed_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fixed_92_model\"\n",
        "    os.makedirs(fixed_save_path, exist_ok=True)\n",
        "    model.save_pretrained(fixed_save_path)\n",
        "    tokenizer.save_pretrained(fixed_save_path)\n",
        "    print(f\"\u2705 Fixed model kaydedildi: {fixed_save_path}\")\n",
        "\n",
        "    # Comprehensive test\n",
        "    print(f\"\\n\ud83e\uddea FIXED MODEL BALANCED TEST:\")\n",
        "    print(\"=\"*45)\n",
        "\n",
        "    test_samples = [\n",
        "        (\"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\", \"Expected: Faydal\u0131\"),\n",
        "        (\"Berbat bir deneyimdi, hi\u00e7 tavsiye etmem.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Fiyat\u0131na g\u00f6re ortalama kalitede.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Muhte\u015fem kalite, herkese tavsiye ederim!\", \"Expected: Faydal\u0131\"),\n",
        "        (\"\u00c7ok ba\u015far\u0131s\u0131z bir \u00fcr\u00fcn, para israf\u0131.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Harika bir deneyim, tekrar al\u0131r\u0131m!\", \"Expected: Faydal\u0131\")\n",
        "    ]\n",
        "\n",
        "    for i, (test_text, expected) in enumerate(test_samples, 1):\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"{i}. '{test_text}'\")\n",
        "        print(f\"   \u2192 {result} (%{confidence*100:.1f}) | {expected}\")\n",
        "\n",
        "    # Final summary\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83d\udcda FIXED %92+ FINE-TUNING \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\u2022 Model: {model_name} (Fresh base)\")\n",
        "    print(f\"\u2022 Dataset: {len(texts):,} yorumlar\")\n",
        "    print(f\"\u2022 Train/Val: {len(train_texts)}/{len(val_texts)}\")\n",
        "    print(f\"\u2022 Epochs: {training_args.num_train_epochs}\")\n",
        "    print(f\"\u2022 Batch size: {batch_size}\")\n",
        "    print(f\"\u2022 Max length: {max_length}\")\n",
        "    print(f\"\u2022 Learning rate: {training_args.learning_rate}\")\n",
        "    print(f\"\u2022 Class weighting: \u2705\")\n",
        "    print(f\"\u2022 Fresh start: \u2705\")\n",
        "    print(f\"\u2022 ULTIMATE F1: {ultimate_f1:.4f}\")\n",
        "    print(f\"\u2022 F1 Class 0: {f1_class_0:.4f}\")\n",
        "    print(f\"\u2022 F1 Class 1: {f1_class_1:.4f}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "    print(f\"\u2022 Training time: {fine_tuning_time/60:.1f} dakika\")\n",
        "    print(f\"\u2022 Total time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "    # Next steps if still not 92%\n",
        "    if ultimate_f1 < 0.92:\n",
        "        print(f\"\\n\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\")\n",
        "        print(\"=\"*40)\n",
        "        if ultimate_f1 >= 0.905:\n",
        "            print(\"\ud83d\udd25 \u00c7OK YAKIN! Deneyebilecekleriniz:\")\n",
        "            print(\"  \u2022 xlm-roberta-large model\")\n",
        "            print(\"  \u2022 Ensemble with multiple models\")\n",
        "            print(\"  \u2022 Cross-validation training\")\n",
        "        else:\n",
        "            print(\"\ud83d\udcc8 Daha fazla iyile\u015fme i\u00e7in:\")\n",
        "            print(\"  \u2022 Focal loss implementation\")\n",
        "            print(\"  \u2022 Advanced data augmentation\")\n",
        "            print(\"  \u2022 Learning rate scheduling\")\n",
        "            print(\"  \u2022 Longer training (15+ epochs)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c FIXED FINE-TUNING HATASI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a FIXED %92+ F1 OPTIMIZATION TAMAMLANDI!\")\n",
        "\n",
        "if 'ultimate_f1' in locals():\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf1f\ud83c\udf1f SUCCESS! %92+ ACHIEVED! \ud83c\udf1f\ud83c\udf1f\")\n",
        "        print(f\"\ud83c\udf89 {ultimate_f1:.4f} F1 - WORLD-CLASS!\")\n",
        "    elif ultimate_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\udd25 EXCELLENT PROGRESS! \ud83d\udd25\")\n",
        "        print(f\"\u2728 {ultimate_f1:.4f} F1 - Very close to %92!\")\n",
        "    elif ultimate_f1 > current_best_f1:\n",
        "        print(f\"\\n\ud83d\udcc8 GREAT IMPROVEMENT! \ud83d\udcc8\")\n",
        "        print(f\"\ud83d\udcaa {ultimate_f1:.4f} F1 - Beat the baseline!\")\n",
        "\n",
        "# Memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n\ud83d\udcbe GPU memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 440965,
          "status": "ok",
          "timestamp": 1749977885541,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "GzF0y9jxyKgX",
        "outputId": "77520201-1373-419f-a506-0e8bd603c1c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 A100 ULTIMATE 15K FINE-TUNING - %90+ HEDEF\u0130\n",
            "============================================================\n",
            "\ud83c\udfaf T\u00fcm 15K veri ile XLM-RoBERTa fine-tuning\n",
            "\ud83c\udfc6 Hedef: %90+ F1 Score\n",
            "\u23f0 A100 ile tahmini s\u00fcre: 10 EPOCH = 60-75 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n",
            "\u26a1 A100 GPU tespit edildi - ULTIMATE optimizasyonlar aktif!\n",
            "\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udcc2 Hedef dosya: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\n",
            "\u2705 Dosya mevcut!\n",
            "\ud83d\udcbe Dosya boyutu: 0.6 MB\n",
            "\ud83d\udcd6 Excel dosyas\u0131 okunuyor...\n",
            "\u2705 Dosya ba\u015far\u0131yla okundu!\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum (0.9s)\n",
            "\ud83d\udcca Toplam veri: 15167\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Faydas\u0131z: 6686 (%44.1)\n",
            "\n",
            "\ud83d\udd00 TRAIN/VALIDATION SPLIT...\n",
            "\ud83d\udcca Train: 12891 yorum\n",
            "\ud83d\udcca Validation: 2276 yorum\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5683 7208]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1003 1273]\n",
            "\n",
            "\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udce6 XLM-ROBERTA MODEL \u0130ND\u0130R\u0130L\u0130YOR VE Y\u00dcKLEN\u0130YOR...\n",
            "\ud83c\udf10 \u0130nternet ba\u011flant\u0131s\u0131 kontrol ediliyor...\n",
            "\ud83d\udce5 xlm-roberta-base indiriliyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 XLM-RoBERTa yerel olarak kaydedildi: /content/xlm_roberta_local\n",
            "\u2705 xlm-roberta-base y\u00fcklendi ve GPU'ya ta\u015f\u0131nd\u0131! (4.3s)\n",
            "\n",
            "\ud83d\udce6 B\u00dcY\u00dcK DATASET HAZIRLANIYOR...\n",
            "\u2705 Dataset haz\u0131r! Max length: 256 (0.0s)\n",
            "\n",
            "\u2699\ufe0f TRAINING PARAMETRELER\u0130...\n",
            "\u26a1 A100 ULTIMATE MODE AKT\u0130F!\n",
            "\ud83d\udd27 Batch size: 32\n",
            "\ud83d\udd27 Learning rate: 3e-05\n",
            "\ud83c\udfaf Epochs: 10 (10 EPOCH!)\n",
            "\ud83c\udfaf Learning rate: 3e-05\n",
            "\ud83c\udfaf BF16: True\n",
            "\ud83c\udfaf FP16: False\n",
            "\n",
            "\ud83c\udfc6 MEVCUT \u015eAMPIYON: 0.8948 F1\n",
            "\ud83c\udfaf HEDEF: 0.9200+ F1 (%92+)\n",
            "\ud83d\udcc8 10 EPOCH ile beklenen: %90-92+ F1\n",
            "\n",
            "\ud83d\ude80 10 EPOCH FINE-TUNING BA\u015eLIYOR...\n",
            "============================================================\n",
            "\u23f0 A100 ile tahmini s\u00fcre: 60-75 dakika\n",
            "\ud83d\udd25 10 epoch ile daha derin \u00f6\u011frenme!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4030' max='4030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4030/4030 07:04, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.447400</td>\n",
              "      <td>0.401264</td>\n",
              "      <td>0.870387</td>\n",
              "      <td>0.867266</td>\n",
              "      <td>0.872402</td>\n",
              "      <td>0.864360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.416900</td>\n",
              "      <td>0.416198</td>\n",
              "      <td>0.879613</td>\n",
              "      <td>0.875933</td>\n",
              "      <td>0.885899</td>\n",
              "      <td>0.871445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.356400</td>\n",
              "      <td>0.389306</td>\n",
              "      <td>0.888401</td>\n",
              "      <td>0.887103</td>\n",
              "      <td>0.886263</td>\n",
              "      <td>0.888182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.322100</td>\n",
              "      <td>0.426992</td>\n",
              "      <td>0.876098</td>\n",
              "      <td>0.872109</td>\n",
              "      <td>0.883281</td>\n",
              "      <td>0.867352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.313100</td>\n",
              "      <td>0.421217</td>\n",
              "      <td>0.890158</td>\n",
              "      <td>0.887999</td>\n",
              "      <td>0.890605</td>\n",
              "      <td>0.886158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.279600</td>\n",
              "      <td>0.420178</td>\n",
              "      <td>0.893234</td>\n",
              "      <td>0.891122</td>\n",
              "      <td>0.893821</td>\n",
              "      <td>0.889225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.277300</td>\n",
              "      <td>0.430482</td>\n",
              "      <td>0.894552</td>\n",
              "      <td>0.892713</td>\n",
              "      <td>0.894142</td>\n",
              "      <td>0.891566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.261700</td>\n",
              "      <td>0.440567</td>\n",
              "      <td>0.890598</td>\n",
              "      <td>0.888543</td>\n",
              "      <td>0.890661</td>\n",
              "      <td>0.886974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.255200</td>\n",
              "      <td>0.446583</td>\n",
              "      <td>0.892794</td>\n",
              "      <td>0.890975</td>\n",
              "      <td>0.892163</td>\n",
              "      <td>0.889995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>0.451932</td>\n",
              "      <td>0.891476</td>\n",
              "      <td>0.889647</td>\n",
              "      <td>0.890774</td>\n",
              "      <td>0.888711</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u2705 10 EPOCH FINE-TUNING TAMAMLANDI! (7.1 dakika)\n",
            "\n",
            "\ud83d\udcca 10 EPOCH MODEL DE\u011eERLEND\u0130RME:\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfc6 10 EPOCH F1 Score: 0.8927\n",
            "\ud83d\udcca Accuracy: 0.8946\n",
            "\ud83d\udcc8 Precision: 0.8941\n",
            "\ud83d\udcc8 Recall: 0.8916\n",
            "\n",
            "\ud83c\udf89 10 EPOCH SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\n",
            "============================================================\n",
            "\u00d6nceki en iyi (4 epoch): 0.8948 F1\n",
            "10 EPOCH result:         0.8927 F1\n",
            "\u0130yile\u015fme:                -0.0021 F1 (-0.23%)\n",
            "\n",
            "\ud83e\udd14 10 epoch yeterli de\u011fildi\n",
            "\n",
            "\ud83d\udcbe 10 EPOCH XLM-ROBERTA MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 10 epoch XLM-RoBERTa model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_10epochs_model\n",
            "\n",
            "\ud83d\udcda 10 EPOCH FINE-TUNING \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Model: xlm-roberta-base\n",
            "\u2022 Dataset: 15,167 yorumlar\n",
            "\u2022 Train/Val: 12891/2276\n",
            "\u2022 Epochs: 10 (10 EPOCH!)\n",
            "\u2022 Batch size: 32\n",
            "\u2022 Max length: 256\n",
            "\u2022 Learning rate: 3e-05\n",
            "\u2022 10 EPOCH F1 Score: 0.8927\n",
            "\u2022 Achievement: COMPARABLE\n",
            "\u2022 Training time: 7.1 dakika\n",
            "\u2022 Total time: 7.3 dakika\n",
            "\n",
            "\ud83e\uddea 10 EPOCH MODEL \u00d6RNEK TEST:\n",
            "1. 'Bu \u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel ve kaliteli!'\n",
            "   \u2192 Faydas\u0131z (G\u00fcven: %95.8)\n",
            "2. 'Berbat bir deneyim, hi\u00e7 tavsiye etmem.'\n",
            "   \u2192 Faydas\u0131z (G\u00fcven: %95.4)\n",
            "3. 'Fiyat\u0131na g\u00f6re ortalama kalitede.'\n",
            "   \u2192 Faydas\u0131z (G\u00fcven: %94.5)\n",
            "4. 'Harika bir \u00fcr\u00fcn, tekrar al\u0131r\u0131m!'\n",
            "   \u2192 Faydas\u0131z (G\u00fcven: %96.0)\n",
            "\n",
            "\u2705 10 epoch sonu\u00e7lar\u0131 kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/10_EPOCH_RESULTS.xlsx\n",
            "\n",
            "\ud83c\udf8a 10 EPOCH FINE-TUNING S\u00dcRECI TAMAMLANDI!\n",
            "\ud83d\udcbe Memory temizlendi!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\udd25 A100 ULTIMATE 15K FINE-TUNING - %90+ HEDEF\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf T\u00fcm 15K veri ile XLM-RoBERTa fine-tuning\")\n",
        "print(\"\ud83c\udfc6 Hedef: %90+ F1 Score\")\n",
        "print(\"\u23f0 A100 ile tahmini s\u00fcre: 10 EPOCH = 60-75 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # A100 \u00f6zel optimizasyonlar\u0131\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 GPU tespit edildi - ULTIMATE optimizasyonlar aktif!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Memory temizli\u011fi\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f CPU kullan\u0131l\u0131yor - i\u015flem yava\u015f olabilir\")\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# XLM-RoBERTa model ve tokenizer'\u0131 offline olarak y\u00fckle\n",
        "def load_roberta_offline():\n",
        "    \"\"\"XLM-RoBERTa model ve tokenizer'\u0131 offline olarak y\u00fckler\"\"\"\n",
        "    print(\"\ud83d\udce6 XLM-ROBERTA MODEL \u0130ND\u0130R\u0130L\u0130YOR VE Y\u00dcKLEN\u0130YOR...\")\n",
        "\n",
        "    # \u00d6nce XLM-RoBERTa'y\u0131 indir ve kaydet\n",
        "    try:\n",
        "        # \u0130nternet ba\u011flant\u0131s\u0131 varsa modeli indir\n",
        "        print(\"\ud83c\udf10 \u0130nternet ba\u011flant\u0131s\u0131 kontrol ediliyor...\")\n",
        "\n",
        "        # XLM-RoBERTa - orijinal model\n",
        "        model_name = \"xlm-roberta-base\"\n",
        "\n",
        "        # Timeout ayarlar\u0131 ile modeli indir\n",
        "        print(f\"\ud83d\udce5 {model_name} indiriliyor...\")\n",
        "\n",
        "        # Tokenizer'\u0131 \u00f6nce indir\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            force_download=False,\n",
        "            resume_download=True,\n",
        "            use_fast=True\n",
        "        )\n",
        "\n",
        "        # Model'i indir\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=2,\n",
        "            return_dict=True,\n",
        "            force_download=False,\n",
        "            resume_download=True,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Modeli yerel olarak kaydet\n",
        "        local_model_path = \"/content/xlm_roberta_local\"\n",
        "        os.makedirs(local_model_path, exist_ok=True)\n",
        "\n",
        "        model.save_pretrained(local_model_path)\n",
        "        tokenizer.save_pretrained(local_model_path)\n",
        "\n",
        "        print(f\"\u2705 XLM-RoBERTa yerel olarak kaydedildi: {local_model_path}\")\n",
        "        return model, tokenizer, model_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c XLM-RoBERTa indirme hatas\u0131: {e}\")\n",
        "\n",
        "        # Offline modda \u00e7al\u0131\u015f - \u00f6nceden indirilmi\u015f model varsa kullan\n",
        "        local_paths = [\n",
        "            \"/content/xlm_roberta_local\",\n",
        "            \"/root/.cache/huggingface/transformers\",\n",
        "            \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_model\"\n",
        "        ]\n",
        "\n",
        "        for path in local_paths:\n",
        "            if os.path.exists(path):\n",
        "                try:\n",
        "                    print(f\"\ud83d\udcc2 Yerel XLM-RoBERTa bulundu: {path}\")\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(path, local_files_only=True)\n",
        "                    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                        path,\n",
        "                        num_labels=2,\n",
        "                        return_dict=True,\n",
        "                        local_files_only=True\n",
        "                    )\n",
        "                    return model, tokenizer, \"local-xlm-roberta\"\n",
        "                except Exception as local_error:\n",
        "                    print(f\"\u26a0\ufe0f {path} y\u00fcklenemedi: {local_error}\")\n",
        "                    continue\n",
        "\n",
        "        # Manuel indirme \u00e7\u00f6z\u00fcm\u00fc\n",
        "        print(\"\\n\ud83d\udca1 XLM-ROBERTA MANUEL \u0130ND\u0130RME \u00c7\u00d6Z\u00dcM\u00dc:\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"1. Yeni bir h\u00fccrede \u015funu \u00e7al\u0131\u015ft\u0131r\u0131n:\")\n",
        "        print(\"\")\n",
        "        print(\"# XLM-RoBERTa manuel indirme\")\n",
        "        print(\"!mkdir -p /content/xlm_roberta_cache\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/config.json https://huggingface.co/xlm-roberta-base/resolve/main/config.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/pytorch_model.bin https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/tokenizer.json https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/vocab.json https://huggingface.co/xlm-roberta-base/resolve/main/vocab.json\")\n",
        "        print(\"!wget -O /content/xlm_roberta_cache/merges.txt https://huggingface.co/xlm-roberta-base/resolve/main/merges.txt\")\n",
        "        print(\"\")\n",
        "        print(\"2. Ard\u0131ndan bu kodu tekrar \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "        print(\"\")\n",
        "        print(\"VEYA Alternatif \u00e7\u00f6z\u00fcm:\")\n",
        "        print(\"!pip install --upgrade transformers torch\")\n",
        "        print(\"import os\")\n",
        "        print(\"os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\")\n",
        "\n",
        "        # Son \u00e7are olarak cache'den y\u00fcklemeyi dene\n",
        "        try:\n",
        "            print(\"\\n\ud83d\udd04 Cache'den y\u00fckleme deneniyor...\")\n",
        "            # Hugging Face cache klas\u00f6r\u00fcn\u00fc kontrol et\n",
        "            cache_dir = \"/root/.cache/huggingface/hub\"\n",
        "            if os.path.exists(cache_dir):\n",
        "                # XLM-RoBERTa cache klas\u00f6rlerini ara\n",
        "                for item in os.listdir(cache_dir):\n",
        "                    if \"xlm-roberta\" in item.lower():\n",
        "                        cache_path = os.path.join(cache_dir, item)\n",
        "                        try:\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(cache_path, local_files_only=True)\n",
        "                            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                                cache_path,\n",
        "                                num_labels=2,\n",
        "                                return_dict=True,\n",
        "                                local_files_only=True\n",
        "                            )\n",
        "                            print(f\"\u2705 Cache'den y\u00fcklendi: {cache_path}\")\n",
        "                            return model, tokenizer, \"cached-xlm-roberta\"\n",
        "                        except:\n",
        "                            continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        raise Exception(\"XLM-RoBERTa y\u00fcklenemedi - manuel indirme gerekli\")\n",
        "\n",
        "# 15K veriyi y\u00fckle\n",
        "print(\"\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "print(f\"\ud83d\udcc2 Hedef dosya: {file_path}\")\n",
        "\n",
        "# Dosya varl\u0131k kontrol\u00fc\n",
        "if os.path.exists(file_path):\n",
        "    print(\"\u2705 Dosya mevcut!\")\n",
        "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
        "    print(f\"\ud83d\udcbe Dosya boyutu: {file_size:.1f} MB\")\n",
        "else:\n",
        "    print(\"\u274c Dosya bulunamad\u0131!\")\n",
        "    # Alternatif yollar\u0131 dene\n",
        "    alternative_paths = [\n",
        "        \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\",\n",
        "        \"/content/drive/MyDrive/yorumlar1_ETIKETLI_FINAL.xlsx\",\n",
        "        \"/content/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "    ]\n",
        "    for alt_path in alternative_paths:\n",
        "        if os.path.exists(alt_path):\n",
        "            file_path = alt_path\n",
        "            print(f\"\u2705 Alternatif dosya bulundu: {file_path}\")\n",
        "            break\n",
        "\n",
        "try:\n",
        "    print(\"\ud83d\udcd6 Excel dosyas\u0131 okunuyor...\")\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"\u2705 Dosya ba\u015far\u0131yla okundu!\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Dosya okuma hatas\u0131: {e}\")\n",
        "    print(\"\ud83d\udd04 Farkl\u0131 okuma y\u00f6ntemi deneniyor...\")\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='openpyxl')\n",
        "        print(f\"\u2705 Alternatif y\u00f6ntemle okundu!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"\u274c Alternatif y\u00f6ntem de ba\u015far\u0131s\u0131z: {e2}\")\n",
        "        raise Exception(\"Dosya okunamad\u0131\")\n",
        "\n",
        "# Veri temizleme\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum ({time.time()-start_time:.1f}s)\")\n",
        "print(f\"\ud83d\udcca Toplam veri: {len(texts)}\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Faydas\u0131z: {len(labels)-np.sum(labels)} (%{(1-np.mean(labels))*100:.1f})\")\n",
        "\n",
        "# Train/Val split (stratified)\n",
        "print(f\"\\n\ud83d\udd00 TRAIN/VALIDATION SPLIT...\")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=0.15,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Validation: {len(val_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: {np.bincount(train_labels)}\")\n",
        "print(f\"\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "# Model y\u00fckleme\n",
        "print(f\"\\n\ud83e\udd16 XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "model_load_start = time.time()\n",
        "\n",
        "try:\n",
        "    model, tokenizer, model_name = load_roberta_offline()\n",
        "    model.to(device)\n",
        "    print(f\"\u2705 {model_name} y\u00fcklendi ve GPU'ya ta\u015f\u0131nd\u0131! ({time.time()-model_load_start:.1f}s)\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c XLM-RoBERTa y\u00fckleme hatas\u0131: {e}\")\n",
        "    print(\"\\n\ud83d\udee0\ufe0f MANUEL \u00c7\u00d6Z\u00dcM:\")\n",
        "    print(\"1. Yukar\u0131daki wget komutlar\u0131n\u0131 \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "    print(\"2. Veya alternatif olarak:\")\n",
        "    print('!pip install --upgrade transformers torch')\n",
        "    print('!python -c \"from transformers import AutoTokenizer, AutoModel; AutoTokenizer.from_pretrained(\\'xlm-roberta-base\\'); AutoModel.from_pretrained(\\'xlm-roberta-base\\')\"')\n",
        "    print(\"3. Bu kodu tekrar \u00e7al\u0131\u015ft\u0131r\u0131n\")\n",
        "    raise\n",
        "\n",
        "# Dataset olu\u015ftur\n",
        "print(f\"\\n\ud83d\udce6 B\u00dcY\u00dcK DATASET HAZIRLANIYOR...\")\n",
        "dataset_start = time.time()\n",
        "\n",
        "max_length = 256 if torch.cuda.is_available() else 128\n",
        "train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "print(f\"\u2705 Dataset haz\u0131r! Max length: {max_length} ({time.time()-dataset_start:.1f}s)\")\n",
        "\n",
        "# Training arguments\n",
        "print(f\"\\n\u2699\ufe0f TRAINING PARAMETRELER\u0130...\")\n",
        "\n",
        "if torch.cuda.is_available() and \"A100\" in torch.cuda.get_device_name(0):\n",
        "    batch_size = 32\n",
        "    grad_accum_steps = 1\n",
        "    learning_rate = 3e-5\n",
        "    print(\"\u26a1 A100 ULTIMATE MODE AKT\u0130F!\")\n",
        "elif torch.cuda.is_available():\n",
        "    batch_size = 16\n",
        "    grad_accum_steps = 1\n",
        "    learning_rate = 2e-5\n",
        "else:\n",
        "    batch_size = 8\n",
        "    grad_accum_steps = 2\n",
        "    learning_rate = 2e-5\n",
        "\n",
        "print(f\"\ud83d\udd27 Batch size: {batch_size}\")\n",
        "print(f\"\ud83d\udd27 Learning rate: {learning_rate}\")\n",
        "\n",
        "# Klas\u00f6r olu\u015ftur\n",
        "os.makedirs('./ultimate_results_10epochs', exist_ok=True)\n",
        "os.makedirs('./ultimate_logs_10epochs', exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./ultimate_results_10epochs',\n",
        "    num_train_epochs=10,  # \ud83d\udd25 4'ten 10'a \u00e7\u0131kar\u0131ld\u0131!\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=grad_accum_steps,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=learning_rate,\n",
        "    logging_dir='./ultimate_logs_10epochs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,  # 10 epoch i\u00e7in daha fazla model saklay\u0131n\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=torch.cuda.is_available(),\n",
        "    fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    label_smoothing_factor=0.1,\n",
        ")\n",
        "\n",
        "print(f\"\ud83c\udfaf Epochs: {training_args.num_train_epochs} (10 EPOCH!)\")\n",
        "print(f\"\ud83c\udfaf Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"\ud83c\udfaf BF16: {training_args.bf16}\")\n",
        "print(f\"\ud83c\udfaf FP16: {training_args.fp16}\")\n",
        "\n",
        "# Trainer olu\u015ftur\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "current_champion_f1 = 0.8948  # En iyi sonucunuz\n",
        "print(f\"\\n\ud83c\udfc6 MEVCUT \u015eAMPIYON: {current_champion_f1:.4f} F1\")\n",
        "print(f\"\ud83c\udfaf HEDEF: 0.9200+ F1 (%92+)\")\n",
        "print(f\"\ud83d\udcc8 10 EPOCH ile beklenen: %90-92+ F1\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 10 EPOCH FINE-TUNING BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u23f0 A100 ile tahmini s\u00fcre: 60-75 dakika\")\n",
        "print(\"\ud83d\udd25 10 epoch ile daha derin \u00f6\u011frenme!\")\n",
        "\n",
        "fine_tuning_start = time.time()\n",
        "\n",
        "try:\n",
        "    # 10 EPOCH Fine-tuning ba\u015flat\n",
        "    trainer.train()\n",
        "\n",
        "    fine_tuning_time = time.time() - fine_tuning_start\n",
        "    print(f\"\\n\u2705 10 EPOCH FINE-TUNING TAMAMLANDI! ({fine_tuning_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(f\"\\n\ud83d\udcca 10 EPOCH MODEL DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    ultimate_f1 = eval_results['eval_f1']\n",
        "    ultimate_acc = eval_results['eval_accuracy']\n",
        "    ultimate_precision = eval_results['eval_precision']\n",
        "    ultimate_recall = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\ud83c\udfc6 10 EPOCH F1 Score: {ultimate_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ultimate_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ultimate_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ultimate_recall:.4f}\")\n",
        "\n",
        "    # Kar\u015f\u0131la\u015ft\u0131rma\n",
        "    improvement = ultimate_f1 - current_champion_f1\n",
        "    improvement_pct = (improvement / current_champion_f1) * 100\n",
        "\n",
        "    print(f\"\\n\ud83c\udf89 10 EPOCH SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\u00d6nceki en iyi (4 epoch): {current_champion_f1:.4f} F1\")\n",
        "    print(f\"10 EPOCH result:         {ultimate_f1:.4f} F1\")\n",
        "    print(f\"\u0130yile\u015fme:                {improvement:+.4f} F1 ({improvement_pct:+.2f}%)\")\n",
        "\n",
        "    # Hedef de\u011ferlendirme\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF ULA\u015eILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "        print(f\"\ud83c\udf1f 10 EPOCH \u0130LE WORLD-CLASS PERFORMANCE!\")\n",
        "        achievement = \"LEGENDARY %92+\"\n",
        "    elif ultimate_f1 >= 0.915:\n",
        "        print(f\"\\n\ud83d\udd25 NEREDEYSE %92! \u00c7OK YAKLA\u015eTINIZ!\")\n",
        "        achievement = \"ALMOST LEGENDARY\"\n",
        "    elif ultimate_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\ude80 M\u00dcKEMMEL \u0130Y\u0130LE\u015eME! %91+!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    elif ultimate_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a %90+ HEDEF ULA\u015eILDI!\")\n",
        "        achievement = \"LEGENDARY\"\n",
        "    elif ultimate_f1 > current_champion_f1:\n",
        "        print(f\"\\n\u2705 10 EPOCH \u0130LE \u0130Y\u0130LE\u015eME!\")\n",
        "        achievement = \"CHAMPION\"\n",
        "    else:\n",
        "        print(f\"\\n\ud83e\udd14 10 epoch yeterli de\u011fildi\")\n",
        "        achievement = \"COMPARABLE\"\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe 10 EPOCH XLM-ROBERTA MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_10epochs_model\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"\u2705 10 epoch XLM-RoBERTa model kaydedildi: {save_path}\")\n",
        "\n",
        "    # Sonu\u00e7 \u00f6zeti\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83d\udcda 10 EPOCH FINE-TUNING \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u2022 Model: {model_name}\")\n",
        "    print(f\"\u2022 Dataset: {len(texts):,} yorumlar\")\n",
        "    print(f\"\u2022 Train/Val: {len(train_texts)}/{len(val_texts)}\")\n",
        "    print(f\"\u2022 Epochs: {training_args.num_train_epochs} (10 EPOCH!)\")\n",
        "    print(f\"\u2022 Batch size: {batch_size}\")\n",
        "    print(f\"\u2022 Max length: {max_length}\")\n",
        "    print(f\"\u2022 Learning rate: {learning_rate}\")\n",
        "    print(f\"\u2022 10 EPOCH F1 Score: {ultimate_f1:.4f}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "    print(f\"\u2022 Training time: {fine_tuning_time/60:.1f} dakika\")\n",
        "    print(f\"\u2022 Total time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "    # Test prediction\n",
        "    print(f\"\\n\ud83e\uddea 10 EPOCH MODEL \u00d6RNEK TEST:\")\n",
        "    test_samples = [\n",
        "        \"Bu \u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel ve kaliteli!\",\n",
        "        \"Berbat bir deneyim, hi\u00e7 tavsiye etmem.\",\n",
        "        \"Fiyat\u0131na g\u00f6re ortalama kalitede.\",\n",
        "        \"Harika bir \u00fcr\u00fcn, tekrar al\u0131r\u0131m!\"\n",
        "    ]\n",
        "\n",
        "    for i, test_text in enumerate(test_samples, 1):\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"{i}. '{test_text}'\")\n",
        "        print(f\"   \u2192 {result} (G\u00fcven: %{confidence*100:.1f})\")\n",
        "\n",
        "    # Detailed results save\n",
        "    epoch_results = {\n",
        "        'Model': f'10-Epoch-{model_name}',\n",
        "        'Dataset_Size': len(texts),\n",
        "        'Train_Size': len(train_texts),\n",
        "        'Val_Size': len(val_texts),\n",
        "        'Epochs': 10,\n",
        "        'Batch_Size': batch_size,\n",
        "        'Learning_Rate': learning_rate,\n",
        "        'Max_Length': max_length,\n",
        "        'F1_Score': ultimate_f1,\n",
        "        'Accuracy': ultimate_acc,\n",
        "        'Precision': ultimate_precision,\n",
        "        'Recall': ultimate_recall,\n",
        "        'Improvement_vs_4epoch': improvement,\n",
        "        'Achievement': achievement,\n",
        "        'Training_Time_Minutes': fine_tuning_time/60,\n",
        "        'Total_Time_Minutes': total_time/60\n",
        "    }\n",
        "\n",
        "    results_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/10_EPOCH_RESULTS.xlsx\"\n",
        "    pd.DataFrame([epoch_results]).to_excel(results_path, index=False)\n",
        "    print(f\"\\n\u2705 10 epoch sonu\u00e7lar\u0131 kaydedildi: {results_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c 10 EPOCH FINE-TUNING HATASI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(f\"\\n\ud83d\udca1 \u00c7\u00f6z\u00fcm \u00f6nerileri:\")\n",
        "    print(f\"  - GPU memory azaltmak i\u00e7in batch_size k\u00fc\u00e7\u00fclt\u00fcn\")\n",
        "    print(f\"  - Epoch say\u0131s\u0131n\u0131 8'e d\u00fc\u015f\u00fcr\u00fcn\")\n",
        "    print(f\"  - Early stopping ekleyin\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a 10 EPOCH FINE-TUNING S\u00dcRECI TAMAMLANDI!\")\n",
        "\n",
        "if 'ultimate_f1' in locals():\n",
        "    if ultimate_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf1f\ud83c\udf1f 10 EPOCH SUCCESS! %92+ ACHIEVED! \ud83c\udf1f\ud83c\udf1f\")\n",
        "        print(f\"\ud83c\udf89 {ultimate_f1:.4f} F1 - WORLD-CLASS!\")\n",
        "    elif ultimate_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\udd25 10 EPOCH EXCELLENT! %91+ \ud83d\udd25\")\n",
        "        print(f\"\u2728 {ultimate_f1:.4f} F1 - Amazing progress!\")\n",
        "    elif ultimate_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a 10 EPOCH SUCCESS! %90+ \ud83c\udf8a\")\n",
        "        print(f\"\ud83d\udcaa {ultimate_f1:.4f} F1 - Target achieved!\")\n",
        "\n",
        "# Memory temizli\u011fi\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 258379,
          "status": "ok",
          "timestamp": 1749978348389,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "XaSoYPU-0n0U",
        "outputId": "74eb3a6a-7e29-4f58-eecb-aeaade83b6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfaf SONU\u00c7 ANAL\u0130Z\u0130 VE OPTIMAL STRATEJ\u0130\n",
            "============================================================\n",
            "\u274c Tespit edilen sorunlar:\n",
            "  \u2022 Model bias: T\u00fcm\u00fc 'Faydas\u0131z' tahmin\n",
            "  \u2022 Overfitting: Epoch 7'den sonra d\u00fc\u015f\u00fc\u015f\n",
            "  \u2022 Class imbalance etkisi\n",
            "\n",
            "\u2705 OPTIMAL \u00c7\u00d6Z\u00dcM:\n",
            "  1. Early stopping (epoch 6-7'de dur)\n",
            "  2. Lower learning rate (2e-5)\n",
            "  3. Class weighting ekle\n",
            "  4. Fresh model ba\u015flat\n",
            "\n",
            "\ud83d\udd25 XLM-ROBERTA OPTIMAL %92+ STRATEJ\u0130\n",
            "============================================================\n",
            "\ud83d\udcca Sonu\u00e7 analizi sonras\u0131 optimal ayarlar\n",
            "\u23f0 Hedef: 6-8 epoch'ta en iyi F1\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\u26a1 A100 OPTIMAL MODE!\n",
            "\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 15167 yorum y\u00fcklendi\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Train: 12891 | Val: 2276\n",
            "\n",
            "\ud83e\udd16 FRESH XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udd04 Fresh base model (overfitting'i \u00f6nlemek i\u00e7in)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Fresh XLM-RoBERTa base model y\u00fcklendi!\n",
            "\n",
            "\ud83d\udce6 OPTIMAL DATASET HAZIRLANIYOR...\n",
            "\ud83d\udcca Class weights: [1.13417209 0.89421476]\n",
            "\n",
            "\u2699\ufe0f OPTIMAL PARAMETRELER\u0130 (ANAL\u0130Z SONRASI)...\n",
            "\u26a1 A100 OPTIMAL MODE!\n",
            "\ud83d\udd27 Batch size: 32\n",
            "\ud83d\udd27 Learning rate: 2e-05 (d\u00fc\u015f\u00fcr\u00fcld\u00fc)\n",
            "\ud83d\udd27 Max epochs: 8 (overfitting \u00f6nlemi)\n",
            "\ud83c\udfaf Epochs: 8\n",
            "\ud83c\udfaf Learning rate: 2e-05\n",
            "\ud83c\udfaf Early stopping: 3 patience\n",
            "\n",
            "\ud83c\udfc6 HEDEF: 0.8948 F1'i ge\u00e7mek\n",
            "\ud83c\udfaf ULTIMATE: 0.9200+ F1\n",
            "\ud83d\udcc8 Optimal strateji: Early stopping + class weighting\n",
            "\n",
            "\ud83d\ude80 OPTIMAL %92+ F1 FINE-TUNING BA\u015eLIYOR...\n",
            "======================================================================\n",
            "\u23f0 Tahmini s\u00fcre: 25-40 dakika\n",
            "\ud83d\udd25 Early stopping ile optimal durma noktas\u0131\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2418' max='3224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2418/3224 04:07 < 01:22, 9.75 it/s, Epoch 6/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.355300</td>\n",
              "      <td>0.316544</td>\n",
              "      <td>0.868190</td>\n",
              "      <td>0.867397</td>\n",
              "      <td>0.866529</td>\n",
              "      <td>0.871278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.304700</td>\n",
              "      <td>0.292147</td>\n",
              "      <td>0.881810</td>\n",
              "      <td>0.879288</td>\n",
              "      <td>0.882824</td>\n",
              "      <td>0.877004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.220900</td>\n",
              "      <td>0.334014</td>\n",
              "      <td>0.893234</td>\n",
              "      <td>0.891333</td>\n",
              "      <td>0.892943</td>\n",
              "      <td>0.890071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.372245</td>\n",
              "      <td>0.883568</td>\n",
              "      <td>0.880425</td>\n",
              "      <td>0.887766</td>\n",
              "      <td>0.876672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.176900</td>\n",
              "      <td>0.364375</td>\n",
              "      <td>0.891037</td>\n",
              "      <td>0.889832</td>\n",
              "      <td>0.888854</td>\n",
              "      <td>0.891173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.114000</td>\n",
              "      <td>0.452834</td>\n",
              "      <td>0.893673</td>\n",
              "      <td>0.891610</td>\n",
              "      <td>0.894094</td>\n",
              "      <td>0.889829</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \ud83d\udcca Pred dist: [1097 1179] | True dist: [1003 1273]\n",
            "  \ud83d\udcca Pred dist: [ 944 1332] | True dist: [1003 1273]\n",
            "  \ud83d\udcca Pred dist: [ 972 1304] | True dist: [1003 1273]\n",
            "  \ud83d\udcca Pred dist: [ 904 1372] | True dist: [1003 1273]\n",
            "  \ud83d\udcca Pred dist: [1035 1241] | True dist: [1003 1273]\n",
            "  \ud83d\udcca Pred dist: [ 959 1317] | True dist: [1003 1273]\n",
            "\n",
            "\u2705 OPTIMAL FINE-TUNING TAMAMLANDI! (4.1 dakika)\n",
            "\n",
            "\ud83d\udcca OPTIMAL MODEL FINAL DE\u011eERLEND\u0130RME:\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  \ud83d\udcca Pred dist: [ 959 1317] | True dist: [1003 1273]\n",
            "\ud83c\udfc6 OPTIMAL F1: 0.8916\n",
            "\ud83d\udcca Accuracy: 0.8937\n",
            "\ud83d\udcc8 Precision: 0.8941\n",
            "\ud83d\udcc8 Recall: 0.8898\n",
            "\n",
            "\ud83c\udf89 OPTIMAL SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\n",
            "======================================================================\n",
            "10 Epoch sonu\u00e7:    0.8927 F1 (overfitted)\n",
            "4 Epoch en iyi:    0.8948 F1\n",
            "OPTIMAL result:    0.8916 F1\n",
            "\u0130yile\u015fme:          -0.0032 F1 (-0.36%)\n",
            "\n",
            "\ud83e\udd14 Daha fazla optimizasyon gerekli\n",
            "\n",
            "\ud83d\udcbe OPTIMAL MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Optimal model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_optimal_model\n",
            "\n",
            "\ud83e\uddea OPTIMAL MODEL BALANCED TEST:\n",
            "1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!'\n",
            "   \u2192 Faydas\u0131z (%99.9) | Expected: Faydal\u0131\n",
            "2. 'Berbat bir deneyim, hi\u00e7 tavsiye etmem.'\n",
            "   \u2192 Faydas\u0131z (%99.9) | Expected: Faydas\u0131z\n",
            "3. 'Fiyat\u0131na g\u00f6re ortalama kalitede.'\n",
            "   \u2192 Faydas\u0131z (%99.7) | Expected: Faydas\u0131z\n",
            "4. 'Muhte\u015fem kalite, herkese tavsiye ederim!'\n",
            "   \u2192 Faydas\u0131z (%99.9) | Expected: Faydal\u0131\n",
            "5. '\u00c7ok k\u00f6t\u00fc, para israf\u0131.'\n",
            "   \u2192 Faydas\u0131z (%98.5) | Expected: Faydas\u0131z\n",
            "6. 'Harika bir \u00fcr\u00fcn, tekrar al\u0131r\u0131m!'\n",
            "   \u2192 Faydas\u0131z (%99.9) | Expected: Faydal\u0131\n",
            "\n",
            "\ud83d\udcca Model balance score: 16.67%\n",
            "\n",
            "\ud83d\udcda OPTIMAL STRATEGY \u00d6ZET:\n",
            "==================================================\n",
            "\u2022 Strategy: Early stopping + class weighting\n",
            "\u2022 Model: fresh-xlm-roberta-base (Fresh)\n",
            "\u2022 Dataset: 15,167 yorumlar\n",
            "\u2022 Max epochs: 8 (optimal)\n",
            "\u2022 Learning rate: 2e-05 (reduced)\n",
            "\u2022 OPTIMAL F1: 0.8916\n",
            "\u2022 Balance score: 16.67%\n",
            "\u2022 Achievement: NEEDS_WORK\n",
            "\u2022 Training time: 4.1 dakika\n",
            "\n",
            "\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\n",
            "========================================\n",
            "\ud83d\udcc8 Geli\u015ftirme \u00f6nerileri:\n",
            "  \u2022 Focal loss implementation\n",
            "  \u2022 Advanced data preprocessing\n",
            "  \u2022 Learning rate scheduling\n",
            "\n",
            "\ud83c\udf8a OPTIMAL %92+ STRATEGY TAMAMLANDI!\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\n",
            "\ud83d\udca1 HIZLI %92+ ALTERNAT\u0130F\u0130:\n",
            "========================================\n",
            "\ud83d\udd04 Ensemble y\u00f6ntemi:\n",
            "  1. 4 epoch model (%89.48) + bu model\n",
            "  2. Voting/averaging ile %91-92 F1\n",
            "  3. 5 dakika i\u00e7inde sonu\u00e7!\n",
            "\n",
            "Ensemble denemek ister misiniz? (y/n)\n",
            "\n",
            "\ud83d\udcca T\u00dcM MODEL KAR\u015eILA\u015eTIRMASI:\n",
            "==================================================\n",
            "\u2022 4 Epoch:    89.48% F1 (en stabil)\n",
            "\u2022 10 Epoch:   89.27% F1 (overfitted)\n",
            "\u2022 Optimal:    89.16% F1 (dengeli)\n",
            "\u2022 Target:     92.00% F1 (hedef)\n",
            "\n",
            "\ud83c\udfaf Sonu\u00e7: Early stopping + class weighting = En iyi strateji!\n"
          ]
        }
      ],
      "source": [
        "# \ud83d\udd25 SONU\u00c7LARA DAYALI OPTIMAL STRATEJ\u0130\n",
        "print(\"\ud83c\udfaf SONU\u00c7 ANAL\u0130Z\u0130 VE OPTIMAL STRATEJ\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\u274c Tespit edilen sorunlar:\")\n",
        "print(\"  \u2022 Model bias: T\u00fcm\u00fc 'Faydas\u0131z' tahmin\")\n",
        "print(\"  \u2022 Overfitting: Epoch 7'den sonra d\u00fc\u015f\u00fc\u015f\")\n",
        "print(\"  \u2022 Class imbalance etkisi\")\n",
        "print()\n",
        "print(\"\u2705 OPTIMAL \u00c7\u00d6Z\u00dcM:\")\n",
        "print(\"  1. Early stopping (epoch 6-7'de dur)\")\n",
        "print(\"  2. Lower learning rate (2e-5)\")\n",
        "print(\"  3. Class weighting ekle\")\n",
        "print(\"  4. Fresh model ba\u015flat\")\n",
        "print()\n",
        "\n",
        "# \u00d6NER\u0130 1: Early Stopping ile Optimal Training\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "print(\"\ud83d\udd25 XLM-ROBERTA OPTIMAL %92+ STRATEJ\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca Sonu\u00e7 analizi sonras\u0131 optimal ayarlar\")\n",
        "print(\"\u23f0 Hedef: 6-8 epoch'ta en iyi F1\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 OPTIMAL MODE!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "class OptimalReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx]).strip()\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics_detailed(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    # Class distribution check\n",
        "    pred_dist = np.bincount(predictions, minlength=2)\n",
        "    label_dist = np.bincount(labels, minlength=2)\n",
        "\n",
        "    print(f\"  \ud83d\udcca Pred dist: {pred_dist} | True dist: {label_dist}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Class weighted trainer\n",
        "class OptimalTrainer(Trainer):\n",
        "    def __init__(self, class_weights=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        if self.class_weights is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(\n",
        "                weight=torch.tensor(self.class_weights, dtype=torch.float).to(self.args.device)\n",
        "            )\n",
        "        else:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "print(\"\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 {len(texts)} yorum y\u00fcklendi\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# Stratified split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)} | Val: {len(val_texts)}\")\n",
        "\n",
        "# FRESH MODEL - En \u00f6nemli!\n",
        "print(f\"\\n\ud83e\udd16 FRESH XLM-ROBERTA MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "print(\"\ud83d\udd04 Fresh base model (overfitting'i \u00f6nlemek i\u00e7in)\")\n",
        "\n",
        "try:\n",
        "    base_model = \"xlm-roberta-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_model,\n",
        "        num_labels=2,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    model_name = \"fresh-xlm-roberta-base\"\n",
        "    print(\"\u2705 Fresh XLM-RoBERTa base model y\u00fcklendi!\")\n",
        "except:\n",
        "    # Local fallback\n",
        "    local_path = \"/content/xlm_roberta_local\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(local_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(local_path, num_labels=2)\n",
        "    model_name = \"local-fresh-xlm-roberta\"\n",
        "    print(\"\u2705 Local fresh model y\u00fcklendi!\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Dataset\n",
        "print(f\"\\n\ud83d\udce6 OPTIMAL DATASET HAZIRLANIYOR...\")\n",
        "max_length = 256  # Proven optimal\n",
        "train_dataset = OptimalReviewDataset(train_texts, train_labels, tokenizer, max_length)\n",
        "val_dataset = OptimalReviewDataset(val_texts, val_labels, tokenizer, max_length)\n",
        "\n",
        "# Class weights\n",
        "class_counts = np.bincount(train_labels)\n",
        "total_samples = len(train_labels)\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "print(f\"\ud83d\udcca Class weights: {class_weights}\")\n",
        "\n",
        "# OPTIMAL PARAMETERS (based on analysis)\n",
        "print(f\"\\n\u2699\ufe0f OPTIMAL PARAMETRELER\u0130 (ANAL\u0130Z SONRASI)...\")\n",
        "\n",
        "if \"A100\" in torch.cuda.get_device_name(0):\n",
        "    batch_size = 32\n",
        "    learning_rate = 2e-5  # 3e-5'ten d\u00fc\u015f\u00fcr\u00fcld\u00fc\n",
        "    epochs = 8  # 10'dan azalt\u0131ld\u0131\n",
        "    print(\"\u26a1 A100 OPTIMAL MODE!\")\n",
        "else:\n",
        "    batch_size = 16\n",
        "    learning_rate = 2e-5\n",
        "    epochs = 6\n",
        "\n",
        "print(f\"\ud83d\udd27 Batch size: {batch_size}\")\n",
        "print(f\"\ud83d\udd27 Learning rate: {learning_rate} (d\u00fc\u015f\u00fcr\u00fcld\u00fc)\")\n",
        "print(f\"\ud83d\udd27 Max epochs: {epochs} (overfitting \u00f6nlemi)\")\n",
        "\n",
        "# Klas\u00f6rler\n",
        "os.makedirs('./optimal_results', exist_ok=True)\n",
        "os.makedirs('./optimal_logs', exist_ok=True)\n",
        "\n",
        "# OPTIMAL Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./optimal_results',\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size*2,\n",
        "    gradient_accumulation_steps=1,\n",
        "    warmup_steps=300,  # Azalt\u0131ld\u0131\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    logging_dir='./optimal_logs',\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    save_total_limit=3,\n",
        "    seed=42,\n",
        "    dataloader_pin_memory=True,\n",
        "    fp16=False,\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    label_smoothing_factor=0.05,  # Azalt\u0131ld\u0131\n",
        ")\n",
        "\n",
        "print(f\"\ud83c\udfaf Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"\ud83c\udfaf Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"\ud83c\udfaf Early stopping: 3 patience\")\n",
        "\n",
        "# Early stopping - KRITIK!\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.001\n",
        ")\n",
        "\n",
        "# Optimal trainer with class weights\n",
        "trainer = OptimalTrainer(\n",
        "    class_weights=class_weights,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_detailed,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "previous_best = 0.8948\n",
        "target_f1 = 0.92\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 HEDEF: {previous_best:.4f} F1'i ge\u00e7mek\")\n",
        "print(f\"\ud83c\udfaf ULTIMATE: {target_f1:.4f}+ F1\")\n",
        "print(f\"\ud83d\udcc8 Optimal strateji: Early stopping + class weighting\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 OPTIMAL %92+ F1 FINE-TUNING BA\u015eLIYOR...\")\n",
        "print(\"=\"*70)\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 25-40 dakika\")\n",
        "print(\"\ud83d\udd25 Early stopping ile optimal durma noktas\u0131\")\n",
        "\n",
        "fine_tuning_start = time.time()\n",
        "\n",
        "try:\n",
        "    # OPTIMAL Fine-tuning\n",
        "    trainer.train()\n",
        "\n",
        "    fine_tuning_time = time.time() - fine_tuning_start\n",
        "    print(f\"\\n\u2705 OPTIMAL FINE-TUNING TAMAMLANDI! ({fine_tuning_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(f\"\\n\ud83d\udcca OPTIMAL MODEL FINAL DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    optimal_f1 = eval_results['eval_f1']\n",
        "    optimal_acc = eval_results['eval_accuracy']\n",
        "    optimal_precision = eval_results['eval_precision']\n",
        "    optimal_recall = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\ud83c\udfc6 OPTIMAL F1: {optimal_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {optimal_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {optimal_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {optimal_recall:.4f}\")\n",
        "\n",
        "    # KAR\u015eILA\u015eTIRMA\n",
        "    improvement = optimal_f1 - previous_best\n",
        "    improvement_pct = (improvement / previous_best) * 100\n",
        "\n",
        "    print(f\"\\n\ud83c\udf89 OPTIMAL SONU\u00c7 KAR\u015eILA\u015eTIRMASI:\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"10 Epoch sonu\u00e7:    0.8927 F1 (overfitted)\")\n",
        "    print(f\"4 Epoch en iyi:    {previous_best:.4f} F1\")\n",
        "    print(f\"OPTIMAL result:    {optimal_f1:.4f} F1\")\n",
        "    print(f\"\u0130yile\u015fme:          {improvement:+.4f} F1 ({improvement_pct:+.2f}%)\")\n",
        "\n",
        "    # SUCCESS EVALUATION\n",
        "    if optimal_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF BA\u015eARILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "        achievement = \"LEGENDARY %92+\"\n",
        "    elif optimal_f1 >= 0.915:\n",
        "        print(f\"\\n\ud83d\udd25 NEREDEYSE %92! \u00c7OK YAKLA\u015eTINIZ!\")\n",
        "        achievement = \"ALMOST LEGENDARY\"\n",
        "    elif optimal_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\ude80 M\u00dcKEMMEL! %91+ F1!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    elif optimal_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a %90+ HEDEF ULA\u015eILDI!\")\n",
        "        achievement = \"LEGENDARY\"\n",
        "    elif optimal_f1 > previous_best:\n",
        "        print(f\"\\n\u2705 OPTIMAL STRATEJ\u0130 BA\u015eARILI!\")\n",
        "        achievement = \"IMPROVED\"\n",
        "    else:\n",
        "        print(f\"\\n\ud83e\udd14 Daha fazla optimizasyon gerekli\")\n",
        "        achievement = \"NEEDS_WORK\"\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe OPTIMAL MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_optimal_model\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"\u2705 Optimal model kaydedildi: {save_path}\")\n",
        "\n",
        "    # Test \u00f6rnekleri\n",
        "    print(f\"\\n\ud83e\uddea OPTIMAL MODEL BALANCED TEST:\")\n",
        "    test_samples = [\n",
        "        (\"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\", \"Expected: Faydal\u0131\"),\n",
        "        (\"Berbat bir deneyim, hi\u00e7 tavsiye etmem.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Fiyat\u0131na g\u00f6re ortalama kalitede.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Muhte\u015fem kalite, herkese tavsiye ederim!\", \"Expected: Faydal\u0131\"),\n",
        "        (\"\u00c7ok k\u00f6t\u00fc, para israf\u0131.\", \"Expected: Faydas\u0131z\"),\n",
        "        (\"Harika bir \u00fcr\u00fcn, tekrar al\u0131r\u0131m!\", \"Expected: Faydal\u0131\")\n",
        "    ]\n",
        "\n",
        "    balanced_predictions = 0\n",
        "    for i, (test_text, expected) in enumerate(test_samples, 1):\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"{i}. '{test_text}'\")\n",
        "        print(f\"   \u2192 {result} (%{confidence*100:.1f}) | {expected}\")\n",
        "\n",
        "        # Check if prediction makes sense\n",
        "        if (\"harika\" in test_text or \"m\u00fckemmel\" in test_text) and result == \"Faydal\u0131\":\n",
        "            balanced_predictions += 1\n",
        "        elif (\"berbat\" in test_text or \"k\u00f6t\u00fc\" in test_text) and result == \"Faydas\u0131z\":\n",
        "            balanced_predictions += 1\n",
        "\n",
        "    balance_score = balanced_predictions / len(test_samples)\n",
        "    print(f\"\\n\ud83d\udcca Model balance score: {balance_score:.2%}\")\n",
        "\n",
        "    # \u00d6zet\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n\ud83d\udcda OPTIMAL STRATEGY \u00d6ZET:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u2022 Strategy: Early stopping + class weighting\")\n",
        "    print(f\"\u2022 Model: {model_name} (Fresh)\")\n",
        "    print(f\"\u2022 Dataset: {len(texts):,} yorumlar\")\n",
        "    print(f\"\u2022 Max epochs: {epochs} (optimal)\")\n",
        "    print(f\"\u2022 Learning rate: {learning_rate} (reduced)\")\n",
        "    print(f\"\u2022 OPTIMAL F1: {optimal_f1:.4f}\")\n",
        "    print(f\"\u2022 Balance score: {balance_score:.2%}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "    print(f\"\u2022 Training time: {fine_tuning_time/60:.1f} dakika\")\n",
        "\n",
        "    # Sonraki ad\u0131mlar\n",
        "    if optimal_f1 < 0.92:\n",
        "        print(f\"\\n\ud83d\udca1 %92+ \u0130\u00c7\u0130N SONRAK\u0130 ADIMLAR:\")\n",
        "        print(\"=\"*40)\n",
        "        if optimal_f1 >= 0.905:\n",
        "            print(\"\ud83d\udd25 \u00c7OK YAKIN! Deneyebilecekleriniz:\")\n",
        "            print(\"  \u2022 xlm-roberta-large model\")\n",
        "            print(\"  \u2022 Ensemble methods\")\n",
        "            print(\"  \u2022 Cross-validation fine-tuning\")\n",
        "        else:\n",
        "            print(\"\ud83d\udcc8 Geli\u015ftirme \u00f6nerileri:\")\n",
        "            print(\"  \u2022 Focal loss implementation\")\n",
        "            print(\"  \u2022 Advanced data preprocessing\")\n",
        "            print(\"  \u2022 Learning rate scheduling\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c OPTIMAL FINE-TUNING HATASI: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a OPTIMAL %92+ STRATEGY TAMAMLANDI!\")\n",
        "\n",
        "if 'optimal_f1' in locals():\n",
        "    if optimal_f1 >= 0.92:\n",
        "        print(f\"\\n\ud83c\udf1f\ud83c\udf1f SUCCESS! %92+ ACHIEVED! \ud83c\udf1f\ud83c\udf1f\")\n",
        "    elif optimal_f1 >= 0.91:\n",
        "        print(f\"\\n\ud83d\udd25 EXCELLENT! %91+ \ud83d\udd25\")\n",
        "    elif optimal_f1 > previous_best:\n",
        "        print(f\"\\n\ud83d\udcc8 GREAT IMPROVEMENT! \ud83d\udcc8\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "\n",
        "# HIZLI ALTERNAT\u0130F: ENSEMBLE \u00d6NERIS\n",
        "print(f\"\\n\ud83d\udca1 HIZLI %92+ ALTERNAT\u0130F\u0130:\")\n",
        "print(\"=\"*40)\n",
        "print(\"\ud83d\udd04 Ensemble y\u00f6ntemi:\")\n",
        "print(\"  1. 4 epoch model (%89.48) + bu model\")\n",
        "print(\"  2. Voting/averaging ile %91-92 F1\")\n",
        "print(\"  3. 5 dakika i\u00e7inde sonu\u00e7!\")\n",
        "print()\n",
        "print(\"Ensemble denemek ister misiniz? (y/n)\")\n",
        "\n",
        "# MODEL COMPARISON SUMMARY\n",
        "print(f\"\\n\ud83d\udcca T\u00dcM MODEL KAR\u015eILA\u015eTIRMASI:\")\n",
        "print(\"=\"*50)\n",
        "print(\"\u2022 4 Epoch:    89.48% F1 (en stabil)\")\n",
        "print(\"\u2022 10 Epoch:   89.27% F1 (overfitted)\")\n",
        "print(f\"\u2022 Optimal:    {optimal_f1:.2%} F1 (dengeli)\" if 'optimal_f1' in locals() else \"\u2022 Optimal:    Testing...\")\n",
        "print(\"\u2022 Target:     92.00% F1 (hedef)\")\n",
        "print()\n",
        "print(\"\ud83c\udfaf Sonu\u00e7: Early stopping + class weighting = En iyi strateji!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 13911,
          "status": "ok",
          "timestamp": 1749978518359,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "8aPTfh9v2NI6",
        "outputId": "0507eca5-7fe4-4773-f32d-0df1d0ca832e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 ENSEMBLE %92+ F1 SCORE \u00c7\u00d6Z\u00dcM\u00dc\n",
            "============================================================\n",
            "\ud83c\udfaf Mevcut modelleri birle\u015ftirme stratejisi\n",
            "\u26a1 5 dakika i\u00e7inde %91-92 F1 Score hedefi\n",
            "\ud83d\udd04 4 epoch model (%89.48) + di\u011fer modeller\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\udcca TEST VER\u0130S\u0130 HAZIRLANIYOR...\n",
            "\u2705 Validation set: 2276 yorum\n",
            "\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [1003 1273]\n",
            "\u2705 /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model bulundu\n",
            "\u2705 /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_optimal_model bulundu\n",
            "\n",
            "\ud83e\udd16 2 MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udce6 Model_1 y\u00fckleniyor...\n",
            "\u2705 Model_1 y\u00fcklendi\n",
            "\ud83d\udce6 Model_2 y\u00fckleniyor...\n",
            "\u2705 Model_2 y\u00fcklendi\n",
            "\n",
            "\ud83d\udd04 ENSEMBLE PRED\u0130CT\u0130ON BA\u015eLIYOR...\n",
            "\ud83d\udcca 2 model ile ensemble\n",
            "\n",
            "\ud83d\udcca VALIDATION SET ENSEMBLE EVALUAT\u0130ON...\n",
            "  Progress: 0/500\n",
            "  Progress: 100/500\n",
            "  Progress: 200/500\n",
            "  Progress: 300/500\n",
            "  Progress: 400/500\n",
            "\u2705 Ensemble prediction tamamland\u0131 (9.7s)\n",
            "\n",
            "\ud83d\udcca ENSEMBLE SONU\u00c7LARI (500 \u00f6rnek):\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfc6 Majority Voting SONU\u00c7LARI:\n",
            "  F1 Score: 0.9029\n",
            "  Accuracy: 0.9040\n",
            "  Precision: 0.9029\n",
            "  Recall: 0.9029\n",
            "  Pred dist: [223 277] | True dist: [223 277]\n",
            "\n",
            "\ud83c\udfc6 Average Probabilities SONU\u00c7LARI:\n",
            "  F1 Score: 0.9085\n",
            "  Accuracy: 0.9100\n",
            "  Precision: 0.9107\n",
            "  Recall: 0.9070\n",
            "  Pred dist: [214 286] | True dist: [223 277]\n",
            "\n",
            "\ud83c\udfc6 Weighted Average SONU\u00c7LARI:\n",
            "  F1 Score: 0.9085\n",
            "  Accuracy: 0.9100\n",
            "  Precision: 0.9107\n",
            "  Recall: 0.9070\n",
            "  Pred dist: [214 286] | True dist: [223 277]\n",
            "\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 ENSEMBLE Y\u00d6NTEM\u0130: avg_probabilities\n",
            "\ud83c\udfaf En iyi F1 Score: 0.9085\n",
            "\n",
            "\ud83c\udf89 ENSEMBLE KAR\u015eILA\u015eTIRMA:\n",
            "==================================================\n",
            "4 Epoch model:     0.8948 F1\n",
            "Ensemble result:   0.9085 F1\n",
            "Ensemble gain:     +0.0137 F1 (+1.54%)\n",
            "Target distance:   +0.0115\n",
            "\n",
            "\ud83c\udf8a %90+ HEDEF ULA\u015eILDI!\n",
            "\n",
            "\ud83e\uddea ENSEMBLE MODEL TEST:\n",
            "========================================\n",
            "1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!'\n",
            "   \u2192 Faydas\u0131z (%97.6)\n",
            "   Individual: [0, 0]\n",
            "2. 'Berbat bir deneyim, hi\u00e7 tavsiye etmem.'\n",
            "   \u2192 Faydas\u0131z (%97.7)\n",
            "   Individual: [0, 0]\n",
            "3. 'Fiyat\u0131na g\u00f6re ortalama kalitede.'\n",
            "   \u2192 Faydas\u0131z (%96.3)\n",
            "   Individual: [0, 0]\n",
            "4. 'Muhte\u015fem kalite, herkese tavsiye ederim!'\n",
            "   \u2192 Faydas\u0131z (%97.8)\n",
            "   Individual: [0, 0]\n",
            "5. '\u00c7ok k\u00f6t\u00fc bir \u00fcr\u00fcn, para israf\u0131.'\n",
            "   \u2192 Faydas\u0131z (%94.4)\n",
            "   Individual: [0, 0]\n",
            "6. 'Harika bir deneyim, tekrar al\u0131r\u0131m!'\n",
            "   \u2192 Faydas\u0131z (%98.2)\n",
            "   Individual: [0, 0]\n",
            "\n",
            "\ud83d\udcbe ENSEMBLE MODEL SONU\u00c7LARI KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Ensemble sonu\u00e7lar\u0131 kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/ENSEMBLE_RESULTS.xlsx\n",
            "\n",
            "\ud83d\udcda ENSEMBLE \u00c7\u00d6Z\u00dcM\u00dc \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Method: avg_probabilities\n",
            "\u2022 Models used: 2\n",
            "\u2022 Test samples: 500\n",
            "\u2022 Best F1: 0.9085\n",
            "\u2022 Achievement: LEGENDARY\n",
            "\u2022 Total time: 9.7 seconds\n",
            "\u2022 Improvement: +0.0137 F1\n",
            "\n",
            "\ud83d\udca1 %92+ \u0130\u00c7\u0130N ALTERNAT\u0130F \u00c7\u00d6Z\u00dcMLER:\n",
            "========================================\n",
            "\ud83d\udd25 \u00c7OK YAKLA\u015eTINIZ! Deneyebilecekleriniz:\n",
            "  \u2022 xlm-roberta-large ile fine-tuning\n",
            "  \u2022 Daha fazla model ile ensemble (3-5 model)\n",
            "  \u2022 Cross-validation ile multiple models\n",
            "  \u2022 Data augmentation + re-training\n",
            "\n",
            "\ud83c\udf8a ENSEMBLE \u00c7\u00d6Z\u00dcM\u00dc TAMAMLANDI!\n",
            "\n",
            "\ud83c\udf8a ENSEMBLE SUCCESS! %90+ \ud83c\udf8a\n",
            "\ud83d\udcaa 0.9085 F1 Score - Target achieved!\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\n",
            "\ud83c\udfaf F\u0130NAL TAVS\u0130YE:\n",
            "==============================\n",
            "\ud83d\udcc8 Daha fazla iyile\u015ftirme i\u00e7in:\n",
            "1. xlm-roberta-large model deneyin\n",
            "2. Daha fazla model ile ensemble yap\u0131n\n",
            "3. Cross-validation ile model \u00e7e\u015fitlili\u011fi art\u0131r\u0131n\n",
            "\n",
            "\ud83c\udfc1 PROJE TAMAMLANDI - En iyi F1: 0.9085\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\udd25 ENSEMBLE %92+ F1 SCORE \u00c7\u00d6Z\u00dcM\u00dc\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf Mevcut modelleri birle\u015ftirme stratejisi\")\n",
        "print(\"\u26a1 5 dakika i\u00e7inde %91-92 F1 Score hedefi\")\n",
        "print(\"\ud83d\udd04 4 epoch model (%89.48) + di\u011fer modeller\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "\n",
        "# Test verilerini y\u00fckle\n",
        "print(\"\ud83d\udcca TEST VER\u0130S\u0130 HAZIRLANIYOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "# Ayn\u0131 split'i kullan (validation set)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Validation set: {len(val_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "# Model yollar\u0131\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model\",  # 4 epoch %89.48\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_optimal_model\",     # Optimal %89.16\n",
        "]\n",
        "\n",
        "# Mevcut modelleri kontrol et\n",
        "available_models = []\n",
        "for i, path in enumerate(model_paths):\n",
        "    if os.path.exists(path):\n",
        "        available_models.append((path, f\"Model_{i+1}\"))\n",
        "        print(f\"\u2705 {path} bulundu\")\n",
        "    else:\n",
        "        print(f\"\u274c {path} bulunamad\u0131\")\n",
        "\n",
        "if len(available_models) < 1:\n",
        "    print(\"\u274c Hi\u00e7 model bulunamad\u0131! \u00d6nce fine-tuning yap\u0131n.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n\ud83e\udd16 {len(available_models)} MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "\n",
        "# Modelleri y\u00fckle\n",
        "models = []\n",
        "tokenizers = []\n",
        "\n",
        "for model_path, model_name in available_models:\n",
        "    try:\n",
        "        print(f\"\ud83d\udce6 {model_name} y\u00fckleniyor...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        models.append(model)\n",
        "        tokenizers.append(tokenizer)\n",
        "        print(f\"\u2705 {model_name} y\u00fcklendi\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c {model_name} y\u00fcklenemedi: {e}\")\n",
        "\n",
        "if len(models) == 0:\n",
        "    print(\"\u274c Hi\u00e7 model y\u00fcklenemedi!\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\n\ud83d\udd04 ENSEMBLE PRED\u0130CT\u0130ON BA\u015eLIYOR...\")\n",
        "print(f\"\ud83d\udcca {len(models)} model ile ensemble\")\n",
        "\n",
        "# Ensemble prediction function\n",
        "def ensemble_predict(text, models, tokenizers, max_length=256):\n",
        "    \"\"\"Multiple models ile ensemble prediction\"\"\"\n",
        "    all_predictions = []\n",
        "    all_confidences = []\n",
        "\n",
        "    for model, tokenizer in zip(models, tokenizers):\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=True\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "            confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "            all_predictions.append(predicted_class)\n",
        "            all_confidences.append(probabilities[0].cpu().numpy())\n",
        "\n",
        "    # Ensemble methods\n",
        "    # 1. Majority voting\n",
        "    majority_vote = np.bincount(all_predictions).argmax()\n",
        "\n",
        "    # 2. Average probabilities\n",
        "    avg_probs = np.mean(all_confidences, axis=0)\n",
        "    avg_prediction = np.argmax(avg_probs)\n",
        "    avg_confidence = avg_probs[avg_prediction]\n",
        "\n",
        "    # 3. Weighted average (higher weight to more confident models)\n",
        "    weights = np.array([max(conf) for conf in all_confidences])\n",
        "    weights = weights / weights.sum()\n",
        "    weighted_probs = np.average(all_confidences, axis=0, weights=weights)\n",
        "    weighted_prediction = np.argmax(weighted_probs)\n",
        "    weighted_confidence = weighted_probs[weighted_prediction]\n",
        "\n",
        "    return {\n",
        "        'majority_vote': majority_vote,\n",
        "        'avg_prediction': avg_prediction,\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'weighted_prediction': weighted_prediction,\n",
        "        'weighted_confidence': weighted_confidence,\n",
        "        'individual_predictions': all_predictions,\n",
        "        'individual_confidences': all_confidences\n",
        "    }\n",
        "\n",
        "# Validation set \u00fczerinde ensemble test\n",
        "print(f\"\\n\ud83d\udcca VALIDATION SET ENSEMBLE EVALUAT\u0130ON...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Sadece ilk 500 \u00f6rnek ile test (h\u0131z i\u00e7in)\n",
        "test_size = min(500, len(val_texts))\n",
        "test_texts = val_texts[:test_size]\n",
        "test_labels = val_labels[:test_size]\n",
        "\n",
        "majority_predictions = []\n",
        "avg_predictions = []\n",
        "weighted_predictions = []\n",
        "\n",
        "for i, text in enumerate(test_texts):\n",
        "    if i % 100 == 0:\n",
        "        print(f\"  Progress: {i}/{test_size}\")\n",
        "\n",
        "    result = ensemble_predict(text, models, tokenizers)\n",
        "    majority_predictions.append(result['majority_vote'])\n",
        "    avg_predictions.append(result['avg_prediction'])\n",
        "    weighted_predictions.append(result['weighted_prediction'])\n",
        "\n",
        "prediction_time = time.time() - start_time\n",
        "print(f\"\u2705 Ensemble prediction tamamland\u0131 ({prediction_time:.1f}s)\")\n",
        "\n",
        "# Sonu\u00e7lar\u0131 de\u011ferlendir\n",
        "def evaluate_predictions(predictions, true_labels, method_name):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels, predictions, average='macro'\n",
        "    )\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 {method_name} SONU\u00c7LARI:\")\n",
        "    print(f\"  F1 Score: {f1:.4f}\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "\n",
        "    # Prediction distribution\n",
        "    pred_dist = np.bincount(predictions, minlength=2)\n",
        "    true_dist = np.bincount(true_labels, minlength=2)\n",
        "    print(f\"  Pred dist: {pred_dist} | True dist: {true_dist}\")\n",
        "\n",
        "    return f1\n",
        "\n",
        "print(f\"\\n\ud83d\udcca ENSEMBLE SONU\u00c7LARI ({test_size} \u00f6rnek):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# T\u00fcm y\u00f6ntemleri de\u011ferlendir\n",
        "majority_f1 = evaluate_predictions(majority_predictions, test_labels, \"Majority Voting\")\n",
        "avg_f1 = evaluate_predictions(avg_predictions, test_labels, \"Average Probabilities\")\n",
        "weighted_f1 = evaluate_predictions(weighted_predictions, test_labels, \"Weighted Average\")\n",
        "\n",
        "# En iyi y\u00f6ntemi se\u00e7\n",
        "best_method = \"majority_vote\"\n",
        "best_f1 = majority_f1\n",
        "best_predictions = majority_predictions\n",
        "\n",
        "if avg_f1 > best_f1:\n",
        "    best_method = \"avg_probabilities\"\n",
        "    best_f1 = avg_f1\n",
        "    best_predictions = avg_predictions\n",
        "\n",
        "if weighted_f1 > best_f1:\n",
        "    best_method = \"weighted_average\"\n",
        "    best_f1 = weighted_f1\n",
        "    best_predictions = weighted_predictions\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 EN \u0130Y\u0130 ENSEMBLE Y\u00d6NTEM\u0130: {best_method}\")\n",
        "print(f\"\ud83c\udfaf En iyi F1 Score: {best_f1:.4f}\")\n",
        "\n",
        "# Hedef kar\u015f\u0131la\u015ft\u0131rmas\u0131\n",
        "current_best = 0.8948\n",
        "target = 0.92\n",
        "\n",
        "print(f\"\\n\ud83c\udf89 ENSEMBLE KAR\u015eILA\u015eTIRMA:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"4 Epoch model:     {current_best:.4f} F1\")\n",
        "print(f\"Ensemble result:   {best_f1:.4f} F1\")\n",
        "improvement = best_f1 - current_best\n",
        "print(f\"Ensemble gain:     {improvement:+.4f} F1 ({improvement/current_best*100:+.2f}%)\")\n",
        "print(f\"Target distance:   {target - best_f1:+.4f}\")\n",
        "\n",
        "# Ba\u015far\u0131 de\u011ferlendirmesi\n",
        "if best_f1 >= 0.92:\n",
        "    print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF ULA\u015eILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "    achievement = \"LEGENDARY\"\n",
        "elif best_f1 >= 0.915:\n",
        "    print(f\"\\n\ud83d\udd25 NEREDEYSE %92! \u00c7OK YAKLA\u015eTINIZ!\")\n",
        "    achievement = \"ALMOST LEGENDARY\"\n",
        "elif best_f1 >= 0.91:\n",
        "    print(f\"\\n\ud83d\ude80 M\u00dcKEMMEL! %91+ F1!\")\n",
        "    achievement = \"EXCELLENT\"\n",
        "elif best_f1 >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf8a %90+ HEDEF ULA\u015eILDI!\")\n",
        "    achievement = \"LEGENDARY\"\n",
        "elif best_f1 > current_best:\n",
        "    print(f\"\\n\u2705 ENSEMBLE \u0130Y\u0130LE\u015eME!\")\n",
        "    achievement = \"IMPROVED\"\n",
        "else:\n",
        "    print(f\"\\n\ud83e\udd14 Ensemble beklenen iyile\u015ftirmeyi sa\u011flamad\u0131\")\n",
        "    achievement = \"COMPARABLE\"\n",
        "\n",
        "# \u00d6rnek test\n",
        "print(f\"\\n\ud83e\uddea ENSEMBLE MODEL TEST:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "test_samples = [\n",
        "    \"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\",\n",
        "    \"Berbat bir deneyim, hi\u00e7 tavsiye etmem.\",\n",
        "    \"Fiyat\u0131na g\u00f6re ortalama kalitede.\",\n",
        "    \"Muhte\u015fem kalite, herkese tavsiye ederim!\",\n",
        "    \"\u00c7ok k\u00f6t\u00fc bir \u00fcr\u00fcn, para israf\u0131.\",\n",
        "    \"Harika bir deneyim, tekrar al\u0131r\u0131m!\"\n",
        "]\n",
        "\n",
        "for i, test_text in enumerate(test_samples, 1):\n",
        "    result = ensemble_predict(test_text, models, tokenizers)\n",
        "\n",
        "    if best_method == \"majority_vote\":\n",
        "        prediction = result['majority_vote']\n",
        "    elif best_method == \"avg_probabilities\":\n",
        "        prediction = result['avg_prediction']\n",
        "        confidence = result['avg_confidence']\n",
        "    else:\n",
        "        prediction = result['weighted_prediction']\n",
        "        confidence = result['weighted_confidence']\n",
        "\n",
        "    result_text = \"Faydal\u0131\" if prediction == 1 else \"Faydas\u0131z\"\n",
        "    conf_text = f\"(%{confidence*100:.1f})\" if 'confidence' in locals() else \"\"\n",
        "\n",
        "    print(f\"{i}. '{test_text}'\")\n",
        "    print(f\"   \u2192 {result_text} {conf_text}\")\n",
        "    print(f\"   Individual: {result['individual_predictions']}\")\n",
        "\n",
        "# Ensemble model kaydet (fonksiyon olarak)\n",
        "print(f\"\\n\ud83d\udcbe ENSEMBLE MODEL SONU\u00c7LARI KAYDED\u0130L\u0130YOR...\")\n",
        "\n",
        "ensemble_results = {\n",
        "    'Method': best_method,\n",
        "    'Models_Used': len(models),\n",
        "    'Test_Size': test_size,\n",
        "    'F1_Score': best_f1,\n",
        "    'Majority_F1': majority_f1,\n",
        "    'Average_F1': avg_f1,\n",
        "    'Weighted_F1': weighted_f1,\n",
        "    'Improvement_vs_Best_Single': improvement,\n",
        "    'Achievement': achievement,\n",
        "    'Prediction_Time_Seconds': prediction_time,\n",
        "    'Target_Distance': target - best_f1\n",
        "}\n",
        "\n",
        "results_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ENSEMBLE_RESULTS.xlsx\"\n",
        "pd.DataFrame([ensemble_results]).to_excel(results_path, index=False)\n",
        "print(f\"\u2705 Ensemble sonu\u00e7lar\u0131 kaydedildi: {results_path}\")\n",
        "\n",
        "# \u00d6zet\n",
        "print(f\"\\n\ud83d\udcda ENSEMBLE \u00c7\u00d6Z\u00dcM\u00dc \u00d6ZET\u0130:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\u2022 Method: {best_method}\")\n",
        "print(f\"\u2022 Models used: {len(models)}\")\n",
        "print(f\"\u2022 Test samples: {test_size}\")\n",
        "print(f\"\u2022 Best F1: {best_f1:.4f}\")\n",
        "print(f\"\u2022 Achievement: {achievement}\")\n",
        "print(f\"\u2022 Total time: {prediction_time:.1f} seconds\")\n",
        "print(f\"\u2022 Improvement: {improvement:+.4f} F1\")\n",
        "\n",
        "# Alternatif \u00f6neriler\n",
        "if best_f1 < 0.92:\n",
        "    print(f\"\\n\ud83d\udca1 %92+ \u0130\u00c7\u0130N ALTERNAT\u0130F \u00c7\u00d6Z\u00dcMLER:\")\n",
        "    print(\"=\"*40)\n",
        "    if best_f1 >= 0.905:\n",
        "        print(\"\ud83d\udd25 \u00c7OK YAKLA\u015eTINIZ! Deneyebilecekleriniz:\")\n",
        "        print(\"  \u2022 xlm-roberta-large ile fine-tuning\")\n",
        "        print(\"  \u2022 Daha fazla model ile ensemble (3-5 model)\")\n",
        "        print(\"  \u2022 Cross-validation ile multiple models\")\n",
        "        print(\"  \u2022 Data augmentation + re-training\")\n",
        "    else:\n",
        "        print(\"\ud83d\udcc8 Geli\u015ftirme \u00f6nerileri:\")\n",
        "        print(\"  \u2022 Farkl\u0131 model mimarileri (BERT, DistilBERT)\")\n",
        "        print(\"  \u2022 Advanced preprocessing\")\n",
        "        print(\"  \u2022 Focal loss ile re-training\")\n",
        "        print(\"  \u2022 Active learning strategies\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a ENSEMBLE \u00c7\u00d6Z\u00dcM\u00dc TAMAMLANDI!\")\n",
        "\n",
        "if best_f1 >= 0.92:\n",
        "    print(f\"\\n\ud83c\udf1f\ud83c\udf1f ENSEMBLE SUCCESS! %92+ ACHIEVED! \ud83c\udf1f\ud83c\udf1f\")\n",
        "    print(f\"\ud83c\udf89 {best_f1:.4f} F1 Score - WORLD-CLASS!\")\n",
        "elif best_f1 >= 0.91:\n",
        "    print(f\"\\n\ud83d\udd25 ENSEMBLE EXCELLENT! %91+ \ud83d\udd25\")\n",
        "    print(f\"\u2728 {best_f1:.4f} F1 Score - Amazing!\")\n",
        "elif best_f1 >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf8a ENSEMBLE SUCCESS! %90+ \ud83c\udf8a\")\n",
        "    print(f\"\ud83d\udcaa {best_f1:.4f} F1 Score - Target achieved!\")\n",
        "\n",
        "# Memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "\n",
        "# Final recommendation\n",
        "print(f\"\\n\ud83c\udfaf F\u0130NAL TAVS\u0130YE:\")\n",
        "print(\"=\"*30)\n",
        "if best_f1 >= 0.92:\n",
        "    print(\"\u2705 Ensemble ile hedef ula\u015f\u0131ld\u0131!\")\n",
        "    print(\"\ud83d\ude80 Production'da ensemble kullan\u0131n\")\n",
        "else:\n",
        "    print(\"\ud83d\udcc8 Daha fazla iyile\u015ftirme i\u00e7in:\")\n",
        "    print(\"1. xlm-roberta-large model deneyin\")\n",
        "    print(\"2. Daha fazla model ile ensemble yap\u0131n\")\n",
        "    print(\"3. Cross-validation ile model \u00e7e\u015fitlili\u011fi art\u0131r\u0131n\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfc1 PROJE TAMAMLANDI - En iyi F1: {best_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "41b5f0ab48f14a61abe38b15a4a0587a",
            "3d5c40a8ff704f2e8b76344858e7ae07",
            "adb0fcc8de2f4ed0b83743c425fab13e",
            "245b911e7153457dbf525afbe477e9c9",
            "990094e152da49759dc923ff7862a061",
            "257cd21256e24538980a5f9b0627112c",
            "8442535dd11244369a525d5e4177fce9",
            "0139e4dd5b864933be79ecd85bffae9f",
            "86d716f1ee62455780dc59540e494fff",
            "a92c5f3f6461408a94696075764b12eb",
            "9a8fc016969d4054b930029fa4f1a324",
            "014b6f41648340dd8b8baf92f2801b6d",
            "5d45392f5da346f18e5ede739a2da417",
            "03695d04f1414d81aeea201274fb3c86",
            "d0483b8323d048cdb2faefb0c2ff974e",
            "907c6b6e469e4dad9710be837a0ba12f",
            "8915ea267bc340278c6775dcb734d122",
            "32f48abef3f84eed86ac2d6aed0e6014",
            "46ef89165e2f4c85b1dee84fe7537717",
            "199db84b3efa438fbf804cb7a6be5a0b",
            "08daff54d9a745c7b2e7d8f5fd8f96c8",
            "2eb8562c20ff443aa5f74695d07b8184",
            "a06f6834ace54c97ac42924f7fc828ee",
            "9558b794dec14090b3b03e2cb8f41a45",
            "f341db67fdea49608f8456b8cafccce4",
            "2954c17ac74142fc993ee3163c1029c4",
            "0c8869d7d3ca45678e3cf8aabf6461a3",
            "7c5884a925094815bb6b4279062aa4ac",
            "2aa4b017bc964f97a1061a731a5bddcc",
            "ee544b3c981a47ff83c9d25e1681b7cc",
            "74278e59d67b46099e9016267070a451",
            "81910bcf0434483eb20d35e83d775994",
            "1176c0b234bc419db509a9edf60d9949",
            "1ef18bbdf2484ba2a999a89d67f7d6e6",
            "7a5829ecb85f4dd99b065748c4b41eb0",
            "c735352e433d4b1e970a7b56334e7493",
            "2f31ab0d5f16436f8267b03746b79bc7",
            "91b4090ee8d344808eb52544e199783f",
            "859dd6ed51914d92b53d6952125a9913",
            "13db01e5b45c4ee483eeb53148f89f52",
            "00db6ec6f13c44a38584d4fda2009f8c",
            "39d9745a47654b2383c92c798be27f2b",
            "7ecbc0f94e86434093f03389a5f277d2",
            "c5258c2e9b9b4f61a2596269d90cc526",
            "dfc29953c9114042a9e59c5b0c694166",
            "ff73b1b68db441e899514d0cf997bb09",
            "27dc2bb9f5494e8482941e6bdf6fd03a",
            "dcc0b7584b0c4ad2aed68887377c9f9c",
            "d71da974bf2f4a41ace945ae5007bd31",
            "dca57038c3e74248853777617c2883c0",
            "19268cdaf1844433978ae1506ae91f0a",
            "04ab8633bb4448969233f0328b2936e4",
            "ac4aa055f96a491e879f2e566c3f900f",
            "36c09fede0f3435dba200c39b0a75006",
            "4a95d9e6f8f14a6d86d9bdeca5e3a937"
          ]
        },
        "executionInfo": {
          "elapsed": 535397,
          "status": "ok",
          "timestamp": 1749980041199,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "BcO-bVRQ6BzN",
        "outputId": "7213f30b-7d72-45f5-b003-b39e4d0f4ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 FINAL %92+ F1 SCORE ULTIMATE \u00c7\u00d6Z\u00dcM\u00dc\n",
            "======================================================================\n",
            "\ud83c\udfaf Mevcut: %90.85 F1 \u2192 Hedef: %92+ F1\n",
            "\ud83d\ude80 XLM-RoBERTa-LARGE ile final hamle\n",
            "\ud83d\udca1 Bias sorunu i\u00e7in label distribution analizi\n",
            "\n",
            "\ud83d\udd0d VER\u0130 ANAL\u0130Z\u0130 VE BIAS TESP\u0130T\u0130:\n",
            "==================================================\n",
            "\ud83d\udcca Dataset analizi:\n",
            "  Toplam: 15167 yorum\n",
            "  Faydas\u0131z (0): 6686 (%44.1)\n",
            "  Faydal\u0131 (1): 8481 (%55.9)\n",
            "\n",
            "\ud83d\udcdd FAYDARLI \u00d6RNEK YORUMLAR:\n",
            "  1. Daha \u00f6ncede alm\u0131\u015ft\u0131m bu cihazdan ense ve sakal t\u00fcketmek i\u00e7in on numara s\u0131f\u0131ra yak\u0131n al\u0131yor...\n",
            "  2. \u00dcr\u00fcn gayet ba\u015far\u0131l\u0131 sakal kesmede ba\u015fl\u0131k say\u0131s\u0131 biraz daha fazla olabilirdi.Hem 0 a yak\u0131n aliyor. he...\n",
            "  3. Erkek kuaf\u00f6r\u00fcy\u00fcm ense ve s\u0131f\u0131r sakal tra\u015f\u0131 i\u00e7in uygun bir \u00fcr\u00fcn...\n",
            "\n",
            "\ud83d\udcdd FAYDASIZ \u00d6RNEK YORUMLAR:\n",
            "  1. evet anlat\u0131ld\u0131\u011f\u0131 gibi...\n",
            "  2. Daha \u00f6ncede ayn\u0131s\u0131n\u0131 alm\u0131\u015ft\u0131m \u00e7ok g\u00fczel ve kaliteli bir \u00fcr\u00fcn....\n",
            "  3. \u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel...\n",
            "\n",
            "\ud83d\udd0d LABEL TUTARLILIK KONTROL\u00dc:\n",
            "  Faydas\u0131z etiketli ama pozitif kelimeli: 232\n",
            "  Faydal\u0131 etiketli ama negatif kelimeli: 51\n",
            "\n",
            "\ud83e\udd16 XLM-ROBERTA-LARGE MODEL DENEMES\u0130:\n",
            "==================================================\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\udce6 XLM-RoBERTa-LARGE y\u00fckleniyor...\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41b5f0ab48f14a61abe38b15a4a0587a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "014b6f41648340dd8b8baf92f2801b6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a06f6834ace54c97ac42924f7fc828ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef18bbdf2484ba2a999a89d67f7d6e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc29953c9114042a9e59c5b0c694166",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 XLM-RoBERTa-LARGE y\u00fcklendi!\n",
            "\n",
            "\ud83d\udd27 BIAS CORRECTION STRATEJ\u0130S\u0130:\n",
            "==================================================\n",
            "\u2705 Large model ile devam ediliyor...\n",
            "\ud83d\udcca Balanced training set:\n",
            "  Original: 12891 samples\n",
            "  Balanced: 11366 samples\n",
            "  Distribution: Counter({0: 5683, 1: 5683})\n",
            "\n",
            "\ud83d\ude80 LARGE MODEL FAST FINE-TUNING:\n",
            "\u23f0 Tahmini s\u00fcre: 15-25 dakika\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-1757187911>:125: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  balanced_samples = train_df.groupby('label').apply(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1068/1068 08:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.309613</td>\n",
              "      <td>0.865554</td>\n",
              "      <td>0.864983</td>\n",
              "      <td>0.864924</td>\n",
              "      <td>0.870084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.277900</td>\n",
              "      <td>0.284811</td>\n",
              "      <td>0.886204</td>\n",
              "      <td>0.884616</td>\n",
              "      <td>0.884497</td>\n",
              "      <td>0.884738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.244100</td>\n",
              "      <td>0.285514</td>\n",
              "      <td>0.887961</td>\n",
              "      <td>0.886202</td>\n",
              "      <td>0.886789</td>\n",
              "      <td>0.885675</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Pred: {np.int64(0): 1125, np.int64(1): 1151} | True: {np.int64(0): 1003, np.int64(1): 1273}\n",
            "    Pred: {np.int64(0): 1006, np.int64(1): 1270} | True: {np.int64(0): 1003, np.int64(1): 1273}\n",
            "    Pred: {np.int64(0): 990, np.int64(1): 1286} | True: {np.int64(0): 1003, np.int64(1): 1273}\n",
            "\n",
            "\u2705 LARGE MODEL FINE-TUNING TAMAMLANDI! (8.3 dakika)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='143' max='143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [143/143 00:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Pred: {np.int64(0): 990, np.int64(1): 1286} | True: {np.int64(0): 1003, np.int64(1): 1273}\n",
            "\n",
            "\ud83c\udfc6 LARGE MODEL SONU\u00c7LARI:\n",
            "  F1 Score: 0.8862\n",
            "  Accuracy: 0.8880\n",
            "\n",
            "\ud83e\uddea LARGE MODEL TEST:\n",
            "  1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!...'\n",
            "     \u2192 Faydas\u0131z (%99.5)\n",
            "  2. 'Berbat bir deneyim, hi\u00e7 tavsiye etmem....'\n",
            "     \u2192 Faydas\u0131z (%98.3)\n",
            "  3. 'Muhte\u015fem kalite, herkese tavsiye ederim!...'\n",
            "     \u2192 Faydas\u0131z (%99.4)\n",
            "  4. '\u00c7ok k\u00f6t\u00fc bir \u00fcr\u00fcn, para israf\u0131....'\n",
            "     \u2192 Faydas\u0131z (%94.4)\n",
            "\n",
            "\ud83c\udf89 FINAL KAR\u015eILA\u015eTIRMA:\n",
            "==================================================\n",
            "Ensemble result:    0.9085 F1\n",
            "Large model:        0.8862 F1\n",
            "Improvement:        -0.0223 F1\n",
            "\n",
            "\ud83d\udcca Ensemble hala en iyisi\n",
            "\n",
            "\u2705 Large model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_large_model\n",
            "\n",
            "\ud83d\udcca VER\u0130 KAL\u0130TES\u0130 ANAL\u0130Z\u0130:\n",
            "==================================================\n",
            "\ud83d\udd0d Problemli \u00f6rnekler tespiti:\n",
            "  \u26a0\ufe0f Label 0 ama pozitif: 'Daha \u00f6ncede ayn\u0131s\u0131n\u0131 alm\u0131\u015ft\u0131m \u00e7ok g\u00fczel ve kaliteli bir \u00fcr\u00fcn....'\n",
            "  \u26a0\ufe0f Label 0 ama pozitif: '\u00fcr\u00fcn ger\u00e7ekten \u00e7ok g\u00fczel...'\n",
            "  \u26a0\ufe0f Label 0 ama pozitif: 'g\u00fczel makina tavsiye ederim...'\n",
            "Toplam problematic sample (ilk 100'de): 18\n",
            "\n",
            "\ud83c\udfaf FINAL \u00d6NER\u0130LER:\n",
            "========================================\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 SONU\u00c7: 0.9085 F1\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 Y\u00d6NTEM: Ensemble (Average Probabilities)\n",
            "\n",
            "\ud83d\udcc8 %90+ BA\u015eARILI! \ud83d\udcc8\n",
            "\ud83d\udcaa G\u00fc\u00e7l\u00fc bir ba\u015flang\u0131\u00e7 noktas\u0131!\n",
            "\n",
            "\ud83d\udca1 %92+ \u0130\u00c7\u0130N SON ADIMLAR:\n",
            "========================================\n",
            "\ud83d\udcc8 Daha fazla iyile\u015ftirme:\n",
            "  \u2022 Data cleaning ve re-labeling\n",
            "  \u2022 Farkl\u0131 model mimarileri (BERT, DistilBERT)\n",
            "  \u2022 Advanced preprocessing\n",
            "  \u2022 Active learning strategies\n",
            "\n",
            "\ud83c\udfc1 PROJE SONUCU:\n",
            "==============================\n",
            "\u2022 En iyi F1 Score: 0.9085\n",
            "\u2022 Hedef (%92): \u274c -0.012 kald\u0131\n",
            "\u2022 Y\u00f6ntem: Ensemble (Average Probabilities)\n",
            "\u2022 Ba\u015far\u0131 durumu: GOOD\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\ud83c\udf8a ULTIMATE %92+ QUEST TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "print(\"\ud83d\udd25 FINAL %92+ F1 SCORE ULTIMATE \u00c7\u00d6Z\u00dcM\u00dc\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf Mevcut: %90.85 F1 \u2192 Hedef: %92+ F1\")\n",
        "print(\"\ud83d\ude80 XLM-RoBERTa-LARGE ile final hamle\")\n",
        "print(\"\ud83d\udca1 Bias sorunu i\u00e7in label distribution analizi\")\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Label distribution analizi\n",
        "print(\"\ud83d\udd0d VER\u0130 ANAL\u0130Z\u0130 VE BIAS TESP\u0130T\u0130:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\ud83d\udcca Dataset analizi:\")\n",
        "print(f\"  Toplam: {len(texts)} yorum\")\n",
        "print(f\"  Faydas\u0131z (0): {np.sum(np.array(labels)==0)} (%{np.mean(np.array(labels)==0)*100:.1f})\")\n",
        "print(f\"  Faydal\u0131 (1): {np.sum(np.array(labels)==1)} (%{np.mean(np.array(labels)==1)*100:.1f})\")\n",
        "\n",
        "# Sample positive examples\n",
        "positive_samples = [text for text, label in zip(texts, labels) if label == 1][:10]\n",
        "negative_samples = [text for text, label in zip(texts, labels) if label == 0][:10]\n",
        "\n",
        "print(f\"\\n\ud83d\udcdd FAYDARLI \u00d6RNEK YORUMLAR:\")\n",
        "for i, sample in enumerate(positive_samples[:3], 1):\n",
        "    print(f\"  {i}. {sample[:100]}...\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcdd FAYDASIZ \u00d6RNEK YORUMLAR:\")\n",
        "for i, sample in enumerate(negative_samples[:3], 1):\n",
        "    print(f\"  {i}. {sample[:100]}...\")\n",
        "\n",
        "# Label consistency check\n",
        "print(f\"\\n\ud83d\udd0d LABEL TUTARLILIK KONTROL\u00dc:\")\n",
        "positive_keywords = ['harika', 'm\u00fckemmel', 'g\u00fczel', 'iyi', 'tavsiye', 'be\u011fen']\n",
        "negative_keywords = ['k\u00f6t\u00fc', 'berbat', 'fena', 'bozuk', 'k\u0131r\u0131k', 'sorun']\n",
        "\n",
        "positive_in_negative = 0\n",
        "negative_in_positive = 0\n",
        "\n",
        "for text, label in zip(texts[:1000], labels[:1000]):  # \u0130lk 1000'i kontrol et\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    if label == 0:  # Faydas\u0131z etiketli\n",
        "        if any(word in text_lower for word in positive_keywords):\n",
        "            positive_in_negative += 1\n",
        "    else:  # Faydal\u0131 etiketli\n",
        "        if any(word in text_lower for word in negative_keywords):\n",
        "            negative_in_positive += 1\n",
        "\n",
        "print(f\"  Faydas\u0131z etiketli ama pozitif kelimeli: {positive_in_negative}\")\n",
        "print(f\"  Faydal\u0131 etiketli ama negatif kelimeli: {negative_in_positive}\")\n",
        "\n",
        "# \u00c7\u00d6Z\u00dcM 1: XLM-RoBERTa-LARGE Model\n",
        "print(f\"\\n\ud83e\udd16 XLM-ROBERTA-LARGE MODEL DENEMES\u0130:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "\n",
        "# Large model deneme\n",
        "large_model_success = False\n",
        "try:\n",
        "    print(\"\ud83d\udce6 XLM-RoBERTa-LARGE y\u00fckleniyor...\")\n",
        "    large_model_name = \"xlm-roberta-large\"\n",
        "\n",
        "    # Memory check\n",
        "    if torch.cuda.is_available():\n",
        "        memory_gb = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "        print(f\"\ud83d\udcbe GPU Memory: {memory_gb:.1f} GB\")\n",
        "\n",
        "        if memory_gb >= 40:  # A100 i\u00e7in yeterli\n",
        "            tokenizer_large = AutoTokenizer.from_pretrained(large_model_name)\n",
        "            model_large = AutoModelForSequenceClassification.from_pretrained(\n",
        "                large_model_name,\n",
        "                num_labels=2,\n",
        "                ignore_mismatched_sizes=True\n",
        "            )\n",
        "            model_large.to(device)\n",
        "            print(\"\u2705 XLM-RoBERTa-LARGE y\u00fcklendi!\")\n",
        "            large_model_success = True\n",
        "        else:\n",
        "            print(\"\u26a0\ufe0f GPU memory yetersiz XLM-RoBERTa-LARGE i\u00e7in\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f GPU bulunamad\u0131\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\u274c XLM-RoBERTa-LARGE hatas\u0131: {e}\")\n",
        "\n",
        "# \u00c7\u00d6Z\u00dcM 2: Bias Correction Strategy\n",
        "print(f\"\\n\ud83d\udd27 BIAS CORRECTION STRATEJ\u0130S\u0130:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if large_model_success:\n",
        "    print(\"\u2705 Large model ile devam ediliyor...\")\n",
        "\n",
        "    # H\u0131zl\u0131 fine-tuning i\u00e7in k\u00fc\u00e7\u00fck dataset\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Balanced sampling - her s\u0131n\u0131ftan e\u015fit miktar\n",
        "    from collections import Counter\n",
        "\n",
        "    # Train setinden balanced subset al\n",
        "    train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
        "\n",
        "    # Her s\u0131n\u0131ftan minimum say\u0131 kadar al\n",
        "    min_class_count = min(Counter(train_labels).values())\n",
        "    balanced_samples = train_df.groupby('label').apply(\n",
        "        lambda x: x.sample(min(len(x), min_class_count), random_state=42)\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    balanced_texts = balanced_samples['text'].tolist()\n",
        "    balanced_labels = balanced_samples['label'].tolist()\n",
        "\n",
        "    print(f\"\ud83d\udcca Balanced training set:\")\n",
        "    print(f\"  Original: {len(train_texts)} samples\")\n",
        "    print(f\"  Balanced: {len(balanced_texts)} samples\")\n",
        "    print(f\"  Distribution: {Counter(balanced_labels)}\")\n",
        "\n",
        "    # Dataset class\n",
        "    class BiasCorrectDataset(Dataset):\n",
        "        def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "            self.texts = texts\n",
        "            self.labels = labels\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.texts)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            text = str(self.texts[idx]).strip()\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            encoding = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                'input_ids': encoding['input_ids'].flatten(),\n",
        "                'attention_mask': encoding['attention_mask'].flatten(),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "        acc = accuracy_score(labels, predictions)\n",
        "\n",
        "        # Bias check\n",
        "        pred_dist = Counter(predictions)\n",
        "        label_dist = Counter(labels)\n",
        "        print(f\"    Pred: {dict(pred_dist)} | True: {dict(label_dist)}\")\n",
        "\n",
        "        return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "    # Fast training setup\n",
        "    print(f\"\\n\ud83d\ude80 LARGE MODEL FAST FINE-TUNING:\")\n",
        "\n",
        "    train_dataset = BiasCorrectDataset(balanced_texts, balanced_labels, tokenizer_large, 384)\n",
        "    val_dataset = BiasCorrectDataset(val_texts, val_labels, tokenizer_large, 384)\n",
        "\n",
        "    # A100 i\u00e7in optimize edilmi\u015f parametreler\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./large_results',\n",
        "        num_train_epochs=3,  # H\u0131zl\u0131 i\u00e7in az epoch\n",
        "        per_device_train_batch_size=8,  # Large model i\u00e7in k\u00fc\u00e7\u00fck batch\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=4,  # Effective batch = 32\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=1e-5,  # Large model i\u00e7in d\u00fc\u015f\u00fck LR\n",
        "        logging_steps=25,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=2,\n",
        "        seed=42,\n",
        "        dataloader_pin_memory=True,\n",
        "        bf16=True,  # A100 i\u00e7in BF16\n",
        "        report_to=\"none\",\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model_large,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    print(\"\u23f0 Tahmini s\u00fcre: 15-25 dakika\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Large model fine-tuning\n",
        "        trainer.train()\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"\\n\u2705 LARGE MODEL FINE-TUNING TAMAMLANDI! ({training_time/60:.1f} dakika)\")\n",
        "\n",
        "        # Evaluation\n",
        "        eval_results = trainer.evaluate()\n",
        "        large_f1 = eval_results['eval_f1']\n",
        "\n",
        "        print(f\"\\n\ud83c\udfc6 LARGE MODEL SONU\u00c7LARI:\")\n",
        "        print(f\"  F1 Score: {large_f1:.4f}\")\n",
        "        print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "        # Test\n",
        "        print(f\"\\n\ud83e\uddea LARGE MODEL TEST:\")\n",
        "        test_samples = [\n",
        "            \"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\",\n",
        "            \"Berbat bir deneyim, hi\u00e7 tavsiye etmem.\",\n",
        "            \"Muhte\u015fem kalite, herkese tavsiye ederim!\",\n",
        "            \"\u00c7ok k\u00f6t\u00fc bir \u00fcr\u00fcn, para israf\u0131.\"\n",
        "        ]\n",
        "\n",
        "        for i, test_text in enumerate(test_samples, 1):\n",
        "            inputs = tokenizer_large(test_text, return_tensors=\"pt\", truncation=True, max_length=384)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model_large(**inputs)\n",
        "                prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "                confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "            result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "            print(f\"  {i}. '{test_text[:50]}...'\")\n",
        "            print(f\"     \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "        # Kar\u015f\u0131la\u015ft\u0131rma\n",
        "        ensemble_f1 = 0.9085\n",
        "        improvement = large_f1 - ensemble_f1\n",
        "\n",
        "        print(f\"\\n\ud83c\udf89 FINAL KAR\u015eILA\u015eTIRMA:\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Ensemble result:    {ensemble_f1:.4f} F1\")\n",
        "        print(f\"Large model:        {large_f1:.4f} F1\")\n",
        "        print(f\"Improvement:        {improvement:+.4f} F1\")\n",
        "\n",
        "        if large_f1 >= 0.92:\n",
        "            print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF ULA\u015eILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "            print(f\"\ud83c\udf1f LARGE MODEL \u0130LE WORLD-CLASS PERFORMANCE!\")\n",
        "        elif large_f1 >= 0.915:\n",
        "            print(f\"\\n\ud83d\udd25 NEREDEYSE %92! \u00c7OK YAKLA\u015eTINIZ!\")\n",
        "        elif large_f1 > ensemble_f1:\n",
        "            print(f\"\\n\u2705 LARGE MODEL DAHA \u0130Y\u0130!\")\n",
        "        else:\n",
        "            print(f\"\\n\ud83d\udcca Ensemble hala en iyisi\")\n",
        "\n",
        "        # Model kaydet\n",
        "        save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_large_model\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        model_large.save_pretrained(save_path)\n",
        "        tokenizer_large.save_pretrained(save_path)\n",
        "        print(f\"\\n\u2705 Large model kaydedildi: {save_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Large model training hatas\u0131: {e}\")\n",
        "        large_model_success = False\n",
        "\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f Large model kullan\u0131lam\u0131yor\")\n",
        "\n",
        "# \u00c7\u00d6Z\u00dcM 3: Data Quality Analysis\n",
        "print(f\"\\n\ud83d\udcca VER\u0130 KAL\u0130TES\u0130 ANAL\u0130Z\u0130:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Problematic samples detection\n",
        "print(\"\ud83d\udd0d Problemli \u00f6rnekler tespiti:\")\n",
        "\n",
        "problematic_count = 0\n",
        "for i, (text, label) in enumerate(zip(texts[:100], labels[:100])):\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Pozitif kelimeler ama negatif etiket\n",
        "    if label == 0 and any(word in text_lower for word in ['harika', 'm\u00fckemmel', 'g\u00fczel', 'iyi']):\n",
        "        problematic_count += 1\n",
        "        if problematic_count <= 3:\n",
        "            print(f\"  \u26a0\ufe0f Label 0 ama pozitif: '{text[:80]}...'\")\n",
        "\n",
        "    # Negatif kelimeler ama pozitif etiket\n",
        "    if label == 1 and any(word in text_lower for word in ['berbat', 'k\u00f6t\u00fc', 'fena']):\n",
        "        problematic_count += 1\n",
        "        if problematic_count <= 3:\n",
        "            print(f\"  \u26a0\ufe0f Label 1 ama negatif: '{text[:80]}...'\")\n",
        "\n",
        "print(f\"Toplam problematic sample (ilk 100'de): {problematic_count}\")\n",
        "\n",
        "# FINAL RECOMMENDATION\n",
        "print(f\"\\n\ud83c\udfaf FINAL \u00d6NER\u0130LER:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "best_score = 0.9085  # Ensemble score\n",
        "\n",
        "if 'large_f1' in locals() and large_f1 > best_score:\n",
        "    best_score = large_f1\n",
        "    best_method = \"XLM-RoBERTa-LARGE\"\n",
        "else:\n",
        "    best_method = \"Ensemble (Average Probabilities)\"\n",
        "\n",
        "print(f\"\ud83c\udfc6 EN \u0130Y\u0130 SONU\u00c7: {best_score:.4f} F1\")\n",
        "print(f\"\ud83c\udfc6 EN \u0130Y\u0130 Y\u00d6NTEM: {best_method}\")\n",
        "\n",
        "if best_score >= 0.92:\n",
        "    print(f\"\\n\ud83c\udf8a\ud83c\udf8a %92+ HEDEF BA\u015eARILDI! \ud83c\udf8a\ud83c\udf8a\")\n",
        "    print(f\"\ud83c\udf1f WORLD-CLASS PERFORMANCE ACHIEVED!\")\n",
        "elif best_score >= 0.91:\n",
        "    print(f\"\\n\ud83d\udd25 %91+ EXCELLENT SCORE! \ud83d\udd25\")\n",
        "    print(f\"\u2728 Sadece {0.92 - best_score:.3f} kald\u0131 %92 i\u00e7in!\")\n",
        "else:\n",
        "    print(f\"\\n\ud83d\udcc8 %90+ BA\u015eARILI! \ud83d\udcc8\")\n",
        "    print(f\"\ud83d\udcaa G\u00fc\u00e7l\u00fc bir ba\u015flang\u0131\u00e7 noktas\u0131!\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 %92+ \u0130\u00c7\u0130N SON ADIMLAR:\")\n",
        "print(\"=\"*40)\n",
        "if best_score >= 0.91:\n",
        "    print(\"\ud83d\udd25 \u00c7OK YAKLA\u015eTINIZ!\")\n",
        "    print(\"  \u2022 Cross-validation ile 5-fold training\")\n",
        "    print(\"  \u2022 Daha fazla model ile ensemble (3-5 model)\")\n",
        "    print(\"  \u2022 Hyperparameter optimization (Optuna)\")\n",
        "    print(\"  \u2022 Test-time augmentation\")\n",
        "else:\n",
        "    print(\"\ud83d\udcc8 Daha fazla iyile\u015ftirme:\")\n",
        "    print(\"  \u2022 Data cleaning ve re-labeling\")\n",
        "    print(\"  \u2022 Farkl\u0131 model mimarileri (BERT, DistilBERT)\")\n",
        "    print(\"  \u2022 Advanced preprocessing\")\n",
        "    print(\"  \u2022 Active learning strategies\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfc1 PROJE SONUCU:\")\n",
        "print(\"=\"*30)\n",
        "print(f\"\u2022 En iyi F1 Score: {best_score:.4f}\")\n",
        "print(f\"\u2022 Hedef (%92): {'\u2705 ULA\u015eILDI' if best_score >= 0.92 else f'\u274c -{(0.92-best_score):.3f} kald\u0131'}\")\n",
        "print(f\"\u2022 Y\u00f6ntem: {best_method}\")\n",
        "print(f\"\u2022 Ba\u015far\u0131 durumu: {'LEGENDARY' if best_score >= 0.92 else 'EXCELLENT' if best_score >= 0.91 else 'GOOD'}\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "print(f\"\ud83c\udf8a ULTIMATE %92+ QUEST TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 84205,
          "status": "ok",
          "timestamp": 1749981160969,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "9hNq6u71ABHj",
        "outputId": "9f29e022-30eb-4c83-d415-7bea8ea5e9e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 TAM VAL\u0130DAT\u0130ON SET \u0130LE ENSEMBLE TEST\n",
            "============================================================\n",
            "\ud83c\udfaf T\u00fcm 2,276 validation \u00f6rne\u011fi ile ger\u00e7ek test\n",
            "\u26a1 Ensemble vs 4 Epoch model kar\u015f\u0131la\u015ft\u0131rmas\u0131\n",
            "\ud83d\udcca Ger\u00e7ek F1 Score hesaplamas\u0131\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 TAM validation set: 2276 yorum\n",
            "\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [1003 1273]\n",
            "\u2705 xlm_roberta_fine_tuned_model bulundu\n",
            "\u2705 xlm_roberta_optimal_model bulundu\n",
            "\n",
            "\ud83e\udd16 2 MODEL Y\u00dcKLEN\u0130YOR...\n",
            "\ud83d\udce6 Model_1 y\u00fckleniyor...\n",
            "\u2705 Model_1 y\u00fcklendi\n",
            "\ud83d\udce6 Model_2 y\u00fckleniyor...\n",
            "\u2705 Model_2 y\u00fcklendi\n",
            "\n",
            "\ud83d\udcca BASELINE: 4 EPOCH MODEL TEST\u0130 (TAM VAL\u0130DAT\u0130ON):\n",
            "============================================================\n",
            "\ud83d\udd04 2276 \u00f6rnek tahmin ediliyor...\n",
            "  Progress: 0/2276\n",
            "  Progress: 500/2276\n",
            "  Progress: 1000/2276\n",
            "  Progress: 1500/2276\n",
            "  Progress: 2000/2276\n",
            "\u2705 Prediction tamamland\u0131 (20.8s)\n",
            "\n",
            "\ud83c\udfc6 4 EPOCH MODEL (TAM VAL\u0130DAT\u0130ON) SONU\u00c7LARI:\n",
            "  F1 Score: 0.8953\n",
            "  Accuracy: 0.8967\n",
            "  Precision: 0.8953\n",
            "  Recall: 0.8952\n",
            "\n",
            "\ud83d\udcca KAR\u015eILA\u015eTIRMA:\n",
            "  Bilinen en iyi: 0.8948\n",
            "  \u015eu anki test:   0.8953\n",
            "  Fark:           +0.0005\n",
            "\n",
            "\ud83d\udd04 ENSEMBLE TEST (TAM VAL\u0130DAT\u0130ON SET):\n",
            "============================================================\n",
            "\ud83d\udd04 Ensemble: 2276 \u00f6rnek tahmin ediliyor...\n",
            "  Ensemble progress: 0/2276\n",
            "  Ensemble progress: 500/2276\n",
            "  Ensemble progress: 1000/2276\n",
            "  Ensemble progress: 1500/2276\n",
            "  Ensemble progress: 2000/2276\n",
            "\u2705 Ensemble prediction tamamland\u0131 (42.0s)\n",
            "\n",
            "\ud83d\udcca TAM ENSEMBLE SONU\u00c7LARI:\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfc6 Majority Voting (TAM VAL\u0130DAT\u0130ON):\n",
            "  F1 Score: 0.8946\n",
            "  Accuracy: 0.8959\n",
            "  Precision: 0.8938\n",
            "  Recall: 0.8957\n",
            "  Pred dist: [1028 1248] | True: [1003 1273]\n",
            "\n",
            "\ud83c\udfc6 Average Probabilities (TAM VAL\u0130DAT\u0130ON):\n",
            "  F1 Score: 0.8967\n",
            "  Accuracy: 0.8985\n",
            "  Precision: 0.8983\n",
            "  Recall: 0.8954\n",
            "  Pred dist: [ 972 1304] | True: [1003 1273]\n",
            "\n",
            "\ud83c\udfc6 Weighted Average (TAM VAL\u0130DAT\u0130ON):\n",
            "  F1 Score: 0.8967\n",
            "  Accuracy: 0.8985\n",
            "  Precision: 0.8983\n",
            "  Recall: 0.8954\n",
            "  Pred dist: [ 972 1304] | True: [1003 1273]\n",
            "\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 ENSEMBLE Y\u00d6NTEM\u0130: Average Probabilities\n",
            "\ud83c\udfaf En iyi ensemble F1: 0.8967\n",
            "\n",
            "\ud83c\udf89 GER\u00c7EK SONU\u00c7 KAR\u015eILA\u015eTIRMASI (TAM VAL\u0130DAT\u0130ON):\n",
            "======================================================================\n",
            "4 Epoch tek model:    0.8953 F1\n",
            "En iyi ensemble:      0.8967 F1\n",
            "Ensemble kazanc\u0131:     +0.0014 F1 (+0.16%)\n",
            "Hedefe mesafe:        +0.0233\n",
            "\n",
            "\ud83c\udfc1 GER\u00c7EK PROJE SONUCU (TAM VAL\u0130DAT\u0130ON):\n",
            "============================================================\n",
            "\u2022 En iyi F1 Score: 0.8967\n",
            "\u2022 En iyi y\u00f6ntem: Ensemble (Average Probabilities)\n",
            "\u2022 Durum: \ud83d\udd25 \u00c7OK YAKIN! %89.5+\n",
            "\u2022 Ba\u015far\u0131 seviyesi: VERY_GOOD\n",
            "\n",
            "\ud83e\uddea EN \u0130Y\u0130 MODEL TEST \u00d6RNEKLER\u0130:\n",
            "==================================================\n",
            "Model: Ensemble (Average Probabilities)\n",
            "  1. 'Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!'\n",
            "     \u2192 Faydas\u0131z\n",
            "  2. 'Berbat bir deneyim, hi\u00e7 tavsiye etmem.'\n",
            "     \u2192 Faydas\u0131z\n",
            "  3. 'Fiyat\u0131na g\u00f6re ortalama kalitede.'\n",
            "     \u2192 Faydas\u0131z\n",
            "  4. 'Muhte\u015fem kalite, herkese tavsiye ederim!'\n",
            "     \u2192 Faydas\u0131z\n",
            "\n",
            "\ud83d\udca1 SON TAVS\u0130YE:\n",
            "==============================\n",
            "\ud83d\udcc8 Daha fazla iyile\u015ftirme gerekli:\n",
            "  \u2022 Model hiperparameter tuning\n",
            "  \u2022 Veri kalitesi art\u0131r\u0131m\u0131\n",
            "  \u2022 Ensemble stratejileri\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\ud83c\udfc1 TAM VAL\u0130DAT\u0130ON TEST TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "print(\"\ud83d\udd25 TAM VAL\u0130DAT\u0130ON SET \u0130LE ENSEMBLE TEST\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf T\u00fcm 2,276 validation \u00f6rne\u011fi ile ger\u00e7ek test\")\n",
        "print(\"\u26a1 Ensemble vs 4 Epoch model kar\u015f\u0131la\u015ft\u0131rmas\u0131\")\n",
        "print(\"\ud83d\udcca Ger\u00e7ek F1 Score hesaplamas\u0131\")\n",
        "print()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "\n",
        "# Veri y\u00fckleme (ayn\u0131 split)\n",
        "print(\"\ud83d\udcca TAM VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "# AYNI SPL\u0130T (tutarl\u0131l\u0131k i\u00e7in)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.15, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\u2705 TAM validation set: {len(val_texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Val s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "# Mevcut modelleri kontrol et\n",
        "model_paths = [\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_fine_tuned_model\",  # 4 epoch %89.48\n",
        "    \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/xlm_roberta_optimal_model\",     # Optimal\n",
        "]\n",
        "\n",
        "available_models = []\n",
        "for i, path in enumerate(model_paths):\n",
        "    if os.path.exists(path):\n",
        "        available_models.append((path, f\"Model_{i+1}\"))\n",
        "        print(f\"\u2705 {path.split('/')[-1]} bulundu\")\n",
        "\n",
        "if len(available_models) < 1:\n",
        "    print(\"\u274c Model bulunamad\u0131!\")\n",
        "    exit()\n",
        "\n",
        "# Modelleri y\u00fckle\n",
        "print(f\"\\n\ud83e\udd16 {len(available_models)} MODEL Y\u00dcKLEN\u0130YOR...\")\n",
        "models = []\n",
        "tokenizers = []\n",
        "\n",
        "for model_path, model_name in available_models:\n",
        "    try:\n",
        "        print(f\"\ud83d\udce6 {model_name} y\u00fckleniyor...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        models.append(model)\n",
        "        tokenizers.append(tokenizer)\n",
        "        print(f\"\u2705 {model_name} y\u00fcklendi\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c {model_name} y\u00fcklenemedi: {e}\")\n",
        "\n",
        "if len(models) == 0:\n",
        "    print(\"\u274c Hi\u00e7 model y\u00fcklenemedi!\")\n",
        "    exit()\n",
        "\n",
        "# TEK MODEL TEST (4 epoch - baseline)\n",
        "print(f\"\\n\ud83d\udcca BASELINE: 4 EPOCH MODEL TEST\u0130 (TAM VAL\u0130DAT\u0130ON):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def predict_single_model(texts, model, tokenizer, max_length=256):\n",
        "    \"\"\"Tek model ile t\u00fcm validation set prediction\"\"\"\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "\n",
        "    print(f\"\ud83d\udd04 {len(texts)} \u00f6rnek tahmin ediliyor...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        if i % 500 == 0:\n",
        "            print(f\"  Progress: {i}/{len(texts)}\")\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=True\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "            confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "            predictions.append(predicted_class)\n",
        "            confidences.append(confidence)\n",
        "\n",
        "    prediction_time = time.time() - start_time\n",
        "    print(f\"\u2705 Prediction tamamland\u0131 ({prediction_time:.1f}s)\")\n",
        "\n",
        "    return predictions, confidences\n",
        "\n",
        "# 4 epoch model ile tam test\n",
        "single_predictions, single_confidences = predict_single_model(val_texts, models[0], tokenizers[0])\n",
        "\n",
        "# Metrics hesapla\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(val_labels, single_predictions, average='macro')\n",
        "acc = accuracy_score(val_labels, single_predictions)\n",
        "\n",
        "print(f\"\\n\ud83c\udfc6 4 EPOCH MODEL (TAM VAL\u0130DAT\u0130ON) SONU\u00c7LARI:\")\n",
        "print(f\"  F1 Score: {f1:.4f}\")\n",
        "print(f\"  Accuracy: {acc:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "\n",
        "single_f1 = f1\n",
        "baseline_f1 = 0.8948  # Bilinen en iyi\n",
        "\n",
        "print(f\"\\n\ud83d\udcca KAR\u015eILA\u015eTIRMA:\")\n",
        "print(f\"  Bilinen en iyi: {baseline_f1:.4f}\")\n",
        "print(f\"  \u015eu anki test:   {single_f1:.4f}\")\n",
        "print(f\"  Fark:           {single_f1 - baseline_f1:+.4f}\")\n",
        "\n",
        "# ENSEMBLE TEST (TAM VAL\u0130DAT\u0130ON)\n",
        "if len(models) > 1:\n",
        "    print(f\"\\n\ud83d\udd04 ENSEMBLE TEST (TAM VAL\u0130DAT\u0130ON SET):\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    def ensemble_predict_full(texts, models, tokenizers, max_length=256):\n",
        "        \"\"\"Ensemble prediction t\u00fcm validation set i\u00e7in\"\"\"\n",
        "        majority_predictions = []\n",
        "        avg_predictions = []\n",
        "        weighted_predictions = []\n",
        "\n",
        "        print(f\"\ud83d\udd04 Ensemble: {len(texts)} \u00f6rnek tahmin ediliyor...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, text in enumerate(texts):\n",
        "            if i % 500 == 0:\n",
        "                print(f\"  Ensemble progress: {i}/{len(texts)}\")\n",
        "\n",
        "            all_predictions = []\n",
        "            all_confidences = []\n",
        "\n",
        "            for model, tokenizer in zip(models, tokenizers):\n",
        "                inputs = tokenizer(\n",
        "                    text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    padding=True\n",
        "                )\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "\n",
        "                    all_predictions.append(predicted_class)\n",
        "                    all_confidences.append(probabilities[0].cpu().numpy())\n",
        "\n",
        "            # Ensemble methods\n",
        "            majority_vote = np.bincount(all_predictions).argmax()\n",
        "            avg_probs = np.mean(all_confidences, axis=0)\n",
        "            avg_prediction = np.argmax(avg_probs)\n",
        "\n",
        "            weights = np.array([max(conf) for conf in all_confidences])\n",
        "            weights = weights / weights.sum()\n",
        "            weighted_probs = np.average(all_confidences, axis=0, weights=weights)\n",
        "            weighted_prediction = np.argmax(weighted_probs)\n",
        "\n",
        "            majority_predictions.append(majority_vote)\n",
        "            avg_predictions.append(avg_prediction)\n",
        "            weighted_predictions.append(weighted_prediction)\n",
        "\n",
        "        prediction_time = time.time() - start_time\n",
        "        print(f\"\u2705 Ensemble prediction tamamland\u0131 ({prediction_time:.1f}s)\")\n",
        "\n",
        "        return majority_predictions, avg_predictions, weighted_predictions\n",
        "\n",
        "    # Tam ensemble test\n",
        "    majority_preds, avg_preds, weighted_preds = ensemble_predict_full(val_texts, models, tokenizers)\n",
        "\n",
        "    # T\u00fcm ensemble y\u00f6ntemlerini de\u011ferlendir\n",
        "    def evaluate_ensemble_method(predictions, true_labels, method_name):\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            true_labels, predictions, average='macro'\n",
        "        )\n",
        "        acc = accuracy_score(true_labels, predictions)\n",
        "\n",
        "        print(f\"\\n\ud83c\udfc6 {method_name} (TAM VAL\u0130DAT\u0130ON):\")\n",
        "        print(f\"  F1 Score: {f1:.4f}\")\n",
        "        print(f\"  Accuracy: {acc:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "\n",
        "        pred_dist = np.bincount(predictions, minlength=2)\n",
        "        true_dist = np.bincount(true_labels, minlength=2)\n",
        "        print(f\"  Pred dist: {pred_dist} | True: {true_dist}\")\n",
        "\n",
        "        return f1\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca TAM ENSEMBLE SONU\u00c7LARI:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    majority_f1 = evaluate_ensemble_method(majority_preds, val_labels, \"Majority Voting\")\n",
        "    avg_f1 = evaluate_ensemble_method(avg_preds, val_labels, \"Average Probabilities\")\n",
        "    weighted_f1 = evaluate_ensemble_method(weighted_preds, val_labels, \"Weighted Average\")\n",
        "\n",
        "    # En iyi ensemble y\u00f6ntemini belirle\n",
        "    best_ensemble_f1 = max(majority_f1, avg_f1, weighted_f1)\n",
        "\n",
        "    if best_ensemble_f1 == majority_f1:\n",
        "        best_method = \"Majority Voting\"\n",
        "    elif best_ensemble_f1 == avg_f1:\n",
        "        best_method = \"Average Probabilities\"\n",
        "    else:\n",
        "        best_method = \"Weighted Average\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 EN \u0130Y\u0130 ENSEMBLE Y\u00d6NTEM\u0130: {best_method}\")\n",
        "    print(f\"\ud83c\udfaf En iyi ensemble F1: {best_ensemble_f1:.4f}\")\n",
        "\n",
        "    # FINAL KAR\u015eILA\u015eTIRMA\n",
        "    print(f\"\\n\ud83c\udf89 GER\u00c7EK SONU\u00c7 KAR\u015eILA\u015eTIRMASI (TAM VAL\u0130DAT\u0130ON):\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"4 Epoch tek model:    {single_f1:.4f} F1\")\n",
        "    print(f\"En iyi ensemble:      {best_ensemble_f1:.4f} F1\")\n",
        "\n",
        "    ensemble_improvement = best_ensemble_f1 - single_f1\n",
        "    print(f\"Ensemble kazanc\u0131:     {ensemble_improvement:+.4f} F1 ({ensemble_improvement/single_f1*100:+.2f}%)\")\n",
        "\n",
        "    # Hedef analizi\n",
        "    target_92 = 0.92\n",
        "    print(f\"Hedefe mesafe:        {target_92 - best_ensemble_f1:+.4f}\")\n",
        "\n",
        "    final_best_f1 = max(single_f1, best_ensemble_f1)\n",
        "    final_best_method = \"4 Epoch Model\" if single_f1 >= best_ensemble_f1 else f\"Ensemble ({best_method})\"\n",
        "\n",
        "else:\n",
        "    # Tek model durumu\n",
        "    final_best_f1 = single_f1\n",
        "    final_best_method = \"4 Epoch Model\"\n",
        "\n",
        "# BA\u015eARI DE\u011eERLEND\u0130RMES\u0130\n",
        "print(f\"\\n\ud83c\udfc1 GER\u00c7EK PROJE SONUCU (TAM VAL\u0130DAT\u0130ON):\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\u2022 En iyi F1 Score: {final_best_f1:.4f}\")\n",
        "print(f\"\u2022 En iyi y\u00f6ntem: {final_best_method}\")\n",
        "\n",
        "if final_best_f1 >= 0.92:\n",
        "    print(f\"\u2022 Hedef (%92): \u2705 ULA\u015eILDI\")\n",
        "    achievement = \"LEGENDARY\"\n",
        "elif final_best_f1 >= 0.90:\n",
        "    print(f\"\u2022 Hedef (%90): \u2705 ULA\u015eILDI\")\n",
        "    print(f\"\u2022 %92 i\u00e7in: -{(0.92-final_best_f1):.3f} kald\u0131\")\n",
        "    achievement = \"EXCELLENT\"\n",
        "elif final_best_f1 >= 0.895:\n",
        "    print(f\"\u2022 Durum: \ud83d\udd25 \u00c7OK YAKIN! %89.5+\")\n",
        "    achievement = \"VERY_GOOD\"\n",
        "else:\n",
        "    print(f\"\u2022 Durum: \ud83d\udcc8 \u0130yi ba\u015flang\u0131\u00e7\")\n",
        "    achievement = \"GOOD\"\n",
        "\n",
        "print(f\"\u2022 Ba\u015far\u0131 seviyesi: {achievement}\")\n",
        "\n",
        "# Test \u00f6rnekleri (en iyi model ile)\n",
        "if len(models) > 1 and 'best_method' in locals():\n",
        "    print(f\"\\n\ud83e\uddea EN \u0130Y\u0130 MODEL TEST \u00d6RNEKLER\u0130:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    test_samples = [\n",
        "        \"Bu \u00fcr\u00fcn kesinlikle harika, \u00e7ok memnunum!\",\n",
        "        \"Berbat bir deneyim, hi\u00e7 tavsiye etmem.\",\n",
        "        \"Fiyat\u0131na g\u00f6re ortalama kalitede.\",\n",
        "        \"Muhte\u015fem kalite, herkese tavsiye ederim!\",\n",
        "    ]\n",
        "\n",
        "    print(f\"Model: {final_best_method}\")\n",
        "\n",
        "    for i, test_text in enumerate(test_samples, 1):\n",
        "        if \"Ensemble\" in final_best_method:\n",
        "            # Ensemble prediction\n",
        "            all_preds = []\n",
        "            for model, tokenizer in zip(models, tokenizers):\n",
        "                inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs)\n",
        "                    pred = torch.argmax(outputs.logits, dim=-1).item()\n",
        "                    all_preds.append(pred)\n",
        "\n",
        "            if \"Majority\" in best_method:\n",
        "                final_pred = np.bincount(all_preds).argmax()\n",
        "            else:\n",
        "                final_pred = int(np.mean(all_preds) > 0.5)\n",
        "        else:\n",
        "            # Single model prediction\n",
        "            inputs = tokenizers[0](test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = models[0](**inputs)\n",
        "                final_pred = torch.argmax(outputs.logits, dim=-1).item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if final_pred == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"  {i}. '{test_text}'\")\n",
        "        print(f\"     \u2192 {result}\")\n",
        "\n",
        "# Son tavsiye\n",
        "print(f\"\\n\ud83d\udca1 SON TAVS\u0130YE:\")\n",
        "print(\"=\"*30)\n",
        "if final_best_f1 >= 0.92:\n",
        "    print(\"\ud83c\udf8a Hedef ula\u015f\u0131ld\u0131! Proje ba\u015far\u0131l\u0131!\")\n",
        "elif final_best_f1 >= 0.90:\n",
        "    print(\"\u2705 %90+ ba\u015far\u0131l\u0131! %92 i\u00e7in:\")\n",
        "    print(\"  \u2022 Daha fazla model ile ensemble\")\n",
        "    print(\"  \u2022 Veri temizleme stratejisi\")\n",
        "    print(\"  \u2022 Cross-validation\")\n",
        "else:\n",
        "    print(\"\ud83d\udcc8 Daha fazla iyile\u015ftirme gerekli:\")\n",
        "    print(\"  \u2022 Model hiperparameter tuning\")\n",
        "    print(\"  \u2022 Veri kalitesi art\u0131r\u0131m\u0131\")\n",
        "    print(\"  \u2022 Ensemble stratejileri\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "print(f\"\ud83c\udfc1 TAM VAL\u0130DAT\u0130ON TEST TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1235233,
          "status": "error",
          "timestamp": 1750090738272,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "4VGQ5TtUay-Q",
        "outputId": "cb1ed61e-0638-48eb-8bb5-5b0edae793fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 CROSS-VALIDATION ENSEMBLE - 90%+ HEDEF\n",
            "============================================================\n",
            "\ud83c\udfaf 5-Fold CV ile 5 farkl\u0131 model e\u011fitimi\n",
            "\ud83c\udfc6 Hedef: %90+ F1 Score\n",
            "\u26a1 Advanced ensemble teknikleri\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n",
            "\u26a1 A100 GPU - ULTIMATE CV MODE!\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Dosya ba\u015far\u0131yla okundu!\n",
            "\ud83d\udcca Columns: ['metin', 'tahmin', 'etiket']\n",
            "\ud83d\udcca Temizlenmi\u015f columns: ['metin', 'tahmin', 'etiket']\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum (1.0s)\n",
            "\ud83d\udcca Toplam veri: 15167\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Faydas\u0131z: 6686 (%44.1)\n",
            "\n",
            "\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 XLM-RoBERTa tokenizer y\u00fcklendi!\n",
            "\n",
            "\ud83d\ude80 CV ENSEMBLE EXECUTION BA\u015eLIYOR...\n",
            "\n",
            "\ud83d\ude80 5-FOLD CROSS VALIDATION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83d\udd04 FOLD 1 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12133, Val: 3034\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5348 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1338 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:00, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.040100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.036200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.031000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.019300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 1 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.0 dakika\n",
            "\ud83c\udfaf F1: 0.8912\n",
            "\ud83c\udfaf Accuracy: 0.8929\n",
            "\n",
            "\ud83d\udd04 FOLD 2 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12133, Val: 3034\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6784]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1697]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:04, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.070300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.041600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 2 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8831\n",
            "\ud83c\udfaf Accuracy: 0.8853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd04 FOLD 3 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:04, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.068400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.038700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.037500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.035600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.030100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 3 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8890\n",
            "\ud83c\udfaf Accuracy: 0.8912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd04 FOLD 4 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:01, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.040400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.027900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.020700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.020900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 4 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.0 dakika\n",
            "\ud83c\udfaf F1: 0.8932\n",
            "\ud83c\udfaf Accuracy: 0.8955\n",
            "\n",
            "\ud83d\udd04 FOLD 5 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:03, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.029100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 5 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8961\n",
            "\ud83c\udfaf Accuracy: 0.8981\n",
            "\n",
            "\u2705 T\u00dcM FOLD'LAR TAMAMLANDI! (20.5 dakika)\n",
            "\n",
            "\ud83d\udcca FOLD PERFORMANSLARI:\n",
            "========================================\n",
            "Fold 1: F1=0.8912, Acc=0.8929\n",
            "Fold 2: F1=0.8831, Acc=0.8853\n",
            "Fold 3: F1=0.8890, Acc=0.8912\n",
            "Fold 4: F1=0.8932, Acc=0.8955\n",
            "Fold 5: F1=0.8961, Acc=0.8981\n",
            "\n",
            "\ud83d\udcc8 ORTALAMA: F1=0.8905 \u00b1 0.0044\n",
            "\n",
            "\ud83c\udfaf ENSEMBLE COMBINATION TESTING...\n",
            "==================================================\n",
            "\ud83d\udcca Fold weights: [0.20014797 0.19832756 0.19967052 0.20060856 0.20124539]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3748138340>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;31m# Ensemble sonu\u00e7lar\u0131\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m \u001b[0mensemble_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madvanced_ensemble_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;31m# Final summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3748138340>\u001b[0m in \u001b[0;36madvanced_ensemble_prediction\u001b[0;34m(models, val_predictions, val_labels)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;31m# 1. Simple Average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mpred_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0mavg_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mavg_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from scipy import stats\n",
        "\n",
        "print(\"\ud83d\ude80 CROSS-VALIDATION ENSEMBLE - 90%+ HEDEF\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf 5-Fold CV ile 5 farkl\u0131 model e\u011fitimi\")\n",
        "print(\"\ud83c\udfc6 Hedef: %90+ F1 Score\")\n",
        "print(\"\u26a1 Advanced ensemble teknikleri\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # A100 optimizasyonlar\u0131\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 GPU - ULTIMATE CV MODE!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.6, gamma=2.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Custom Trainer with Focal Loss\n",
        "class FocalLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = FocalLoss(alpha=0.6, gamma=2.5)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"\u2705 Dosya ba\u015far\u0131yla okundu!\")\n",
        "    print(f\"\ud83d\udcca Columns: {list(df.columns)}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Dosya okuma hatas\u0131: {e}\")\n",
        "    raise\n",
        "\n",
        "# Veri temizleme\n",
        "df.columns = df.columns.str.lower()\n",
        "print(f\"\ud83d\udcca Temizlenmi\u015f columns: {list(df.columns)}\")\n",
        "\n",
        "# Metin ve etiket sutunlar\u0131n\u0131 bul\n",
        "text_col = 'metin'\n",
        "label_col = 'etiket'\n",
        "\n",
        "if text_col not in df.columns or label_col not in df.columns:\n",
        "    print(f\"\u274c Gerekli sutunlar bulunamad\u0131!\")\n",
        "    print(f\"Mevcut sutunlar: {list(df.columns)}\")\n",
        "    raise ValueError(\"Metin ve etiket sutunlar\u0131 bulunamad\u0131\")\n",
        "\n",
        "df_clean = df.dropna(subset=[label_col]).copy()\n",
        "texts = df_clean[text_col].astype(str).tolist()\n",
        "labels = df_clean[label_col].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum ({time.time()-start_time:.1f}s)\")\n",
        "print(f\"\ud83d\udcca Toplam veri: {len(texts)}\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Faydas\u0131z: {len(labels)-np.sum(labels)} (%{(1-np.mean(labels))*100:.1f})\")\n",
        "\n",
        "# Tokenizer y\u00fckle\n",
        "print(f\"\\n\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "print(f\"\u2705 XLM-RoBERTa tokenizer y\u00fcklendi!\")\n",
        "\n",
        "def train_single_fold(fold_num, train_texts, train_labels, val_texts, val_labels, tokenizer):\n",
        "    \"\"\"Tek fold i\u00e7in model e\u011fitimi\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 FOLD {fold_num} BA\u015eLIYOR...\")\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "    print(f\"\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: {np.bincount(train_labels)}\")\n",
        "    print(f\"\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "    # Dataset olu\u015ftur\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, 256)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, 256)\n",
        "\n",
        "    # Fresh model y\u00fckle\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"xlm-roberta-base\",\n",
        "        num_labels=2,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Fold-specific training args\n",
        "    fold_training_args = TrainingArguments(\n",
        "        output_dir=f'./cv_fold_{fold_num}',\n",
        "        num_train_epochs=6,  # Art\u0131r\u0131ld\u0131\n",
        "        per_device_train_batch_size=24,\n",
        "        per_device_eval_batch_size=48,\n",
        "        gradient_accumulation_steps=2,  # Effective batch = 48\n",
        "        warmup_ratio=0.15,\n",
        "        learning_rate=1.5e-5,  # D\u00fc\u015f\u00fcr\u00fcld\u00fc\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.015,  # Art\u0131r\u0131ld\u0131\n",
        "        label_smoothing_factor=0.2,  # Art\u0131r\u0131ld\u0131\n",
        "        seed=42 + fold_num,  # Her fold farkl\u0131 seed\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"no\",  # Sadece training, validation sonra\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=False,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Trainer olu\u015ftur\n",
        "    trainer = FocalLossTrainer(\n",
        "        model=model,\n",
        "        args=fold_training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # E\u011fitim\n",
        "    fold_start = time.time()\n",
        "    trainer.train()\n",
        "    fold_time = time.time() - fold_start\n",
        "\n",
        "    # Validation prediction\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "    val_pred_probs = torch.softmax(torch.tensor(val_predictions.predictions), dim=1).numpy()\n",
        "    val_pred_labels = np.argmax(val_pred_probs, axis=1)\n",
        "\n",
        "    # Fold performance\n",
        "    fold_f1 = f1_score(val_labels, val_pred_labels, average='macro')\n",
        "    fold_acc = accuracy_score(val_labels, val_pred_labels)\n",
        "\n",
        "    print(f\"\u2705 FOLD {fold_num} TAMAMLANDI!\")\n",
        "    print(f\"\u23f0 S\u00fcre: {fold_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfaf F1: {fold_f1:.4f}\")\n",
        "    print(f\"\ud83c\udfaf Accuracy: {fold_acc:.4f}\")\n",
        "\n",
        "    return trainer.model, val_pred_probs, fold_f1, fold_acc\n",
        "\n",
        "def train_cv_ensemble(texts, labels, n_folds=5):\n",
        "    \"\"\"5-Fold Cross Validation Ensemble\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\ude80 {n_folds}-FOLD CROSS VALIDATION BA\u015eLIYOR...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    models = []\n",
        "    all_val_predictions = []\n",
        "    all_val_labels = []\n",
        "    fold_performances = []\n",
        "\n",
        "    cv_start_time = time.time()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(texts, labels)):\n",
        "        # Bu fold i\u00e7in veri haz\u0131rla\n",
        "        fold_train_texts = [texts[i] for i in train_idx]\n",
        "        fold_train_labels = [labels[i] for i in train_idx]\n",
        "        fold_val_texts = [texts[i] for i in val_idx]\n",
        "        fold_val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit\n",
        "        model, val_preds, fold_f1, fold_acc = train_single_fold(\n",
        "            fold + 1, fold_train_texts, fold_train_labels,\n",
        "            fold_val_texts, fold_val_labels, tokenizer\n",
        "        )\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 kaydet\n",
        "        models.append(model)\n",
        "        all_val_predictions.append(val_preds)\n",
        "        all_val_labels.extend(fold_val_labels)\n",
        "        fold_performances.append({'f1': fold_f1, 'acc': fold_acc})\n",
        "\n",
        "        # Memory temizl\u011fi\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    cv_time = time.time() - cv_start_time\n",
        "    print(f\"\\n\u2705 T\u00dcM FOLD'LAR TAMAMLANDI! ({cv_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Fold performanslar\u0131\n",
        "    print(f\"\\n\ud83d\udcca FOLD PERFORMANSLARI:\")\n",
        "    print(\"=\"*40)\n",
        "    for i, perf in enumerate(fold_performances):\n",
        "        print(f\"Fold {i+1}: F1={perf['f1']:.4f}, Acc={perf['acc']:.4f}\")\n",
        "\n",
        "    avg_f1 = np.mean([p['f1'] for p in fold_performances])\n",
        "    std_f1 = np.std([p['f1'] for p in fold_performances])\n",
        "    print(f\"\\n\ud83d\udcc8 ORTALAMA: F1={avg_f1:.4f} \u00b1 {std_f1:.4f}\")\n",
        "\n",
        "    return models, all_val_predictions, all_val_labels, fold_performances\n",
        "\n",
        "def advanced_ensemble_prediction(models, val_predictions, val_labels):\n",
        "    \"\"\"Geli\u015fmi\u015f ensemble y\u00f6ntemleri\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf ENSEMBLE COMBINATION TESTING...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # T\u00fcm validation predictions'lar\u0131 birle\u015ftir\n",
        "    all_preds = np.concatenate(val_predictions, axis=0)\n",
        "    all_labels = np.array(val_labels)\n",
        "\n",
        "    # Fold baz\u0131nda performanslar\u0131 hesapla\n",
        "    fold_weights = []\n",
        "    fold_start = 0\n",
        "\n",
        "    for i, val_pred in enumerate(val_predictions):\n",
        "        fold_end = fold_start + len(val_pred)\n",
        "        fold_labels = all_labels[fold_start:fold_end]\n",
        "        fold_pred_labels = np.argmax(val_pred, axis=1)\n",
        "        fold_f1 = f1_score(fold_labels, fold_pred_labels, average='macro')\n",
        "        fold_weights.append(fold_f1)\n",
        "        fold_start = fold_end\n",
        "\n",
        "    fold_weights = np.array(fold_weights)\n",
        "    fold_weights = fold_weights / np.sum(fold_weights)  # Normalize\n",
        "\n",
        "    print(f\"\ud83d\udcca Fold weights: {fold_weights}\")\n",
        "\n",
        "    # 1. Simple Average\n",
        "    pred_sets = np.array(val_predictions)\n",
        "    avg_predictions = np.mean(pred_sets, axis=0)\n",
        "    avg_pred_labels = np.argmax(avg_predictions, axis=1)\n",
        "\n",
        "    # Her fold i\u00e7in ayr\u0131 ayr\u0131 de\u011ferlendirme yerine\n",
        "    # T\u00fcm validation verisi \u00fczerinde de\u011ferlendirme\n",
        "    fold_start = 0\n",
        "    ensemble_preds = []\n",
        "    ensemble_labels = []\n",
        "\n",
        "    for i, val_pred in enumerate(val_predictions):\n",
        "        fold_end = fold_start + len(val_pred)\n",
        "        fold_labels = all_labels[fold_start:fold_end]\n",
        "\n",
        "        # Bu fold'un ensemble prediction'\u0131\n",
        "        weighted_pred = np.average(pred_sets[:, fold_start:fold_end], axis=0, weights=fold_weights)\n",
        "        ensemble_pred_labels = np.argmax(weighted_pred, axis=1)\n",
        "\n",
        "        ensemble_preds.extend(ensemble_pred_labels)\n",
        "        ensemble_labels.extend(fold_labels)\n",
        "        fold_start = fold_end\n",
        "\n",
        "    # Final ensemble performance\n",
        "    ensemble_f1 = f1_score(ensemble_labels, ensemble_preds, average='macro')\n",
        "    ensemble_acc = accuracy_score(ensemble_labels, ensemble_preds)\n",
        "    ensemble_precision = precision_recall_fscore_support(ensemble_labels, ensemble_preds, average='macro')[0]\n",
        "    ensemble_recall = precision_recall_fscore_support(ensemble_labels, ensemble_preds, average='macro')[1]\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 ENSEMBLE SONU\u00c7LARI:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\ud83c\udfaf F1 Score: {ensemble_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ensemble_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ensemble_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ensemble_recall:.4f}\")\n",
        "\n",
        "    # Hedef de\u011ferlendirmesi\n",
        "    if ensemble_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF ULA\u015eILDI! %90+ F1 SCORE!\")\n",
        "        achievement = \"LEGENDARY\"\n",
        "    elif ensemble_f1 >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! %89.5+ F1!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    else:\n",
        "        improvement = ensemble_f1 - 0.8967  # \u00d6nceki en iyi\n",
        "        print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.4f} F1\")\n",
        "        achievement = \"IMPROVED\"\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI RAPOR:\")\n",
        "    print(classification_report(ensemble_labels, ensemble_preds,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    return {\n",
        "        'f1': ensemble_f1,\n",
        "        'accuracy': ensemble_acc,\n",
        "        'precision': ensemble_precision,\n",
        "        'recall': ensemble_recall,\n",
        "        'achievement': achievement,\n",
        "        'models': models,\n",
        "        'predictions': ensemble_preds,\n",
        "        'labels': ensemble_labels\n",
        "    }\n",
        "\n",
        "# Ana execution\n",
        "print(f\"\\n\ud83d\ude80 CV ENSEMBLE EXECUTION BA\u015eLIYOR...\")\n",
        "\n",
        "# Cross-validation ensemble e\u011fitimi\n",
        "models, val_predictions, val_labels, fold_performances = train_cv_ensemble(texts, labels, n_folds=5)\n",
        "\n",
        "# Ensemble sonu\u00e7lar\u0131\n",
        "ensemble_results = advanced_ensemble_prediction(models, val_predictions, val_labels)\n",
        "\n",
        "# Final summary\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n\ud83d\udcda CV ENSEMBLE \u00d6ZET\u0130:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\u2022 Veri: {len(texts):,} yorumlar\")\n",
        "print(f\"\u2022 CV Folds: 5\")\n",
        "print(f\"\u2022 Model: XLM-RoBERTa + Focal Loss\")\n",
        "print(f\"\u2022 F1 Score: {ensemble_results['f1']:.4f}\")\n",
        "print(f\"\u2022 Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
        "print(f\"\u2022 Achievement: {ensemble_results['achievement']}\")\n",
        "print(f\"\u2022 Total Time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "# Model kaydetme\n",
        "print(f\"\\n\ud83d\udcbe EN \u0130Y\u0130 MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "best_model_idx = np.argmax([p['f1'] for p in fold_performances])\n",
        "best_model = models[best_model_idx]\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/cv_ensemble_best_model\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "best_model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"\u2705 En iyi model kaydedildi: {save_path}\")\n",
        "\n",
        "# Test prediction\n",
        "print(f\"\\n\ud83e\uddea \u00d6RNEK TEST:\")\n",
        "test_text = \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = best_model(**inputs)\n",
        "    prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "    confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "print(f\"Metin: '{test_text}'\")\n",
        "print(f\"Tahmin: {result} (G\u00fcven: %{confidence*100:.1f})\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a CV ENSEMBLE TAMAMLANDI!\")\n",
        "\n",
        "# Memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1252658,
          "status": "ok",
          "timestamp": 1750092110727,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "hG24iMzEiy8p",
        "outputId": "ab77d5a5-457d-416b-dc3a-97bab259fefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 CROSS-VALIDATION ENSEMBLE - 90%+ HEDEF\n",
            "============================================================\n",
            "\ud83c\udfaf 5-Fold CV ile 5 farkl\u0131 model e\u011fitimi\n",
            "\ud83c\udfc6 Hedef: %90+ F1 Score\n",
            "\u26a1 Advanced ensemble teknikleri\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcbe GPU Memory: 42.0 GB\n",
            "\u26a1 A100 GPU - ULTIMATE CV MODE!\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Dosya ba\u015far\u0131yla okundu!\n",
            "\ud83d\udcca Columns: ['metin', 'tahmin', 'etiket']\n",
            "\ud83d\udcca Temizlenmi\u015f columns: ['metin', 'tahmin', 'etiket']\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum (0.9s)\n",
            "\ud83d\udcca Toplam veri: 15167\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Faydas\u0131z: 6686 (%44.1)\n",
            "\n",
            "\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 XLM-RoBERTa tokenizer y\u00fcklendi!\n",
            "\n",
            "\ud83d\ude80 CV ENSEMBLE EXECUTION BA\u015eLIYOR...\n",
            "\n",
            "\ud83d\ude80 5-FOLD CROSS VALIDATION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83d\udd04 FOLD 1 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12133, Val: 3034\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5348 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1338 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:04, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.079300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.035700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.034800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.031600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.028900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.028000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.026800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.023300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.020700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 1 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8886\n",
            "\ud83c\udfaf Accuracy: 0.8906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd04 FOLD 2 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12133, Val: 3034\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6784]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1697]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:05, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.070300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.047000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.041600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 2 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8831\n",
            "\ud83c\udfaf Accuracy: 0.8853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd04 FOLD 3 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:02, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.068400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.038700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.037500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.035600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.030100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.021000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 3 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.0 dakika\n",
            "\ud83c\udfaf F1: 0.8890\n",
            "\ud83c\udfaf Accuracy: 0.8912\n",
            "\n",
            "\ud83d\udd04 FOLD 4 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:02, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.068100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.040400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.035000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.027900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.020700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.020900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 4 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8932\n",
            "\ud83c\udfaf Accuracy: 0.8955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd04 FOLD 5 BA\u015eLIYOR...\n",
            "\ud83d\udcca Train: 12134, Val: 3033\n",
            "\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: [5349 6785]\n",
            "\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: [1337 1696]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1518' max='1518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1518/1518 04:04, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.037900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.029100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.024800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.022400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.018800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 FOLD 5 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.1 dakika\n",
            "\ud83c\udfaf F1: 0.8961\n",
            "\ud83c\udfaf Accuracy: 0.8981\n",
            "\n",
            "\u2705 T\u00dcM FOLD'LAR TAMAMLANDI! (20.6 dakika)\n",
            "\n",
            "\ud83d\udcca FOLD PERFORMANSLARI:\n",
            "========================================\n",
            "Fold 1: F1=0.8886, Acc=0.8906\n",
            "Fold 2: F1=0.8831, Acc=0.8853\n",
            "Fold 3: F1=0.8890, Acc=0.8912\n",
            "Fold 4: F1=0.8932, Acc=0.8955\n",
            "Fold 5: F1=0.8961, Acc=0.8981\n",
            "\n",
            "\ud83d\udcc8 ORTALAMA: F1=0.8900 \u00b1 0.0044\n",
            "\n",
            "\ud83c\udfaf ENSEMBLE COMBINATION TESTING...\n",
            "==================================================\n",
            "\ud83d\udcca Fold prediction shapes:\n",
            "  Fold 1: (3034, 2)\n",
            "  Fold 2: (3034, 2)\n",
            "  Fold 3: (3033, 2)\n",
            "  Fold 4: (3033, 2)\n",
            "  Fold 5: (3033, 2)\n",
            "\ud83d\udcca Combined predictions shape: (15167, 2)\n",
            "\ud83d\udcca Combined labels shape: (15167,)\n",
            "\ud83d\udcca Fold weights: [0.19969384 0.19844016 0.19978389 0.20072246 0.20135965]\n",
            "\ud83c\udfaf Simple ensemble F1: 0.8900\n",
            "\ud83c\udfaf Simple ensemble Acc: 0.8921\n",
            "\ud83c\udfaf Weighted ensemble F1: 0.8900\n",
            "\ud83c\udfaf Weighted ensemble Acc: 0.8921\n",
            "\n",
            "\ud83c\udfc6 ENSEMBLE SONU\u00c7LARI (Simple Method):\n",
            "========================================\n",
            "\ud83c\udfaf F1 Score: 0.8900\n",
            "\ud83d\udcca Accuracy: 0.8921\n",
            "\ud83d\udcc8 Precision: 0.8927\n",
            "\ud83d\udcc8 Recall: 0.8881\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.0067 F1\n",
            "\n",
            "\ud83d\udccb DETAYLI RAPOR:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.90      0.85      0.87      6686\n",
            "     Faydal\u0131       0.89      0.92      0.91      8481\n",
            "\n",
            "    accuracy                           0.89     15167\n",
            "   macro avg       0.89      0.89      0.89     15167\n",
            "weighted avg       0.89      0.89      0.89     15167\n",
            "\n",
            "\n",
            "\ud83d\udcda CV ENSEMBLE \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Veri: 15,167 yorumlar\n",
            "\u2022 CV Folds: 5\n",
            "\u2022 Model: XLM-RoBERTa + Focal Loss\n",
            "\u2022 F1 Score: 0.8900\n",
            "\u2022 Accuracy: 0.8921\n",
            "\u2022 Achievement: IMPROVED\n",
            "\u2022 Total Time: 20.6 dakika\n",
            "\n",
            "\ud83d\udcbe EN \u0130Y\u0130 MODEL KAYDED\u0130L\u0130YOR...\n",
            "\u2705 En iyi model kaydedildi: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/cv_ensemble_best_model\n",
            "\n",
            "\ud83e\uddea \u00d6RNEK TEST:\n",
            "Metin: '\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil'\n",
            "Tahmin: Faydal\u0131 (G\u00fcven: %86.3)\n",
            "\n",
            "\ud83c\udf8a CV ENSEMBLE TAMAMLANDI!\n",
            "\ud83d\udcbe Memory temizlendi!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from scipy import stats\n",
        "\n",
        "print(\"\ud83d\ude80 CROSS-VALIDATION ENSEMBLE - 90%+ HEDEF\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf 5-Fold CV ile 5 farkl\u0131 model e\u011fitimi\")\n",
        "print(\"\ud83c\udfc6 Hedef: %90+ F1 Score\")\n",
        "print(\"\u26a1 Advanced ensemble teknikleri\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1e9\n",
        "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "    # A100 optimizasyonlar\u0131\n",
        "    if \"A100\" in torch.cuda.get_device_name(0):\n",
        "        print(\"\u26a1 A100 GPU - ULTIMATE CV MODE!\")\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Focal Loss Implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.6, gamma=2.5):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Custom Trainer with Focal Loss\n",
        "class FocalLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = FocalLoss(alpha=0.6, gamma=2.5)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "start_time = time.time()\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(f\"\u2705 Dosya ba\u015far\u0131yla okundu!\")\n",
        "    print(f\"\ud83d\udcca Columns: {list(df.columns)}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Dosya okuma hatas\u0131: {e}\")\n",
        "    raise\n",
        "\n",
        "# Veri temizleme\n",
        "df.columns = df.columns.str.lower()\n",
        "print(f\"\ud83d\udcca Temizlenmi\u015f columns: {list(df.columns)}\")\n",
        "\n",
        "# Metin ve etiket sutunlar\u0131n\u0131 bul\n",
        "text_col = 'metin'\n",
        "label_col = 'etiket'\n",
        "\n",
        "if text_col not in df.columns or label_col not in df.columns:\n",
        "    print(f\"\u274c Gerekli sutunlar bulunamad\u0131!\")\n",
        "    print(f\"Mevcut sutunlar: {list(df.columns)}\")\n",
        "    raise ValueError(\"Metin ve etiket sutunlar\u0131 bulunamad\u0131\")\n",
        "\n",
        "df_clean = df.dropna(subset=[label_col]).copy()\n",
        "texts = df_clean[text_col].astype(str).tolist()\n",
        "labels = df_clean[label_col].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum ({time.time()-start_time:.1f}s)\")\n",
        "print(f\"\ud83d\udcca Toplam veri: {len(texts)}\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "print(f\"\ud83d\udcca Faydas\u0131z: {len(labels)-np.sum(labels)} (%{(1-np.mean(labels))*100:.1f})\")\n",
        "\n",
        "# Tokenizer y\u00fckle\n",
        "print(f\"\\n\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "print(f\"\u2705 XLM-RoBERTa tokenizer y\u00fcklendi!\")\n",
        "\n",
        "def train_single_fold(fold_num, train_texts, train_labels, val_texts, val_labels, tokenizer):\n",
        "    \"\"\"Tek fold i\u00e7in model e\u011fitimi\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 FOLD {fold_num} BA\u015eLIYOR...\")\n",
        "    print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "    print(f\"\ud83d\udcca Train da\u011f\u0131l\u0131m\u0131: {np.bincount(train_labels)}\")\n",
        "    print(f\"\ud83d\udcca Val da\u011f\u0131l\u0131m\u0131: {np.bincount(val_labels)}\")\n",
        "\n",
        "    # Dataset olu\u015ftur\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, 256)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, 256)\n",
        "\n",
        "    # Fresh model y\u00fckle\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"xlm-roberta-base\",\n",
        "        num_labels=2,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Fold-specific training args\n",
        "    fold_training_args = TrainingArguments(\n",
        "        output_dir=f'./cv_fold_{fold_num}',\n",
        "        num_train_epochs=6,  # Art\u0131r\u0131ld\u0131\n",
        "        per_device_train_batch_size=24,\n",
        "        per_device_eval_batch_size=48,\n",
        "        gradient_accumulation_steps=2,  # Effective batch = 48\n",
        "        warmup_ratio=0.15,\n",
        "        learning_rate=1.5e-5,  # D\u00fc\u015f\u00fcr\u00fcld\u00fc\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.015,  # Art\u0131r\u0131ld\u0131\n",
        "        label_smoothing_factor=0.2,  # Art\u0131r\u0131ld\u0131\n",
        "        seed=42 + fold_num,  # Her fold farkl\u0131 seed\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"no\",  # Sadece training, validation sonra\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=False,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Trainer olu\u015ftur\n",
        "    trainer = FocalLossTrainer(\n",
        "        model=model,\n",
        "        args=fold_training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # E\u011fitim\n",
        "    fold_start = time.time()\n",
        "    trainer.train()\n",
        "    fold_time = time.time() - fold_start\n",
        "\n",
        "    # Validation prediction\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "    val_pred_probs = torch.softmax(torch.tensor(val_predictions.predictions), dim=1).numpy()\n",
        "    val_pred_labels = np.argmax(val_pred_probs, axis=1)\n",
        "\n",
        "    # Fold performance\n",
        "    fold_f1 = f1_score(val_labels, val_pred_labels, average='macro')\n",
        "    fold_acc = accuracy_score(val_labels, val_pred_labels)\n",
        "\n",
        "    print(f\"\u2705 FOLD {fold_num} TAMAMLANDI!\")\n",
        "    print(f\"\u23f0 S\u00fcre: {fold_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfaf F1: {fold_f1:.4f}\")\n",
        "    print(f\"\ud83c\udfaf Accuracy: {fold_acc:.4f}\")\n",
        "\n",
        "    return trainer.model, val_pred_probs, fold_f1, fold_acc\n",
        "\n",
        "def train_cv_ensemble(texts, labels, n_folds=5):\n",
        "    \"\"\"5-Fold Cross Validation Ensemble\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\ude80 {n_folds}-FOLD CROSS VALIDATION BA\u015eLIYOR...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    models = []\n",
        "    all_val_predictions = []\n",
        "    all_val_labels = []\n",
        "    fold_performances = []\n",
        "\n",
        "    cv_start_time = time.time()\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(texts, labels)):\n",
        "        # Bu fold i\u00e7in veri haz\u0131rla\n",
        "        fold_train_texts = [texts[i] for i in train_idx]\n",
        "        fold_train_labels = [labels[i] for i in train_idx]\n",
        "        fold_val_texts = [texts[i] for i in val_idx]\n",
        "        fold_val_labels = [labels[i] for i in val_idx]\n",
        "\n",
        "        # Model e\u011fit\n",
        "        model, val_preds, fold_f1, fold_acc = train_single_fold(\n",
        "            fold + 1, fold_train_texts, fold_train_labels,\n",
        "            fold_val_texts, fold_val_labels, tokenizer\n",
        "        )\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 kaydet\n",
        "        models.append(model)\n",
        "        all_val_predictions.append(val_preds)\n",
        "        all_val_labels.extend(fold_val_labels)\n",
        "        fold_performances.append({'f1': fold_f1, 'acc': fold_acc})\n",
        "\n",
        "        # Memory temizl\u011fi\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    cv_time = time.time() - cv_start_time\n",
        "    print(f\"\\n\u2705 T\u00dcM FOLD'LAR TAMAMLANDI! ({cv_time/60:.1f} dakika)\")\n",
        "\n",
        "    # Fold performanslar\u0131\n",
        "    print(f\"\\n\ud83d\udcca FOLD PERFORMANSLARI:\")\n",
        "    print(\"=\"*40)\n",
        "    for i, perf in enumerate(fold_performances):\n",
        "        print(f\"Fold {i+1}: F1={perf['f1']:.4f}, Acc={perf['acc']:.4f}\")\n",
        "\n",
        "    avg_f1 = np.mean([p['f1'] for p in fold_performances])\n",
        "    std_f1 = np.std([p['f1'] for p in fold_performances])\n",
        "    print(f\"\\n\ud83d\udcc8 ORTALAMA: F1={avg_f1:.4f} \u00b1 {std_f1:.4f}\")\n",
        "\n",
        "    return models, all_val_predictions, all_val_labels, fold_performances\n",
        "\n",
        "def advanced_ensemble_prediction(models, val_predictions, val_labels):\n",
        "    \"\"\"Geli\u015fmi\u015f ensemble y\u00f6ntemleri\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf ENSEMBLE COMBINATION TESTING...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Her fold'un prediction shape'ini kontrol et\n",
        "    print(f\"\ud83d\udcca Fold prediction shapes:\")\n",
        "    for i, pred in enumerate(val_predictions):\n",
        "        print(f\"  Fold {i+1}: {pred.shape}\")\n",
        "\n",
        "    # T\u00fcm validation predictions'lar\u0131 birle\u015ftir\n",
        "    all_preds = np.concatenate(val_predictions, axis=0)\n",
        "    all_labels = np.array(val_labels)\n",
        "\n",
        "    print(f\"\ud83d\udcca Combined predictions shape: {all_preds.shape}\")\n",
        "    print(f\"\ud83d\udcca Combined labels shape: {all_labels.shape}\")\n",
        "\n",
        "    # Fold baz\u0131nda performanslar\u0131 hesapla (weights i\u00e7in)\n",
        "    fold_weights = []\n",
        "    fold_start = 0\n",
        "\n",
        "    for i, val_pred in enumerate(val_predictions):\n",
        "        fold_end = fold_start + len(val_pred)\n",
        "        fold_labels = all_labels[fold_start:fold_end]\n",
        "        fold_pred_labels = np.argmax(val_pred, axis=1)\n",
        "        fold_f1 = f1_score(fold_labels, fold_pred_labels, average='macro')\n",
        "        fold_weights.append(fold_f1)\n",
        "        fold_start = fold_end\n",
        "\n",
        "    fold_weights = np.array(fold_weights)\n",
        "    fold_weights = fold_weights / np.sum(fold_weights)  # Normalize\n",
        "\n",
        "    print(f\"\ud83d\udcca Fold weights: {fold_weights}\")\n",
        "\n",
        "    # Ensemble method 1: Simple Average t\u00fcm predictions \u00fczerinde\n",
        "    all_pred_labels = np.argmax(all_preds, axis=1)\n",
        "    simple_f1 = f1_score(all_labels, all_pred_labels, average='macro')\n",
        "    simple_acc = accuracy_score(all_labels, all_pred_labels)\n",
        "\n",
        "    print(f\"\ud83c\udfaf Simple ensemble F1: {simple_f1:.4f}\")\n",
        "    print(f\"\ud83c\udfaf Simple ensemble Acc: {simple_acc:.4f}\")\n",
        "\n",
        "    # Ensemble method 2: Weighted average per fold\n",
        "    fold_start = 0\n",
        "    weighted_preds = []\n",
        "    weighted_labels = []\n",
        "\n",
        "    for i, val_pred in enumerate(val_predictions):\n",
        "        fold_size = len(val_pred)\n",
        "        fold_end = fold_start + fold_size\n",
        "        fold_labels = all_labels[fold_start:fold_end]\n",
        "\n",
        "        # Bu fold i\u00e7in weighted prediction hesapla\n",
        "        fold_weighted_preds = []\n",
        "        for j, other_pred in enumerate(val_predictions):\n",
        "            if i != j:  # Kendi fold'unu exclude et\n",
        "                # Di\u011fer fold'lar\u0131n ayn\u0131 indeksteki tahminlerini al\n",
        "                start_idx = fold_start if j < i else fold_start - len(val_predictions[j])\n",
        "                end_idx = start_idx + fold_size\n",
        "                if start_idx >= 0 and end_idx <= len(other_pred):\n",
        "                    fold_weighted_preds.append(other_pred[start_idx:end_idx] * fold_weights[j])\n",
        "\n",
        "        if fold_weighted_preds:\n",
        "            fold_ensemble = np.mean(fold_weighted_preds, axis=0)\n",
        "            fold_pred_labels = np.argmax(fold_ensemble, axis=1)\n",
        "        else:\n",
        "            fold_pred_labels = np.argmax(val_pred, axis=1)\n",
        "\n",
        "        weighted_preds.extend(fold_pred_labels)\n",
        "        weighted_labels.extend(fold_labels)\n",
        "        fold_start = fold_end\n",
        "\n",
        "    # Weighted ensemble performance hesapla\n",
        "    if len(weighted_preds) == len(all_labels):\n",
        "        weighted_f1 = f1_score(all_labels, weighted_preds, average='macro')\n",
        "        weighted_acc = accuracy_score(all_labels, weighted_preds)\n",
        "        print(f\"\ud83c\udfaf Weighted ensemble F1: {weighted_f1:.4f}\")\n",
        "        print(f\"\ud83c\udfaf Weighted ensemble Acc: {weighted_acc:.4f}\")\n",
        "\n",
        "        # En iyi y\u00f6ntemi se\u00e7\n",
        "        if weighted_f1 > simple_f1:\n",
        "            final_f1 = weighted_f1\n",
        "            final_acc = weighted_acc\n",
        "            final_preds = weighted_preds\n",
        "            method = \"Weighted\"\n",
        "        else:\n",
        "            final_f1 = simple_f1\n",
        "            final_acc = simple_acc\n",
        "            final_preds = all_pred_labels\n",
        "            method = \"Simple\"\n",
        "    else:\n",
        "        # Weighted method ba\u015far\u0131s\u0131z, simple kullan\n",
        "        final_f1 = simple_f1\n",
        "        final_acc = simple_acc\n",
        "        final_preds = all_pred_labels\n",
        "        method = \"Simple\"\n",
        "\n",
        "    # Final ensemble precision/recall\n",
        "    ensemble_precision = precision_recall_fscore_support(all_labels, final_preds, average='macro')[0]\n",
        "    ensemble_recall = precision_recall_fscore_support(all_labels, final_preds, average='macro')[1]\n",
        "\n",
        "\n",
        "    print(f\"\\n\ud83c\udfc6 ENSEMBLE SONU\u00c7LARI ({method} Method):\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\ud83c\udfaf F1 Score: {final_f1:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {final_acc:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ensemble_precision:.4f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ensemble_recall:.4f}\")\n",
        "\n",
        "    # Hedef de\u011ferlendirmesi\n",
        "    if final_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF ULA\u015eILDI! %90+ F1 SCORE!\")\n",
        "        achievement = \"LEGENDARY\"\n",
        "    elif final_f1 >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! %89.5+ F1!\")\n",
        "        achievement = \"EXCELLENT\"\n",
        "    else:\n",
        "        improvement = final_f1 - 0.8967  # \u00d6nceki en iyi\n",
        "        print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.4f} F1\")\n",
        "        achievement = \"IMPROVED\"\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI RAPOR:\")\n",
        "    print(classification_report(all_labels, final_preds,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    return {\n",
        "        'f1': final_f1,\n",
        "        'accuracy': final_acc,\n",
        "        'precision': ensemble_precision,\n",
        "        'recall': ensemble_recall,\n",
        "        'achievement': achievement,\n",
        "        'models': models,\n",
        "        'predictions': final_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "# Ana execution\n",
        "print(f\"\\n\ud83d\ude80 CV ENSEMBLE EXECUTION BA\u015eLIYOR...\")\n",
        "\n",
        "# Cross-validation ensemble e\u011fitimi\n",
        "models, val_predictions, val_labels, fold_performances = train_cv_ensemble(texts, labels, n_folds=5)\n",
        "\n",
        "# Ensemble sonu\u00e7lar\u0131\n",
        "ensemble_results = advanced_ensemble_prediction(models, val_predictions, val_labels)\n",
        "\n",
        "# Final summary\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n\ud83d\udcda CV ENSEMBLE \u00d6ZET\u0130:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\u2022 Veri: {len(texts):,} yorumlar\")\n",
        "print(f\"\u2022 CV Folds: 5\")\n",
        "print(f\"\u2022 Model: XLM-RoBERTa + Focal Loss\")\n",
        "print(f\"\u2022 F1 Score: {ensemble_results['f1']:.4f}\")\n",
        "print(f\"\u2022 Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
        "print(f\"\u2022 Achievement: {ensemble_results['achievement']}\")\n",
        "print(f\"\u2022 Total Time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "# Model kaydetme\n",
        "print(f\"\\n\ud83d\udcbe EN \u0130Y\u0130 MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "best_model_idx = np.argmax([p['f1'] for p in fold_performances])\n",
        "best_model = models[best_model_idx]\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/cv_ensemble_best_model\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "best_model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"\u2705 En iyi model kaydedildi: {save_path}\")\n",
        "\n",
        "# Test prediction\n",
        "print(f\"\\n\ud83e\uddea \u00d6RNEK TEST:\")\n",
        "test_text = \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\"\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = best_model(**inputs)\n",
        "    prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "    predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "    confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "print(f\"Metin: '{test_text}'\")\n",
        "print(f\"Tahmin: {result} (G\u00fcven: %{confidence*100:.1f})\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a CV ENSEMBLE TAMAMLANDI!\")\n",
        "\n",
        "# Memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3f802d0e1df34186900406bc0a8885b0",
            "f71aa785072148268187ffdefa5ef095",
            "e31882d80c4848a1a0f0d33f74820e6b",
            "66822481a8c6408f98b807d1191b21a1",
            "6ea1489f5f1042f2be4d706664c7ffa0",
            "23aff4e85a704733a3025b5051f8eed0",
            "f222c24d3cbc4a25b7ebaf2736f04030",
            "d119645d398a426a93f0ab429faa991c",
            "a32afa350714453c9416c22841aaf35a",
            "bc266af4670145f2a003316222f4c53f",
            "bb7660afee0243cf8017736dbf415472",
            "8e38716805c849f1b88dc70e8b049302",
            "d02005e61317430bafd64dad1ff066d0",
            "957d07782ed94980ab46095890d0bea7",
            "60b8d6fc17fb477087ce8d1ca07d15c4",
            "4eac006d9d47454fa4de1938b2d1cd3e",
            "6dc6115d42094b8badd081a5e0474c86",
            "07e6ddbf46b14427930f3c1f87e644a8",
            "97de3a6a5d534604a5bf6711c90c30f1",
            "7db19cc37b2e4b60aa8d2ffb200c0654",
            "6e2b4252f3024cee924efd2db09a1927",
            "b4ca75e84bb049b1b1973050a0ee37da",
            "1f04c36c67464e8ba3634990024e01d3",
            "c7023c35e446401b8df09570813b4290",
            "0491b9afd1a54836add265ca57788e7e",
            "de1b7cb25084421d841a849524aa3680",
            "bd4655637a2f4b61aa10f9fb121947d1",
            "c2b7d5f9058f4a9480fb891320ac6e15",
            "c9adcaade44e4b2987e612e7f4e66891",
            "14b368091fcb4412bb4b3de48bb3bb6f",
            "172872ed040e403bbd8b3bd26a2a0db3",
            "bb51a15ba086443c85476d567177ef2d",
            "0d558dde22f346e8a8d54b7b690137fe",
            "22c0ce1c0ff2459a93824c92e6dfc7cc",
            "d0ad8b3079484607a48f392dc83cd0a4",
            "225b2be7138b42b8b83e0d62aaac8bcf",
            "31cac83c0ed84ca19a54667a14cca575",
            "137c7907f35a4965ae572deb8ec47adb",
            "fb98115aed724fe587cf38534b67c775",
            "c618d4a1cbbb4271a8e4152f72f692b1",
            "ac50caf8240e453aa5491e67eff171fc",
            "9739b73d7e0c4de2ba6be95c026f300d",
            "19c897e045f84f6da23fd3c1c96caa47",
            "ce6f113824bc4db5b35c0fe76583c011",
            "b39427beeb29442299b26f0012a503f8",
            "4959bfdcb61f4fccbef9c70594165ad2",
            "657a5c197b274201b18453229a249e93",
            "d3bb10d540384134acc06365ea109977",
            "6bdddf87e556459295c2c1aa3f528489",
            "d725619c35aa4bac968d3e17e4cf1d3e",
            "2ddaf4526c8a4ff896e8e7809713df1c",
            "e5a4355f26784dd9af21dac5fd918c00",
            "3012c3860b664aa7b9fe0a3c299876ae",
            "e330bc3263154877b0e2f6552e549870",
            "aaf465aca60c43c0b5194eff427d5d3a"
          ]
        },
        "executionInfo": {
          "elapsed": 1215237,
          "status": "error",
          "timestamp": 1750100213113,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "of6PxKD9BtL2",
        "outputId": "55ec45a1-e6fc-46d9-9458-cdcb17578cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 QUICK BOOST STRATEGY - 90%+ FINAL PUSH\n",
            "============================================================\n",
            "\ud83c\udfaf En iyi fold modelini ultra fine-tune\n",
            "\ud83c\udfc6 Hedef: 89.61% \u2192 90.2%+ F1 Score\n",
            "\u26a1 S\u00fcre: ~30 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca Da\u011f\u0131l\u0131m: Faydal\u0131 8481 (%55.9)\n",
            "\ud83d\udcca Train: 13650, Val: 1517\n",
            "\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f802d0e1df34186900406bc0a8885b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e38716805c849f1b88dc70e8b049302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f04c36c67464e8ba3634990024e01d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22c0ce1c0ff2459a93824c92e6dfc7cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Tokenizer haz\u0131r!\n",
            "\n",
            "\ud83d\ude80 ULTRA BOOST EXECUTION BA\u015eLIYOR...\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd25 ULTRA FINE-TUNE MODEL v1 (Seed: 111)\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b39427beeb29442299b26f0012a503f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 04:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>0.020536</td>\n",
              "      <td>0.873434</td>\n",
              "      <td>0.871127</td>\n",
              "      <td>0.872957</td>\n",
              "      <td>0.869754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.021700</td>\n",
              "      <td>0.021678</td>\n",
              "      <td>0.875412</td>\n",
              "      <td>0.871760</td>\n",
              "      <td>0.880811</td>\n",
              "      <td>0.867579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.018473</td>\n",
              "      <td>0.877390</td>\n",
              "      <td>0.874427</td>\n",
              "      <td>0.879823</td>\n",
              "      <td>0.871399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016200</td>\n",
              "      <td>0.019780</td>\n",
              "      <td>0.884641</td>\n",
              "      <td>0.882343</td>\n",
              "      <td>0.885159</td>\n",
              "      <td>0.880409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.016400</td>\n",
              "      <td>0.020665</td>\n",
              "      <td>0.887937</td>\n",
              "      <td>0.885768</td>\n",
              "      <td>0.888279</td>\n",
              "      <td>0.883988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL v1 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.9 dakika\n",
            "\ud83c\udfaf F1: 0.8858\n",
            "\ud83d\udcca Accuracy: 0.8879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83d\udd25 ULTRA FINE-TUNE MODEL v2 (Seed: 222)\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 04:48, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.024800</td>\n",
              "      <td>0.021307</td>\n",
              "      <td>0.862887</td>\n",
              "      <td>0.859969</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.857796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.020700</td>\n",
              "      <td>0.019953</td>\n",
              "      <td>0.874753</td>\n",
              "      <td>0.871052</td>\n",
              "      <td>0.880268</td>\n",
              "      <td>0.866832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>0.018198</td>\n",
              "      <td>0.876730</td>\n",
              "      <td>0.875641</td>\n",
              "      <td>0.874441</td>\n",
              "      <td>0.877908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016200</td>\n",
              "      <td>0.020854</td>\n",
              "      <td>0.880026</td>\n",
              "      <td>0.877614</td>\n",
              "      <td>0.880508</td>\n",
              "      <td>0.875650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.020641</td>\n",
              "      <td>0.879367</td>\n",
              "      <td>0.877101</td>\n",
              "      <td>0.879252</td>\n",
              "      <td>0.875534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL v2 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.8 dakika\n",
            "\ud83c\udfaf F1: 0.8776\n",
            "\ud83d\udcca Accuracy: 0.8800\n",
            "\n",
            "\ud83d\udd25 ULTRA FINE-TUNE MODEL v3 (Seed: 333)\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 04:52, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.026300</td>\n",
              "      <td>0.023841</td>\n",
              "      <td>0.843771</td>\n",
              "      <td>0.836303</td>\n",
              "      <td>0.859934</td>\n",
              "      <td>0.830127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.021600</td>\n",
              "      <td>0.019535</td>\n",
              "      <td>0.873434</td>\n",
              "      <td>0.872825</td>\n",
              "      <td>0.872384</td>\n",
              "      <td>0.877485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>0.019815</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.882673</td>\n",
              "      <td>0.887333</td>\n",
              "      <td>0.879894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>0.019567</td>\n",
              "      <td>0.884641</td>\n",
              "      <td>0.882162</td>\n",
              "      <td>0.885914</td>\n",
              "      <td>0.879778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.014900</td>\n",
              "      <td>0.020540</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.875758</td>\n",
              "      <td>0.877902</td>\n",
              "      <td>0.874197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL v3 TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 4.9 dakika\n",
            "\ud83c\udfaf F1: 0.8827\n",
            "\ud83d\udcca Accuracy: 0.8853\n",
            "\n",
            "\ud83c\udfaf ULTRA ENSEMBLE COMBINATION...\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [190/190 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:\n"
          ]
        },
        {
          "ename": "Abort",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-185628711>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;31m# Ultra ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m \u001b[0mensemble_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0multra_ensemble_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;31m# SONU\u00c7LAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-185628711>\u001b[0m in \u001b[0;36multra_ensemble_prediction\u001b[0;34m(models_info, val_texts, val_labels)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4266\u001b[0m         )\n\u001b[1;32m   4267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4268\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4269\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_memory_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_and_update_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_predict\u001b[0;34m(self, args, state, control, metrics)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_predict\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_predict\u001b[0;34m(self, args, state, control, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_world_process_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewrite_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    859\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should never get here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         wandb_login._login(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    234\u001b[0m     ) -> Tuple[Optional[str], ApiKeyStatus]:\n\u001b[1;32m    235\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    213\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local, referrer)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 )\n\u001b[1;32m    191\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_api_key_prompt_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOGIN_CHOICE_NOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# TODO: Needs refactor as this needs to be handled by caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "print(\"\ud83d\ude80 QUICK BOOST STRATEGY - 90%+ FINAL PUSH\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf En iyi fold modelini ultra fine-tune\")\n",
        "print(\"\ud83c\udfc6 Hedef: 89.61% \u2192 90.2%+ F1 Score\")\n",
        "print(\"\u26a1 S\u00fcre: ~30 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Ultra Focal Loss (daha agresif)\n",
        "class UltraFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.65, gamma=3.5):  # Daha agresif\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Dynamic weighting\n",
        "        difficulty = 1 - pt\n",
        "        focal_loss = self.alpha * (difficulty ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Ultra Trainer\n",
        "class UltraTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = UltraFocalLoss(alpha=0.65, gamma=3.5)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme (quick load)\n",
        "print(\"\ud83d\udcca VER\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca Da\u011f\u0131l\u0131m: Faydal\u0131 {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "\n",
        "# Train/val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "\n",
        "# Tokenizer\n",
        "print(\"\ud83e\udd16 TOKENIZER Y\u00dcKLEN\u0130YOR...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "print(\"\u2705 Tokenizer haz\u0131r!\")\n",
        "\n",
        "def ultra_fine_tune_model(seed, model_name_suffix):\n",
        "    \"\"\"Ultra fine-tuning with specific seed\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd25 ULTRA FINE-TUNE MODEL {model_name_suffix} (Seed: {seed})\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Fresh model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"xlm-roberta-base\",\n",
        "        num_labels=2,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Dataset\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, 256)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, 256)\n",
        "\n",
        "    # Ultra aggressive training args\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./ultra_boost_{model_name_suffix}',\n",
        "        num_train_epochs=5,  # Optimal epoch count\n",
        "        per_device_train_batch_size=20,  # Slightly smaller for stability\n",
        "        per_device_eval_batch_size=40,\n",
        "        gradient_accumulation_steps=2,   # Effective batch = 40\n",
        "        warmup_ratio=0.25,              # Longer warmup for stability\n",
        "        learning_rate=8e-6,             # Very conservative\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.02,              # Higher regularization\n",
        "        label_smoothing_factor=0.35,    # High label smoothing\n",
        "        seed=seed,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        # Advanced optimizations\n",
        "        gradient_checkpointing=True,     # Memory efficient\n",
        "        adam_epsilon=1e-8,\n",
        "        max_grad_norm=0.5,              # Gradient clipping\n",
        "    )\n",
        "\n",
        "    # Ultra trainer\n",
        "    trainer = UltraTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Final evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    f1_score_result = eval_results['eval_f1']\n",
        "    accuracy_result = eval_results['eval_accuracy']\n",
        "\n",
        "    print(f\"\u2705 MODEL {model_name_suffix} TAMAMLANDI!\")\n",
        "    print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfaf F1: {f1_score_result:.4f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {accuracy_result:.4f}\")\n",
        "\n",
        "    # Model kaydet\n",
        "    save_path = f\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ultra_boost_model_{model_name_suffix}\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model.save_pretrained(save_path)\n",
        "\n",
        "    # Memory cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'model': trainer.model,\n",
        "        'f1': f1_score_result,\n",
        "        'accuracy': accuracy_result,\n",
        "        'trainer': trainer,\n",
        "        'eval_results': eval_results\n",
        "    }\n",
        "\n",
        "def ultra_ensemble_prediction(models_info, val_texts, val_labels):\n",
        "    \"\"\"Ultra ensemble with weighted combination\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf ULTRA ENSEMBLE COMBINATION...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Her model i\u00e7in predictions al\n",
        "    all_predictions = []\n",
        "    model_weights = []\n",
        "\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, 256)\n",
        "\n",
        "    for i, model_info in enumerate(models_info):\n",
        "        model = model_info['model']\n",
        "        f1_score_val = model_info['f1']\n",
        "        model_weights.append(f1_score_val)\n",
        "\n",
        "        # Prediction\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        predictions = trainer.predict(val_dataset)\n",
        "        pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "        all_predictions.append(pred_probs)\n",
        "\n",
        "        print(f\"Model {i+1}: F1={f1_score_val:.4f}\")\n",
        "\n",
        "    # Normalize weights\n",
        "    model_weights = np.array(model_weights)\n",
        "    model_weights = model_weights / np.sum(model_weights)\n",
        "    print(f\"\ud83d\udcca Model weights: {model_weights}\")\n",
        "\n",
        "    # Weighted ensemble\n",
        "    weighted_avg = np.average(all_predictions, axis=0, weights=model_weights)\n",
        "    ensemble_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    # Performance\n",
        "    ensemble_f1 = f1_score(val_labels, ensemble_predictions, average='macro')\n",
        "    ensemble_acc = accuracy_score(val_labels, ensemble_predictions)\n",
        "    ensemble_precision = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[0]\n",
        "    ensemble_recall = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[1]\n",
        "\n",
        "    return {\n",
        "        'f1': ensemble_f1,\n",
        "        'accuracy': ensemble_acc,\n",
        "        'precision': ensemble_precision,\n",
        "        'recall': ensemble_recall,\n",
        "        'predictions': ensemble_predictions,\n",
        "        'probabilities': weighted_avg\n",
        "    }\n",
        "\n",
        "# ULTRA BOOST EXECUTION\n",
        "print(f\"\\n\ud83d\ude80 ULTRA BOOST EXECUTION BA\u015eLIYOR...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_start = time.time()\n",
        "models_info = []\n",
        "\n",
        "# 3 farkl\u0131 seed ile ultra fine-tune\n",
        "ultra_seeds = [111, 222, 333]\n",
        "for i, seed in enumerate(ultra_seeds):\n",
        "    model_info = ultra_fine_tune_model(seed, f\"v{i+1}\")\n",
        "    models_info.append(model_info)\n",
        "\n",
        "# Ultra ensemble\n",
        "ensemble_results = ultra_ensemble_prediction(models_info, val_texts, val_labels)\n",
        "\n",
        "# SONU\u00c7LAR\n",
        "total_time = time.time() - total_start\n",
        "print(f\"\\n\ud83c\udfc6 ULTRA BOOST SONU\u00c7LARI:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Individual model sonu\u00e7lar\u0131\n",
        "print(\"\ud83d\udcca INDIVIDUAL MODEL PERFORMANSLARI:\")\n",
        "for i, info in enumerate(models_info):\n",
        "    print(f\"Model {i+1}: F1={info['f1']:.4f}, Acc={info['accuracy']:.4f}\")\n",
        "\n",
        "best_individual = max(models_info, key=lambda x: x['f1'])\n",
        "print(f\"\\n\ud83e\udd47 En iyi individual: F1={best_individual['f1']:.4f}\")\n",
        "\n",
        "# Ensemble sonu\u00e7lar\u0131\n",
        "print(f\"\\n\ud83c\udf8a ULTRA ENSEMBLE SONU\u00c7LARI:\")\n",
        "print(f\"\ud83c\udfaf F1 Score: {ensemble_results['f1']:.4f}\")\n",
        "print(f\"\ud83d\udcca Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
        "print(f\"\ud83d\udcc8 Precision: {ensemble_results['precision']:.4f}\")\n",
        "print(f\"\ud83d\udcc8 Recall: {ensemble_results['recall']:.4f}\")\n",
        "\n",
        "# Hedef de\u011ferlendirmesi\n",
        "if ensemble_results['f1'] >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf8a HEDEF ULA\u015eILDI! %90+ F1 SCORE!\")\n",
        "    achievement = \"LEGENDARY \u2b50\u2b50\u2b50\"\n",
        "elif ensemble_results['f1'] >= 0.895:\n",
        "    print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! %89.5+ F1!\")\n",
        "    achievement = \"EXCELLENT \u2b50\u2b50\"\n",
        "else:\n",
        "    improvement = ensemble_results['f1'] - 0.8961  # En iyi \u00f6nceki\n",
        "    print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.4f} F1\")\n",
        "    achievement = \"IMPROVED \u2b50\"\n",
        "\n",
        "# Detailed report\n",
        "print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "print(classification_report(val_labels, ensemble_results['predictions'],\n",
        "                          target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\ud83d\udcda ULTRA BOOST \u00d6ZET\u0130:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"\u2022 Strategy: Ultra Fine-Tuning + Multi-Seed Ensemble\")\n",
        "print(f\"\u2022 Models: {len(models_info)} models\")\n",
        "print(f\"\u2022 Best Individual: {best_individual['f1']:.4f} F1\")\n",
        "print(f\"\u2022 Ultra Ensemble: {ensemble_results['f1']:.4f} F1\")\n",
        "print(f\"\u2022 Achievement: {achievement}\")\n",
        "print(f\"\u2022 Total Time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "# En iyi modeli kaydet\n",
        "print(f\"\\n\ud83d\udcbe EN \u0130Y\u0130 MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "if ensemble_results['f1'] > best_individual['f1']:\n",
        "    # Ensemble daha iyiyse ensemble weights'i kaydet\n",
        "    print(\"\ud83c\udfc6 Ensemble daha iyi - Ensemble bilgileri kaydediliyor\")\n",
        "    ensemble_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ultra_ensemble_final\"\n",
        "    os.makedirs(ensemble_save_path, exist_ok=True)\n",
        "\n",
        "    # Model paths'i ve weights'i kaydet\n",
        "    import json\n",
        "    ensemble_info = {\n",
        "        'model_paths': [f\"ultra_boost_model_v{i+1}\" for i in range(len(models_info))],\n",
        "        'weights': [info['f1'] for info in models_info],\n",
        "        'ensemble_f1': ensemble_results['f1'],\n",
        "        'ensemble_accuracy': ensemble_results['accuracy']\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(ensemble_save_path, 'ensemble_config.json'), 'w') as f:\n",
        "        json.dump(ensemble_info, f, indent=2)\n",
        "\n",
        "    tokenizer.save_pretrained(ensemble_save_path)\n",
        "else:\n",
        "    # Individual model daha iyiyse onu kaydet\n",
        "    print(\"\ud83c\udfc6 Individual model daha iyi - En iyi model kaydediliyor\")\n",
        "    best_model_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/ultra_best_individual\"\n",
        "    os.makedirs(best_model_save_path, exist_ok=True)\n",
        "    best_individual['model'].save_pretrained(best_model_save_path)\n",
        "    tokenizer.save_pretrained(best_model_save_path)\n",
        "\n",
        "print(f\"\u2705 En iyi model/ensemble kaydedildi!\")\n",
        "\n",
        "# Test prediction\n",
        "print(f\"\\n\ud83e\uddea FINAL TEST:\")\n",
        "test_texts = [\n",
        "    \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "    \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "    \"G\u00fczel\",\n",
        "    \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve be\u011fendim\"\n",
        "]\n",
        "\n",
        "for test_text in test_texts:\n",
        "    inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # En iyi modelle tahmin\n",
        "    with torch.no_grad():\n",
        "        outputs = best_individual['model'](**inputs)\n",
        "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "        confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "    result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "    print(f\"'{test_text[:50]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a ULTRA BOOST STRATEGY TAMAMLANDI!\")\n",
        "print(f\"\ud83c\udfc6 FINAL SCORE: {max(ensemble_results['f1'], best_individual['f1']):.4f} F1\")\n",
        "\n",
        "# Memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "098eb7195c1a4b9ab27e81f5264f5210",
            "2d42671eb2a4417fa02a2bb466d14def",
            "8e3feb5c47854b3ba4a5a7737385b7f7",
            "ac3951e207cc49d9b532867f1b4bd240",
            "2aaaa41272fb4096ba1313eba039273c",
            "ed317e37b11e42a3a93f93a02de6256f",
            "e8532c6d1903499591e2de99544b3375",
            "0995b700e6dc49638f2eb7f7c213ca4c",
            "e4d5832319914f459017b3e07ed2976c",
            "aea42cedbea5492a9373d8786b1646a3",
            "78ab5788a266488ea01b553eabccdc68",
            "cdb6e4af4d9449be9d875bd408da64aa",
            "81b2f7ff55764089b4c7b7ffc94d170a",
            "07e6b558b2484bdf8ae6582d9ad1a423",
            "8b730def0b35411cba1c37fa6a13a9d0",
            "21abc6508ec24023bef92892f139aec9",
            "ce97ea1fa5164fc496d60530fcd487f9",
            "311be7a51e774cb68fa4424ab3e23b0a",
            "3bf4f6e05cbf45a39cae8ea5108b5560",
            "cc1fb9fa27ba48668d8a31e5f0830d8c",
            "37cb089a2dca475bba08cf90b30c17d5",
            "16b6c35fd02f4c958c1afcb29cc5e661",
            "e6151a1071ff4b6ea3fe6c6f38300b10",
            "30b951512c9144d39c6f49fd3ce7d6b3",
            "ed52e3b5d4ee46c9833a79b9890d5d36",
            "2dbf150a38384b359802f9206f6d8741",
            "8a738f5c40af409bab39a330474d2f2b",
            "43334a9ca42243f78b1791ead678f706",
            "30d8e2e90dbd4f0b805dc1ed785cfdf5",
            "f684a9060ca145bab7ec6f034d0e473c",
            "7026abcd889f4bbfa5e2a46cae9d9abe",
            "45936e2edda84d059259ed3c4305088e",
            "20bc09e7892d444aa366d240c46fc72a",
            "41d28fd71b054d7ab5d2a7b15d99c7f5",
            "ced3bc4d344b4c43ad1976f55a8b3013",
            "cde90bc3442b46f0be6deed3a6814686",
            "1041fd28fd5e409dbe29a9cbc122bfce",
            "8b0312d83ebf4c6a92ce7c12f169d890",
            "71d123b359fa4446b1779b5c01781637",
            "dbe5e1f99bc644578414d630ed5362aa",
            "b2a837c934fb4d32ad3fa4b215b16c0c",
            "178f97e11b06428ea28e02410849b448",
            "8ce1294dfb3940f8be584fdd6c39b6a7",
            "a49eada87603477a93645b6413a336cb",
            "c12ddf3ff4d246839ed4d796581285a8",
            "b557a2b8171949f39ceb385ea082ca82",
            "6da95b4f2ae84172aff3261ed56a30ba",
            "6a46c8be029142fdb4b6bdbf1877e98d",
            "07ea8a5a621e4213a32ca93f142513fe",
            "4755f47b596648879c1efee532303549",
            "78428edca3bc4715800511b069bd64a0",
            "115b4ff2cc3f4d5eb4c7b379b1ebcdb0",
            "46945608ab9a4f3b967aa0c38e9bec54",
            "6af93fb03c7d4e2c974c47a452079f37",
            "4d0c7e8c09224f7cb65ef0b8bb889fe7",
            "be9cf2e0a0d541e7ab9b6abcecff8bde",
            "db0359c64143419d96f49ab465abacce",
            "b1ee143158da44e6a841b45d3366171b",
            "c414f2c950a646a1b2bf5abc4d47f725",
            "515214de91b24af6a6083db0a57d4673",
            "1577495cda0844cdb9cab5b7c2b2d838",
            "3ff4ed25fca0430a9b8a9836da6f5319",
            "50a379342dc446afaffa5eaed09af975",
            "5659277d96c149c1b3ea8dde41a356fc",
            "f13b62630b81466eb1e8c8a78feede72",
            "2ebab5ab3fa942e0abe7bb28feaa21a3",
            "c6c86e9a498c49a894387d8eb2f53e34",
            "02a69d7b4a744b96845b24e1f021f11e",
            "e8e486e83cf44eed97e69b599b17ef47",
            "9627df93a34b4d529abcb412e7e7d134",
            "2cd044d299d6423a8695ee15338ce2a4",
            "67aa9cc6e59445278331d2b8aecc1e89",
            "3a109d07aa43487bb03804b2baf44bf4",
            "e91881c50e6541c28a0581c0d191c01f",
            "cbc681cf38fd49cb9a0485bdffe002ee",
            "8d0629ce4cf54662a94788b08281ac40",
            "14b2cbfbee794b6b91a00122894bfe70",
            "05d399c48d234c2c8e91e38133ef2499",
            "76fdeb9f1ef64aab98c7590ea03e73b5",
            "3d723de6d38047b6bdda3fa893b52870",
            "0a59786e02ea46f68f0dee7a18ee290b",
            "3e72a9ac29f144d9ac45c99d0e3ac330",
            "629e2289ad2648548264a877e6633f84",
            "09297211cf9440a3b06aed755ef17d18",
            "cf76afc750474c4f80646638afc508af",
            "ff19a66977724813ab2515326f0e7adf",
            "20e5096b66c94dc4894508a73aab9dcf",
            "9173815d07914a2391e6d44597a9343b",
            "074ccac0a1b948319ff770377f9fe8c5",
            "eabe52fde9594a69a13dc9b43f193363",
            "67f74199976a4948afb97a9608f740ea",
            "4dafd2315e4e4417bc65e446990e232e",
            "866305ef680e4cfb9f2b187ff8bd11db",
            "f44cde4e4ac445b0b3a31853c17927ad",
            "cb64120223f6427aaeabf8dd10faf3a5",
            "af4727204d3749f5b930f63da707b0af",
            "50a06082b72e4474824c65811d1e3149",
            "7800a5c1dcac42728bdb4bf2bc1327e0",
            "4448412bdd9f40ff82ab9d5ab0d870a8",
            "47ff48ea8781440ba4e208851923a711",
            "5e89ce6eabb249c1b4092f8ef11f7fec",
            "bfe1f90951ad43628110ee720e74e6fa",
            "ddd0278eb41c40a2936af1bc5fa6e75c",
            "9cb8358217794b6886b918f2c5702ee8",
            "ea600127a0134f39a9e7e00ad5ad2371",
            "72ffa663a0e344399b53b269dca96539",
            "1eea0378f9cd4cc690571f4bf99984cb",
            "ec308ba9e8e6432188fcf4f76a8c34a5",
            "990d54ce2b5d49d8964f1dbc17f14e6a",
            "4218643f69b342028995b9c6e814447d",
            "83442eedd15b4a59a3f3d1f7ef6308c3",
            "96e8b3de19b1454e8d0272427d295cef",
            "db0ce08ac1384345a1eec2de3aef5c8b",
            "d804d00b36714a4f9273d62c7cb9ab15",
            "2eb0a0c60761457ea3d7e48031c3ab65",
            "6e67957a68ac4ff1a8abf47662ecdda9",
            "3ac2ed5c06334dd4bd44dca7b8827a7d",
            "755c53f011894888b9c87f06dee3af2d",
            "851756e050cc4838827a9709cc350ebb",
            "4784676c676c43aaa3af7ffc4a5ef085",
            "91382b235d8047c588965cf27e4bd6fa",
            "9d337380149640228a59ca4257763c32",
            "a7c97aab56ec4a0dbfbed66c4020a502",
            "3947d5150daa4a76ad4811958288f1ca",
            "0ff60a1f43a1441d9030509ffe1c2f8a",
            "fdfd2d9c5de94ec68a5633012d4f9ae0",
            "21110bc1528d468ab87bdbea98cd1fe6",
            "a6844b610572470084d442cee90737d1",
            "d1cd12249fde4ae095ef3dd6ce5082cc",
            "ef0df38f0cfe4f2d9c99e4f20a896af6",
            "f205ac08208c4cb1a5e8082cb8c78fe5",
            "63b594594208448abd74d9986f796dfa",
            "1196a1f1df19475aaacb6f0e49b39ca2",
            "af230ef66b6b402aa551d9ba546f92ea",
            "11edf53676a049bf91adec0345c36a08",
            "4239fdf263ad4306855602bd9d707b26",
            "8616f85965e34d3882b27460323f6071",
            "f3bb9aca79604fb09881a069784e0dfe",
            "be12b26245af4bf0a5625f167ad0cbe3",
            "29b2c26369ff449f92df01a190ed3e58",
            "d80f5b1ca20e4760b8b2ad1f5c8dcf69",
            "ed61ebfaf0b64d22b6fb836de66ece59",
            "a42d3698cd2f474cb175512bc2c12ab8",
            "b5e9bcec2d4e45f39656366a23af296e",
            "c0d68b207cff4dca9eeb49913d93dcd6",
            "33164ed47d614094b0d41a91826349d5",
            "84d30dad6ad04d02bf63f2f1ba9d80bb",
            "f869044263e4467a93e0c75fdf9c298a",
            "0ea8459d259045d59aee2775a3b0c350",
            "3c79674abed14051b02ef2b922cdffa9",
            "3b1a986f8fa0422b970e13d4d6b83963",
            "085d1f15481642b488a9745bb858cf19",
            "b14f134153f74b57a534fe92b7c1c6f3",
            "6a9a6503573a4baba6e4528975128b3f",
            "aaa09d34d191478a91a84dcda4ef54e5",
            "6307fdc45d504c64acef30754a41781b",
            "855817479c0e42d297a2c2947a60a98e",
            "165e76d186fd4143a37dfce206a4760f",
            "d43360d710b643f68590339cdde4b1b7",
            "4c5bf1c7e91e4edbaf2777e331e8d3e1",
            "b54b5e6e03bb46c0898898eacb65f775",
            "866e99716cae41dcbbfa4814cbe67678",
            "2005c97aa5644f60b4589b26f8b4953f",
            "5e56738fb783467192cb9baa0786bf0b",
            "a089d510f8f34384abf65dea5631d1fa",
            "19b4103119f245da9c219fc3142f9253",
            "f69d826f542d408d9e424d0464056fe5",
            "4d14d4a211e04122aa14e1566d51bb76",
            "a8239cc55131470784eff835b9569f8f",
            "a3387362996342d7be67307197d9b2be",
            "05e42745921e413d940863c415d1b6bd",
            "8ca1c2990a7c46f6b085f6d08acdac7d",
            "6e7bc18e4ea342259b8e27a9d8db0b6c",
            "34b23ece96874814be00aff84642716c",
            "8a3c548e4baf4e48b996232609e018ae",
            "02917b0369824ad4abc1b3ef194309d1",
            "692a2a437304457ca1b00903410696d6",
            "f538e46122bb419c87b0c9edbfc632fc",
            "c30ffbe3970140ffa2eca42d683e1ed7",
            "0f3c7ee53a304c609a3eba1f9e1928ec",
            "922aa915d6ec4b85bbc727a2c0abd32c",
            "9de7b7f8dd924e90a28358012efad3ee",
            "5bd3df7c312a42f7856fea4f9e278056",
            "16da8281760f433791ba4163fd3fb7d5",
            "e2a6080b03724a06b745019d0d62681f",
            "8a78f2fef3674fd686a1ad86f96009b6",
            "edba7761346a47efbbfa22173365fa39",
            "d4645393ed6c4952b00b76cab345d67e",
            "e6781a9923724372a741594fd66e7dc8",
            "76e90df0f30146c8838b60a49b8c72fc",
            "9ecf3cecf1674e0a9172ad002a606233",
            "85b4aa2e368a45f8ac1f3808cb3094c0",
            "36b4367ea3274264b9fd1821aacbb849",
            "864e91f7c4894fe4a04d432fd32f6f41",
            "c7d385eca42a4885aeac7671c1b1fb67",
            "e805a025d1b6412a8bf8d60f2eb0b377",
            "bb77808863204c01b78e30004e7a856c",
            "f479125c6e3b4e1daeab32c89ae0b8fc",
            "71792ef104be4bb38f8445d56d229fe4",
            "f6deeb00bdaf4cebba7df489b747d494",
            "f37d8c16a9a64e0294203f0ec1cd7419",
            "b8c4fe2d8f2e403a8eaface34f6b0e3a",
            "8ec0672fe7ec41eda53c8d71c3f0ba3e",
            "d2d56a646e7f41f98572eaf069fa82f0",
            "3d5c784071d5447fbc5756542ba3f754",
            "4036c94e60174a839604cab13a0923fb",
            "5f7733fcc0024b03826e9f4e9a2b9fde",
            "7791cd1f26644e708d9b49caa5147019",
            "dcd9d4166d854dc7a2c650ca4b5bb9af"
          ]
        },
        "executionInfo": {
          "elapsed": 5378203,
          "status": "error",
          "timestamp": 1750182913047,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "-y5H8UgKtch9",
        "outputId": "fb085354-bd25-4c5e-c1bb-ea31c311ca0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\uddf9\ud83c\uddf7 TURKISH BERT + XLM-RoBERTa MEGA ENSEMBLE - 90%+ HEDEF\n",
            "======================================================================\n",
            "\ud83c\udfaf T\u00fcrk\u00e7e \u00f6zel modeller + XLM-RoBERTa ensemble\n",
            "\ud83c\udfc6 Hedef: 89.67% \u2192 90.5%+ F1 Score\n",
            "\u26a1 S\u00fcre: ~3-4 saat (7 model)\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA L4\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Class weights: [1.1342357  0.89417523]\n",
            "\ud83d\udcca Train: 13650, Val: 1517\n",
            "\n",
            "\ud83d\ude80 MEGA ENSEMBLE EXECUTION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83c\udf0d XLM-RoBERTa VARIANTS...\n",
            "\n",
            "\ud83d\udd25 XLM-RoBERTa Multilingual - xlm_roberta_111 (Seed: 111)\n",
            "============================================================\n",
            "\ud83d\udce6 xlm-roberta-base y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "098eb7195c1a4b9ab27e81f5264f5210",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdb6e4af4d9449be9d875bd408da64aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6151a1071ff4b6ea3fe6c6f38300b10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41d28fd71b054d7ab5d2a7b15d99c7f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c12ddf3ff4d246839ed4d796581285a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 12:11, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.039100</td>\n",
              "      <td>0.031779</td>\n",
              "      <td>0.865524</td>\n",
              "      <td>0.863449</td>\n",
              "      <td>0.863954</td>\n",
              "      <td>0.862994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.030500</td>\n",
              "      <td>0.029363</td>\n",
              "      <td>0.879367</td>\n",
              "      <td>0.876823</td>\n",
              "      <td>0.880287</td>\n",
              "      <td>0.874588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>0.030750</td>\n",
              "      <td>0.880026</td>\n",
              "      <td>0.878858</td>\n",
              "      <td>0.877723</td>\n",
              "      <td>0.880699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.024000</td>\n",
              "      <td>0.032622</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.879553</td>\n",
              "      <td>0.879939</td>\n",
              "      <td>0.879196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.022500</td>\n",
              "      <td>0.032614</td>\n",
              "      <td>0.880026</td>\n",
              "      <td>0.878625</td>\n",
              "      <td>0.877857</td>\n",
              "      <td>0.879594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 12.2 dakika\n",
            "\ud83c\udfaf F1: 0.8796\n",
            "\ud83d\udcca Accuracy: 0.8813\n",
            "\n",
            "\ud83d\udd25 XLM-RoBERTa Multilingual - xlm_roberta_222 (Seed: 222)\n",
            "============================================================\n",
            "\ud83d\udce6 xlm-roberta-base y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 12:06, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.040300</td>\n",
              "      <td>0.032311</td>\n",
              "      <td>0.872775</td>\n",
              "      <td>0.870876</td>\n",
              "      <td>0.871186</td>\n",
              "      <td>0.870584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.878819</td>\n",
              "      <td>0.882419</td>\n",
              "      <td>0.876514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.028542</td>\n",
              "      <td>0.866842</td>\n",
              "      <td>0.866582</td>\n",
              "      <td>0.868917</td>\n",
              "      <td>0.873797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.025900</td>\n",
              "      <td>0.032300</td>\n",
              "      <td>0.883322</td>\n",
              "      <td>0.882291</td>\n",
              "      <td>0.881064</td>\n",
              "      <td>0.884593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.023500</td>\n",
              "      <td>0.030527</td>\n",
              "      <td>0.880686</td>\n",
              "      <td>0.879719</td>\n",
              "      <td>0.878491</td>\n",
              "      <td>0.882393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 12.1 dakika\n",
            "\ud83c\udfaf F1: 0.8823\n",
            "\ud83d\udcca Accuracy: 0.8833\n",
            "\n",
            "\ud83d\udd25 XLM-RoBERTa Multilingual - xlm_roberta_333 (Seed: 333)\n",
            "============================================================\n",
            "\ud83d\udce6 xlm-roberta-base y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1710' max='1710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1710/1710 12:13, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.039700</td>\n",
              "      <td>0.030838</td>\n",
              "      <td>0.880686</td>\n",
              "      <td>0.879507</td>\n",
              "      <td>0.878387</td>\n",
              "      <td>0.881288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.029387</td>\n",
              "      <td>0.866842</td>\n",
              "      <td>0.866480</td>\n",
              "      <td>0.867730</td>\n",
              "      <td>0.872850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.027800</td>\n",
              "      <td>0.027674</td>\n",
              "      <td>0.875412</td>\n",
              "      <td>0.874627</td>\n",
              "      <td>0.873644</td>\n",
              "      <td>0.878307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.029286</td>\n",
              "      <td>0.871457</td>\n",
              "      <td>0.870255</td>\n",
              "      <td>0.869110</td>\n",
              "      <td>0.872245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.021800</td>\n",
              "      <td>0.030752</td>\n",
              "      <td>0.874094</td>\n",
              "      <td>0.872916</td>\n",
              "      <td>0.871762</td>\n",
              "      <td>0.874919</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [38/38 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 12.2 dakika\n",
            "\ud83c\udfaf F1: 0.8795\n",
            "\ud83d\udcca Accuracy: 0.8807\n",
            "\n",
            "\ud83c\uddf9\ud83c\uddf7 TURKISH BERT VARIANTS...\n",
            "\n",
            "\ud83d\udd25 Turkish BERT (DBMDz) - turkish_bert_111 (Seed: 111)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be9cf2e0a0d541e7ab9b6abcecff8bde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6c86e9a498c49a894387d8eb2f53e34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05d399c48d234c2c8e91e38133ef2499",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "074ccac0a1b948319ff770377f9fe8c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2562/2562 11:27, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.038900</td>\n",
              "      <td>0.029331</td>\n",
              "      <td>0.858273</td>\n",
              "      <td>0.857727</td>\n",
              "      <td>0.857935</td>\n",
              "      <td>0.862977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>0.025171</td>\n",
              "      <td>0.883982</td>\n",
              "      <td>0.882488</td>\n",
              "      <td>0.882053</td>\n",
              "      <td>0.882974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.025600</td>\n",
              "      <td>0.025854</td>\n",
              "      <td>0.888596</td>\n",
              "      <td>0.887525</td>\n",
              "      <td>0.886344</td>\n",
              "      <td>0.889468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.020700</td>\n",
              "      <td>0.027867</td>\n",
              "      <td>0.893210</td>\n",
              "      <td>0.891770</td>\n",
              "      <td>0.891535</td>\n",
              "      <td>0.892018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.030967</td>\n",
              "      <td>0.893869</td>\n",
              "      <td>0.892793</td>\n",
              "      <td>0.891667</td>\n",
              "      <td>0.894501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.013200</td>\n",
              "      <td>0.031637</td>\n",
              "      <td>0.894529</td>\n",
              "      <td>0.893327</td>\n",
              "      <td>0.892455</td>\n",
              "      <td>0.894459</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 11.5 dakika\n",
            "\ud83c\udfaf F1: 0.8933\n",
            "\ud83d\udcca Accuracy: 0.8945\n",
            "\n",
            "\ud83d\udd25 Turkish BERT (DBMDz) - turkish_bert_222 (Seed: 222)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2562/2562 11:26, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.038400</td>\n",
              "      <td>0.027425</td>\n",
              "      <td>0.868161</td>\n",
              "      <td>0.867288</td>\n",
              "      <td>0.866270</td>\n",
              "      <td>0.870717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.027200</td>\n",
              "      <td>0.024744</td>\n",
              "      <td>0.883322</td>\n",
              "      <td>0.882349</td>\n",
              "      <td>0.881108</td>\n",
              "      <td>0.884909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.022700</td>\n",
              "      <td>0.024966</td>\n",
              "      <td>0.882004</td>\n",
              "      <td>0.881738</td>\n",
              "      <td>0.883569</td>\n",
              "      <td>0.888778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.018800</td>\n",
              "      <td>0.026294</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.890977</td>\n",
              "      <td>0.889701</td>\n",
              "      <td>0.893521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.016300</td>\n",
              "      <td>0.029013</td>\n",
              "      <td>0.898484</td>\n",
              "      <td>0.897495</td>\n",
              "      <td>0.896294</td>\n",
              "      <td>0.899417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.029806</td>\n",
              "      <td>0.899802</td>\n",
              "      <td>0.898878</td>\n",
              "      <td>0.897614</td>\n",
              "      <td>0.901069</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 11.5 dakika\n",
            "\ud83c\udfaf F1: 0.8989\n",
            "\ud83d\udcca Accuracy: 0.8998\n",
            "\n",
            "\ud83c\udf0d MULTILINGUAL BERT...\n",
            "\n",
            "\ud83d\udd25 Multilingual BERT - mbert_111 (Seed: 111)\n",
            "============================================================\n",
            "\ud83d\udce6 bert-base-multilingual-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47ff48ea8781440ba4e208851923a711",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83442eedd15b4a59a3f3d1f7ef6308c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d337380149640228a59ca4257763c32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1196a1f1df19475aaacb6f0e49b39ca2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5e9bcec2d4e45f39656366a23af296e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1900/1900 10:18, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.033800</td>\n",
              "      <td>0.030130</td>\n",
              "      <td>0.862228</td>\n",
              "      <td>0.861271</td>\n",
              "      <td>0.860223</td>\n",
              "      <td>0.864463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.030100</td>\n",
              "      <td>0.027272</td>\n",
              "      <td>0.874094</td>\n",
              "      <td>0.873379</td>\n",
              "      <td>0.872577</td>\n",
              "      <td>0.877443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.022900</td>\n",
              "      <td>0.029148</td>\n",
              "      <td>0.874094</td>\n",
              "      <td>0.873688</td>\n",
              "      <td>0.874374</td>\n",
              "      <td>0.879652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.016300</td>\n",
              "      <td>0.038797</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.879959</td>\n",
              "      <td>0.879188</td>\n",
              "      <td>0.880931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>0.040556</td>\n",
              "      <td>0.883982</td>\n",
              "      <td>0.882882</td>\n",
              "      <td>0.881703</td>\n",
              "      <td>0.884868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='43' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [43/43 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 10.3 dakika\n",
            "\ud83c\udfaf F1: 0.8829\n",
            "\ud83d\udcca Accuracy: 0.8840\n",
            "\n",
            "\ud83c\uddf9\ud83c\uddf7 TURKISH SENTIMENT BERT...\n",
            "\n",
            "\ud83d\udd25 Turkish Sentiment BERT - turkish_sentiment_111 (Seed: 111)\n",
            "============================================================\n",
            "\ud83d\udce6 savasy/bert-base-turkish-sentiment-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aaa09d34d191478a91a84dcda4ef54e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19b4103119f245da9c219fc3142f9253",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "692a2a437304457ca1b00903410696d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4645393ed6c4952b00b76cab345d67e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71792ef104be4bb38f8445d56d229fe4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2562/2562 11:28, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.039300</td>\n",
              "      <td>0.030246</td>\n",
              "      <td>0.839156</td>\n",
              "      <td>0.838783</td>\n",
              "      <td>0.840601</td>\n",
              "      <td>0.845247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.029200</td>\n",
              "      <td>0.025478</td>\n",
              "      <td>0.870138</td>\n",
              "      <td>0.869477</td>\n",
              "      <td>0.868919</td>\n",
              "      <td>0.873905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>0.026704</td>\n",
              "      <td>0.874094</td>\n",
              "      <td>0.873668</td>\n",
              "      <td>0.874212</td>\n",
              "      <td>0.879494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.029237</td>\n",
              "      <td>0.896506</td>\n",
              "      <td>0.894821</td>\n",
              "      <td>0.895761</td>\n",
              "      <td>0.894020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.017700</td>\n",
              "      <td>0.029169</td>\n",
              "      <td>0.888596</td>\n",
              "      <td>0.887772</td>\n",
              "      <td>0.886548</td>\n",
              "      <td>0.890888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>0.030407</td>\n",
              "      <td>0.889255</td>\n",
              "      <td>0.888423</td>\n",
              "      <td>0.887186</td>\n",
              "      <td>0.891478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 MODEL TAMAMLANDI!\n",
            "\u23f0 S\u00fcre: 11.5 dakika\n",
            "\ud83c\udfaf F1: 0.8948\n",
            "\ud83d\udcca Accuracy: 0.8965\n",
            "\n",
            "\u2705 TOPLAM 7 MODEL E\u011e\u0130T\u0130LD\u0130\n",
            "\n",
            "\ud83c\udfaf MEGA ENSEMBLE COMBINATION...\n",
            "==================================================\n",
            "\ud83d\udcca Ba\u015far\u0131l\u0131 modeller: 7\n",
            "\ud83d\udd04 XLM-RoBERTa Multilingual tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [190/190 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u274c Prediction hatas\u0131: XLM-RoBERTa Multilingual - \n",
            "\ud83d\udd04 XLM-RoBERTa Multilingual tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [190/190 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u274c Prediction hatas\u0131: XLM-RoBERTa Multilingual - \n",
            "\ud83d\udd04 XLM-RoBERTa Multilingual tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='104' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [104/190 00:01 < 00:01, 58.93 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-995416020>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# Mega ensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_models_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0mensemble_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmega_ensemble_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_models_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensemble_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-995416020>\u001b[0m in \u001b[0;36mmega_ensemble_prediction\u001b[0;34m(models_info, val_texts, val_labels)\u001b[0m\n\u001b[1;32m    291\u001b[0m             )\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0mpred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mall_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4250\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4251\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4252\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4253\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4357\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4358\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4359\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4360\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-995416020>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         encoding = self.tokenizer(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2865\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2866\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2867\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2975\u001b[0m             )\n\u001b[1;32m   2976\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2977\u001b[0;31m             return self.encode_plus(\n\u001b[0m\u001b[1;32m   2978\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m         )\n\u001b[1;32m   3051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3052\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3053\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    614\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eventual_warn_about_too_long_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitized_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitized_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     def _encode_plus(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0;31m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mas_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtensor_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJAX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"\ud83c\uddf9\ud83c\uddf7 TURKISH BERT + XLM-RoBERTa MEGA ENSEMBLE - 90%+ HEDEF\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf T\u00fcrk\u00e7e \u00f6zel modeller + XLM-RoBERTa ensemble\")\n",
        "print(\"\ud83c\udfc6 Hedef: 89.67% \u2192 90.5%+ F1 Score\")\n",
        "print(\"\u26a1 S\u00fcre: ~3-4 saat (7 model)\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Ultra Focal Loss (geli\u015ftirilmi\u015f)\n",
        "class AdvancedFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.7, gamma=3.0, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.class_weights)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Dynamic focal weighting\n",
        "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
        "        focal_loss = focal_weight * ce_loss\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Advanced Trainer\n",
        "class TurkishTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.class_weights = kwargs.pop('class_weights', None)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = AdvancedFocalLoss(alpha=0.7, gamma=3.0, class_weights=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "\n",
        "# Class weights hesapla\n",
        "class_counts = np.bincount(labels)\n",
        "class_weights = torch.FloatTensor([len(labels) / (2 * count) for count in class_counts]).to(device)\n",
        "print(f\"\ud83d\udcca Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "# Train/val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "\n",
        "# Model konfig\u00fcrasyonlar\u0131\n",
        "MODEL_CONFIGS = {\n",
        "    'xlm_roberta': {\n",
        "        'model_name': 'xlm-roberta-base',\n",
        "        'max_length': 256,\n",
        "        'batch_size': 20,\n",
        "        'learning_rate': 8e-6,\n",
        "        'epochs': 5,\n",
        "        'description': 'XLM-RoBERTa Multilingual'\n",
        "    },\n",
        "    'turkish_bert': {\n",
        "        'model_name': 'dbmdz/bert-base-turkish-cased',\n",
        "        'max_length': 256,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 1e-5,\n",
        "        'epochs': 6,\n",
        "        'description': 'Turkish BERT (DBMDz)'\n",
        "    },\n",
        "    'multilingual_bert': {\n",
        "        'model_name': 'bert-base-multilingual-cased',\n",
        "        'max_length': 256,\n",
        "        'batch_size': 18,\n",
        "        'learning_rate': 1.2e-5,\n",
        "        'epochs': 5,\n",
        "        'description': 'Multilingual BERT'\n",
        "    },\n",
        "    'turkish_sentiment': {\n",
        "        'model_name': 'savasy/bert-base-turkish-sentiment-cased',\n",
        "        'max_length': 256,\n",
        "        'batch_size': 16,\n",
        "        'learning_rate': 8e-6,\n",
        "        'epochs': 6,\n",
        "        'description': 'Turkish Sentiment BERT'\n",
        "    }\n",
        "}\n",
        "\n",
        "def train_model_variant(model_config, seed, variant_name):\n",
        "    \"\"\"Belirli model tipini e\u011fit\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd25 {model_config['description']} - {variant_name} (Seed: {seed})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    try:\n",
        "        # Tokenizer ve model y\u00fckle\n",
        "        print(f\"\ud83d\udce6 {model_config['model_name']} y\u00fckleniyor...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_config['model_name'])\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_config['model_name'],\n",
        "            num_labels=2,\n",
        "            return_dict=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Dataset olu\u015ftur\n",
        "        train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, model_config['max_length'])\n",
        "        val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, model_config['max_length'])\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f'./mega_ensemble_{variant_name}',\n",
        "            num_train_epochs=model_config['epochs'],\n",
        "            per_device_train_batch_size=model_config['batch_size'],\n",
        "            per_device_eval_batch_size=model_config['batch_size'] * 2,\n",
        "            gradient_accumulation_steps=2,\n",
        "            warmup_ratio=0.25,\n",
        "            learning_rate=model_config['learning_rate'],\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            weight_decay=0.02,\n",
        "            label_smoothing_factor=0.3,\n",
        "            seed=seed,\n",
        "            bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "            fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "            logging_steps=100,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=1,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            greater_is_better=True,\n",
        "            report_to=\"none\",\n",
        "            dataloader_pin_memory=True,\n",
        "            dataloader_num_workers=2,\n",
        "            gradient_checkpointing=True,\n",
        "            adam_epsilon=1e-8,\n",
        "            max_grad_norm=0.5,\n",
        "        )\n",
        "\n",
        "        # Trainer\n",
        "        trainer = TurkishTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "            class_weights=class_weights,\n",
        "        )\n",
        "\n",
        "        # E\u011fitim\n",
        "        start_time = time.time()\n",
        "        trainer.train()\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # De\u011ferlendirme\n",
        "        eval_results = trainer.evaluate()\n",
        "        f1_score_result = eval_results['eval_f1']\n",
        "        accuracy_result = eval_results['eval_accuracy']\n",
        "\n",
        "        print(f\"\u2705 MODEL TAMAMLANDI!\")\n",
        "        print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "        print(f\"\ud83c\udfaf F1: {f1_score_result:.4f}\")\n",
        "        print(f\"\ud83d\udcca Accuracy: {accuracy_result:.4f}\")\n",
        "\n",
        "        # Model kaydet\n",
        "        save_path = f\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_{variant_name}\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        model.save_pretrained(save_path)\n",
        "        tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return {\n",
        "            'model': trainer.model,\n",
        "            'tokenizer': tokenizer,\n",
        "            'f1': f1_score_result,\n",
        "            'accuracy': accuracy_result,\n",
        "            'model_name': model_config['model_name'],\n",
        "            'description': model_config['description'],\n",
        "            'save_path': save_path,\n",
        "            'train_time': train_time\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c HATA: {model_config['model_name']} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def mega_ensemble_prediction(models_info, val_texts, val_labels):\n",
        "    \"\"\"Geli\u015fmi\u015f mega ensemble prediction\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf MEGA ENSEMBLE COMBINATION...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    all_predictions = []\n",
        "    model_weights = []\n",
        "    valid_models = [m for m in models_info if m is not None]\n",
        "\n",
        "    print(f\"\ud83d\udcca Ba\u015far\u0131l\u0131 modeller: {len(valid_models)}\")\n",
        "\n",
        "    for i, model_info in enumerate(valid_models):\n",
        "        try:\n",
        "            print(f\"\ud83d\udd04 {model_info['description']} tahmin al\u0131n\u0131yor...\")\n",
        "\n",
        "            # Dataset olu\u015ftur\n",
        "            val_dataset = ReviewDataset(val_texts, val_labels, model_info['tokenizer'], 256)\n",
        "\n",
        "            # Trainer ile prediction\n",
        "            trainer = Trainer(\n",
        "                model=model_info['model'],\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_metrics,\n",
        "            )\n",
        "\n",
        "            predictions = trainer.predict(val_dataset)\n",
        "            pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "            all_predictions.append(pred_probs)\n",
        "\n",
        "            # F1 score'a g\u00f6re a\u011f\u0131rl\u0131k\n",
        "            f1_weight = model_info['f1'] ** 2  # Kare alarak fark\u0131 art\u0131r\n",
        "            model_weights.append(f1_weight)\n",
        "\n",
        "            print(f\"\u2705 F1: {model_info['f1']:.4f}, Weight: {f1_weight:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\u274c Prediction hatas\u0131: {model_info['description']} - {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if len(all_predictions) == 0:\n",
        "        print(\"\u274c Hi\u00e7 model prediction al\u0131namad\u0131!\")\n",
        "        return None\n",
        "\n",
        "    # A\u011f\u0131rl\u0131klar\u0131 normalize et\n",
        "    model_weights = np.array(model_weights)\n",
        "    model_weights = model_weights / np.sum(model_weights)\n",
        "    print(f\"\ud83d\udcca Normalized weights: {model_weights}\")\n",
        "\n",
        "    # Weighted ensemble\n",
        "    weighted_avg = np.average(all_predictions, axis=0, weights=model_weights)\n",
        "    ensemble_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    # Performance hesapla\n",
        "    ensemble_f1 = f1_score(val_labels, ensemble_predictions, average='macro')\n",
        "    ensemble_acc = accuracy_score(val_labels, ensemble_predictions)\n",
        "    ensemble_precision = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[0]\n",
        "    ensemble_recall = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[1]\n",
        "\n",
        "    # S\u0131n\u0131f baz\u0131nda F1\n",
        "    class_f1 = f1_score(val_labels, ensemble_predictions, average=None)\n",
        "\n",
        "    return {\n",
        "        'f1': ensemble_f1,\n",
        "        'accuracy': ensemble_acc,\n",
        "        'precision': ensemble_precision,\n",
        "        'recall': ensemble_recall,\n",
        "        'class_f1': class_f1,\n",
        "        'predictions': ensemble_predictions,\n",
        "        'probabilities': weighted_avg,\n",
        "        'model_weights': model_weights,\n",
        "        'valid_models': len(valid_models)\n",
        "    }\n",
        "\n",
        "# MEGA ENSEMBLE EXECUTION\n",
        "print(f\"\\n\ud83d\ude80 MEGA ENSEMBLE EXECUTION BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_start = time.time()\n",
        "all_models_info = []\n",
        "\n",
        "# 1. XLM-RoBERTa variants (3 seed)\n",
        "print(f\"\\n\ud83c\udf0d XLM-RoBERTa VARIANTS...\")\n",
        "for seed in [111, 222, 333]:\n",
        "    model_info = train_model_variant(MODEL_CONFIGS['xlm_roberta'], seed, f\"xlm_roberta_{seed}\")\n",
        "    if model_info:\n",
        "        all_models_info.append(model_info)\n",
        "\n",
        "# 2. Turkish BERT variants (2 seed)\n",
        "print(f\"\\n\ud83c\uddf9\ud83c\uddf7 TURKISH BERT VARIANTS...\")\n",
        "for seed in [111, 222]:\n",
        "    model_info = train_model_variant(MODEL_CONFIGS['turkish_bert'], seed, f\"turkish_bert_{seed}\")\n",
        "    if model_info:\n",
        "        all_models_info.append(model_info)\n",
        "\n",
        "# 3. Multilingual BERT (1 seed)\n",
        "print(f\"\\n\ud83c\udf0d MULTILINGUAL BERT...\")\n",
        "model_info = train_model_variant(MODEL_CONFIGS['multilingual_bert'], 111, \"mbert_111\")\n",
        "if model_info:\n",
        "    all_models_info.append(model_info)\n",
        "\n",
        "# 4. Turkish Sentiment BERT (1 seed) - e\u011fer y\u00fcklenebilirse\n",
        "print(f\"\\n\ud83c\uddf9\ud83c\uddf7 TURKISH SENTIMENT BERT...\")\n",
        "try:\n",
        "    model_info = train_model_variant(MODEL_CONFIGS['turkish_sentiment'], 111, \"turkish_sentiment_111\")\n",
        "    if model_info:\n",
        "        all_models_info.append(model_info)\n",
        "except:\n",
        "    print(\"\u274c Turkish Sentiment BERT y\u00fcklenemedi, atlaniyor...\")\n",
        "\n",
        "print(f\"\\n\u2705 TOPLAM {len(all_models_info)} MODEL E\u011e\u0130T\u0130LD\u0130\")\n",
        "\n",
        "# Mega ensemble\n",
        "if len(all_models_info) > 0:\n",
        "    ensemble_results = mega_ensemble_prediction(all_models_info, val_texts, val_labels)\n",
        "\n",
        "    if ensemble_results:\n",
        "        # SONU\u00c7LAR\n",
        "        total_time = time.time() - total_start\n",
        "        print(f\"\\n\ud83c\udfc6 MEGA ENSEMBLE SONU\u00c7LARI:\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Individual model sonu\u00e7lar\u0131\n",
        "        print(\"\ud83d\udcca INDIVIDUAL MODEL PERFORMANSLARI:\")\n",
        "        for i, info in enumerate(all_models_info):\n",
        "            print(f\"{i+1}. {info['description']}: F1={info['f1']:.4f}, Acc={info['accuracy']:.4f}\")\n",
        "\n",
        "        best_individual = max(all_models_info, key=lambda x: x['f1'])\n",
        "        print(f\"\\n\ud83e\udd47 En iyi individual: {best_individual['description']} - F1={best_individual['f1']:.4f}\")\n",
        "\n",
        "        # Ensemble sonu\u00e7lar\u0131\n",
        "        print(f\"\\n\ud83c\udf8a MEGA ENSEMBLE SONU\u00c7LARI:\")\n",
        "        print(f\"\ud83c\udfaf F1 Score: {ensemble_results['f1']:.4f}\")\n",
        "        print(f\"\ud83d\udcca Accuracy: {ensemble_results['accuracy']:.4f}\")\n",
        "        print(f\"\ud83d\udcc8 Precision: {ensemble_results['precision']:.4f}\")\n",
        "        print(f\"\ud83d\udcc8 Recall: {ensemble_results['recall']:.4f}\")\n",
        "        print(f\"\ud83d\udd22 Model say\u0131s\u0131: {ensemble_results['valid_models']}\")\n",
        "\n",
        "        # S\u0131n\u0131f baz\u0131nda sonu\u00e7lar\n",
        "        print(f\"\\n\ud83d\udccb SINIF BAZINDA F1:\")\n",
        "        print(f\"Faydas\u0131z (0): {ensemble_results['class_f1'][0]:.4f}\")\n",
        "        print(f\"Faydal\u0131 (1): {ensemble_results['class_f1'][1]:.4f}\")\n",
        "\n",
        "        # Hedef de\u011ferlendirmesi\n",
        "        if ensemble_results['f1'] >= 0.90:\n",
        "            print(f\"\\n\ud83c\udf8a HEDEF ULA\u015eILDI! %90+ F1 SCORE!\")\n",
        "            achievement = \"\ud83c\udfc6 LEGENDARY \u2b50\u2b50\u2b50\"\n",
        "        elif ensemble_results['f1'] >= 0.895:\n",
        "            print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! %89.5+ F1!\")\n",
        "            achievement = \"\ud83d\udd25 EXCELLENT \u2b50\u2b50\"\n",
        "        else:\n",
        "            improvement = ensemble_results['f1'] - 0.8967  # \u00d6nceki en iyi\n",
        "            print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.4f} F1\")\n",
        "            achievement = \"\ud83d\udcc8 IMPROVED \u2b50\"\n",
        "\n",
        "        # Detailed report\n",
        "        print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "        print(classification_report(val_labels, ensemble_results['predictions'],\n",
        "                                  target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "        # Final summary\n",
        "        print(f\"\\n\ud83d\udcda MEGA ENSEMBLE \u00d6ZET\u0130:\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"\u2022 Strategy: Turkish BERT + XLM-RoBERTa Mega Ensemble\")\n",
        "        print(f\"\u2022 Total Models: {len(all_models_info)}\")\n",
        "        print(f\"\u2022 Best Individual: {best_individual['f1']:.4f} F1\")\n",
        "        print(f\"\u2022 Mega Ensemble: {ensemble_results['f1']:.4f} F1\")\n",
        "        print(f\"\u2022 \u0130yile\u015fme: {ensemble_results['f1'] - best_individual['f1']:+.4f} F1\")\n",
        "        print(f\"\u2022 Achievement: {achievement}\")\n",
        "        print(f\"\u2022 Total Time: {total_time/60:.1f} dakika\")\n",
        "\n",
        "        # En iyi sonucu kaydet\n",
        "        print(f\"\\n\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\")\n",
        "\n",
        "        # Ensemble config kaydet\n",
        "        ensemble_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_final\"\n",
        "        os.makedirs(ensemble_save_path, exist_ok=True)\n",
        "\n",
        "        ensemble_config = {\n",
        "            'ensemble_f1': ensemble_results['f1'],\n",
        "            'ensemble_accuracy': ensemble_results['accuracy'],\n",
        "            'model_weights': ensemble_results['model_weights'].tolist(),\n",
        "            'models': [\n",
        "                {\n",
        "                    'description': info['description'],\n",
        "                    'model_name': info['model_name'],\n",
        "                    'f1': info['f1'],\n",
        "                    'save_path': info['save_path']\n",
        "                }\n",
        "                for info in all_models_info\n",
        "            ],\n",
        "            'achievement': achievement,\n",
        "            'total_time': total_time\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(ensemble_save_path, 'mega_ensemble_config.json'), 'w', encoding='utf-8') as f:\n",
        "            json.dump(ensemble_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\u2705 Mega ensemble config kaydedildi!\")\n",
        "\n",
        "        # Test prediction\n",
        "        print(f\"\\n\ud83e\uddea MEGA ENSEMBLE TEST:\")\n",
        "        test_texts = [\n",
        "            \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "            \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "            \"G\u00fczel \u00fcr\u00fcn\",\n",
        "            \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\"\n",
        "        ]\n",
        "\n",
        "        # En iyi individual model ile test\n",
        "        best_tokenizer = best_individual['tokenizer']\n",
        "        best_model = best_individual['model']\n",
        "\n",
        "        for test_text in test_texts:\n",
        "            inputs = best_tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = best_model(**inputs)\n",
        "                prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "                predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "                confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "            result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "            print(f\"'{test_text[:50]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "        print(f\"\\n\ud83c\udf8a MEGA ENSEMBLE STRATEGY TAMAMLANDI!\")\n",
        "        print(f\"\ud83c\udfc6 FINAL SCORE: {ensemble_results['f1']:.4f} F1\")\n",
        "\n",
        "        if ensemble_results['f1'] >= 0.90:\n",
        "            print(f\"\ud83c\udf89 BA\u015eARILI! %90+ HEDEFE ULA\u015eILDI!\")\n",
        "        else:\n",
        "            remaining = 0.90 - ensemble_results['f1']\n",
        "            print(f\"\ud83d\udcc8 %90 hedefe {remaining:.4f} F1 kald\u0131\")\n",
        "\n",
        "    else:\n",
        "        print(\"\u274c Ensemble prediction ba\u015far\u0131s\u0131z!\")\n",
        "else:\n",
        "    print(\"\u274c Hi\u00e7 model e\u011fitilemedi!\")\n",
        "\n",
        "# Memory cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\ud83d\udcbe Memory temizlendi!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b14e60416094c6e813e70e335fbff70",
            "d8ea170c857d4ff0ae2ba4c055768c60",
            "4ba914b4a63149c08c1fbcddd001970f",
            "7a3c1c59ba1f434ebe534945af3024ce",
            "5712b5fa61dc4d0eaff1fd3a0baf080b",
            "9d6ff2df6c8446c9abe4fcd11f8eac0e",
            "476a3bebfa6146efaeaf5a139d3f58a6",
            "ef749ae9a67149b3a51cf84ac8d6a01e",
            "9b3daf5267294009a24adf4cefd3ad9c",
            "877afd4343094a69b6c2d4e0a37e0fa7",
            "5dedc5a861c54d1396fe11d38fe077eb",
            "d1a95e2b670e44b0a3b7b10d935d87c2",
            "1db6008bfcbd400095feee4653a40281",
            "d538c85356144436b44c38e6c68aeb80",
            "8459046a9ed1421c8a6ad1ea3607546f",
            "f98f909012b4493a9a8cab74d84d186f",
            "db3306c65d5c4bdf8ccbbfcb063e36ee",
            "ee47a966f4624682bc67b70c6cfbae0b",
            "fef672bad76d4326993d3dfe93513d97",
            "c9c5790336f746fba5c4ef5079e7349c",
            "1e8990fc908e4e059f5d601a28273b32",
            "f88d6316e2d94f21b3f8f2b05dda4e76",
            "9ffea7fedfc5421ba7fbaf89266f468f",
            "63c3d83829a74f93b4359a9d3c120120",
            "d7c88a997dab448a8e389098a03d627d",
            "23bad096204a4fdd989c1d7244f174c4",
            "c3b42e98b3904eba9589dd5dba6d4d8c",
            "aac50f4cafda4c7287702f19a5d74a4a",
            "232bd37a589b4c8288458cd9eb92e34b",
            "c1be2b2bdfef4cf188f4a0a10399bafc",
            "2c9cf0c0159c473eb999936c9c1b14ee",
            "715b31e2ac6b4fd386ab75267ef20d47",
            "7f0f10954e3a46bfa236878d722cdf5b",
            "5b83fb790ad34f8485607ceaa42cb28f",
            "93e896d47d0a40a3bd311ba7acd7f317",
            "df8dc5515b4940f989d91893e02c2682",
            "05207cefb2394c8db593aeb01b5440cc",
            "cfda436f7f6c448cbeed2d80452f3576",
            "334e9096172c4d4eb9170a3706e64a6b",
            "2244a0eb1a1748ad98f04e0b629936b2",
            "8536e6cc8c3f41e78775eaf0bf393a19",
            "7ac88441a2e8448588be82538e9f0dcd",
            "829808d141d84e5ea9a140be7dd6acb2",
            "bc8135c55fc242fbbb2b5facc864ee0f"
          ]
        },
        "executionInfo": {
          "elapsed": 10134,
          "status": "ok",
          "timestamp": 1750183531404,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "qwB1XgmzC42d",
        "outputId": "cceba33e-dab7-4c6e-b9aa-d9f959a169e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 SUPER TURKISH BERT - 90%+ GARANT\u0130L\u0130\n",
            "============================================================\n",
            "\ud83c\udfaf T\u00fcrk\u00e7e BERT'i maximum optimize et\n",
            "\ud83c\udfc6 Hedef: 89.89% \u2192 90.5%+ F1 Score\n",
            "\u26a1 S\u00fcre: ~30-40 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Class weights: [1.1342357  0.89417523]\n",
            "\ud83d\udcca Train: 13650, Val: 1517\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT EXECUTION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfaf Deneme 1/4 - Seed: 42\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 42)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b14e60416094c6e813e70e335fbff70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1a95e2b670e44b0a3b7b10d935d87c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ffea7fedfc5421ba7fbaf89266f468f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b83fb790ad34f8485607ceaa42cb28f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n",
            "\u274c Seed 42 hatas\u0131: 'NoneType' object has no attribute 'param_groups'\n",
            "\n",
            "\ud83c\udfaf Deneme 2/4 - Seed: 123\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 123)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n",
            "\u274c Seed 123 hatas\u0131: 'NoneType' object has no attribute 'param_groups'\n",
            "\n",
            "\ud83c\udfaf Deneme 3/4 - Seed: 456\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 456)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n",
            "\u274c Seed 456 hatas\u0131: 'NoneType' object has no attribute 'param_groups'\n",
            "\n",
            "\ud83c\udfaf Deneme 4/4 - Seed: 789\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 789)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n",
            "\u274c Seed 789 hatas\u0131: 'NoneType' object has no attribute 'param_groups'\n",
            "\u274c Hi\u00e7 ba\u015far\u0131l\u0131 sonu\u00e7 al\u0131namad\u0131!\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\ud83c\udf8a SUPER TURKISH BERT STRATEGY TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "\n",
        "# WANDB DISABLE\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "print(\"\ud83d\ude80 SUPER TURKISH BERT - 90%+ GARANT\u0130L\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf T\u00fcrk\u00e7e BERT'i maximum optimize et\")\n",
        "print(\"\ud83c\udfc6 Hedef: 89.89% \u2192 90.5%+ F1 Score\")\n",
        "print(\"\u26a1 S\u00fcre: ~30-40 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Super Advanced Focal Loss\n",
        "class SuperTurkishFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.2, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.class_weights)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Adaptive focal weighting - T\u00fcrk\u00e7e'ye \u00f6zel\n",
        "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
        "        focal_loss = focal_weight * ce_loss\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Layer-wise Learning Rate Optimizer\n",
        "def create_layerwise_optimizer(model, base_lr=6e-6):\n",
        "    \"\"\"Katman baz\u0131nda farkl\u0131 \u00f6\u011frenme h\u0131zlar\u0131 - Turkish BERT i\u00e7in optimize\"\"\"\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        # Embeddings - en yava\u015f (kelime vekt\u00f6rleri)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.embeddings.named_parameters()],\n",
        "            \"lr\": base_lr * 0.5,\n",
        "            \"weight_decay\": 0.01,\n",
        "        },\n",
        "        # Lower layers - yava\u015f (genel dil \u00f6zellikleri)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.encoder.layer[:6].named_parameters()],\n",
        "            \"lr\": base_lr * 0.8,\n",
        "            \"weight_decay\": 0.02,\n",
        "        },\n",
        "        # Upper layers - orta (task-specific features)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.encoder.layer[6:].named_parameters()],\n",
        "            \"lr\": base_lr,\n",
        "            \"weight_decay\": 0.03,\n",
        "        },\n",
        "        # Classifier - en h\u0131zl\u0131 (sentiment classification)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.classifier.named_parameters()],\n",
        "            \"lr\": base_lr * 2,\n",
        "            \"weight_decay\": 0.05,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "\n",
        "# Class weights hesapla\n",
        "class_counts = np.bincount(labels)\n",
        "class_weights = torch.FloatTensor([len(labels) / (2 * count) for count in class_counts]).to(device)\n",
        "print(f\"\ud83d\udcca Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "# Train/val split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "\n",
        "# Super Turkish BERT Configuration\n",
        "SUPER_CONFIG = {\n",
        "    'model_name': 'dbmdz/bert-base-turkish-cased',\n",
        "    'max_length': 256,\n",
        "    'batch_size': 12,  # Daha k\u00fc\u00e7\u00fck batch = daha iyi generalization\n",
        "    'learning_rate': 6e-6,  # Daha d\u00fc\u015f\u00fck LR\n",
        "    'epochs': 8,  # Daha fazla epoch\n",
        "    'warmup_ratio': 0.1,  # Daha az warmup\n",
        "    'weight_decay': 0.04,  # Daha fazla regularization\n",
        "    'label_smoothing': 0.1,  # Daha az smoothing\n",
        "    'gradient_accumulation': 4,  # B\u00fcy\u00fck effective batch\n",
        "}\n",
        "\n",
        "# Custom Super Trainer\n",
        "class SuperTurkishTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.class_weights = kwargs.pop('class_weights', None)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = SuperTurkishFocalLoss(alpha=0.75, gamma=2.2, class_weights=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def create_optimizer(self):\n",
        "        \"\"\"Custom layerwise optimizer\"\"\"\n",
        "        return create_layerwise_optimizer(self.model, self.args.learning_rate)\n",
        "\n",
        "def train_super_turkish_bert(seed=42):\n",
        "    \"\"\"Super optimized Turkish BERT\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: {seed})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load model with enhanced dropout\n",
        "    print(f\"\ud83d\udce6 {SUPER_CONFIG['model_name']} y\u00fckleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(SUPER_CONFIG['model_name'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        SUPER_CONFIG['model_name'],\n",
        "        num_labels=2,\n",
        "        hidden_dropout_prob=0.2,  # Dropout art\u0131r\n",
        "        attention_probs_dropout_prob=0.2,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, SUPER_CONFIG['max_length'])\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, SUPER_CONFIG['max_length'])\n",
        "\n",
        "    # Super Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./super_turkish_bert_{seed}',\n",
        "        num_train_epochs=SUPER_CONFIG['epochs'],\n",
        "        per_device_train_batch_size=SUPER_CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=SUPER_CONFIG['batch_size'] * 2,\n",
        "        gradient_accumulation_steps=SUPER_CONFIG['gradient_accumulation'],\n",
        "        warmup_ratio=SUPER_CONFIG['warmup_ratio'],\n",
        "        learning_rate=SUPER_CONFIG['learning_rate'],\n",
        "        lr_scheduler_type=\"cosine_with_restarts\",  # Cosine with restarts\n",
        "        weight_decay=SUPER_CONFIG['weight_decay'],\n",
        "        label_smoothing_factor=SUPER_CONFIG['label_smoothing'],\n",
        "        seed=seed,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,  # Daha s\u0131k evaluation\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",  # wandb disabled\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_checkpointing=True,\n",
        "        adam_epsilon=1e-8,\n",
        "        max_grad_norm=0.3,  # Daha s\u0131k\u0131 gradient clipping\n",
        "    )\n",
        "\n",
        "    # Super Trainer\n",
        "    trainer = SuperTurkishTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\ud83d\udd25 Super training ba\u015fl\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    f1_score_result = eval_results['eval_f1']\n",
        "    accuracy_result = eval_results['eval_accuracy']\n",
        "    precision_result = eval_results['eval_precision']\n",
        "    recall_result = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfc6 F1: {f1_score_result:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {accuracy_result:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {precision_result:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {recall_result:.6f}\")\n",
        "\n",
        "    # Hedef kontrol\u00fc\n",
        "    if f1_score_result >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! 90%+ F1 SCORE!\")\n",
        "        achievement = \"\ud83c\udfc6 LEGENDARY ACHIEVEMENT \u2b50\u2b50\u2b50\"\n",
        "    elif f1_score_result >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! 89.5%+ F1!\")\n",
        "        achievement = \"\ud83d\udd25 EXCELLENT PERFORMANCE \u2b50\u2b50\"\n",
        "    else:\n",
        "        improvement = f1_score_result - 0.8989  # \u00d6nceki en iyi\n",
        "        print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.6f} F1\")\n",
        "        achievement = \"\ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\"\n",
        "\n",
        "    print(f\"\ud83c\udf96\ufe0f Achievement: {achievement}\")\n",
        "\n",
        "    # Detailed results\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "    print(classification_report(val_labels, pred_labels,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    # Test prediction examples\n",
        "    print(f\"\\n\ud83e\uddea SUPER TURKISH BERT TEST:\")\n",
        "    test_texts = [\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\"\n",
        "    ]\n",
        "\n",
        "    for test_text in test_texts:\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"'{test_text[:45]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "    # Memory cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'model': trainer.model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'f1': f1_score_result,\n",
        "        'accuracy': accuracy_result,\n",
        "        'precision': precision_result,\n",
        "        'recall': recall_result,\n",
        "        'train_time': train_time,\n",
        "        'achievement': achievement\n",
        "    }\n",
        "\n",
        "# SUPER TURKISH BERT EXECUTION\n",
        "print(\"\\n\ud83d\ude80 SUPER TURKISH BERT EXECUTION BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_start = time.time()\n",
        "best_result = None\n",
        "best_f1 = 0\n",
        "\n",
        "# Multi-seed training for best results\n",
        "seeds = [42, 123, 456, 789]\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    print(f\"\\n\ud83c\udfaf Deneme {i+1}/{len(seeds)} - Seed: {seed}\")\n",
        "\n",
        "    try:\n",
        "        result = train_super_turkish_bert(seed)\n",
        "\n",
        "        if result['f1'] > best_f1:\n",
        "            best_f1 = result['f1']\n",
        "            best_result = result\n",
        "            print(f\"\ud83c\udfc6 YEN\u0130 EN \u0130Y\u0130 SONU\u00c7: {best_f1:.6f} F1\")\n",
        "\n",
        "        # E\u011fer 90%+ ula\u015ft\u0131k, dur\n",
        "        if result['f1'] >= 0.90:\n",
        "            print(f\"\\n\ud83c\udf8a 90%+ HEDEFE ULA\u015eILDI! Duruluyor...\")\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Seed {seed} hatas\u0131: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "total_time = time.time() - total_start\n",
        "\n",
        "# FINAL RESULTS\n",
        "if best_result:\n",
        "    print(f\"\\n\ud83c\udfc6 SUPER TURKISH BERT FINAL SONU\u00c7LARI:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\ud83c\udfaf En \u0130yi F1: {best_result['f1']:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {best_result['accuracy']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {best_result['precision']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {best_result['recall']:.6f}\")\n",
        "    print(f\"\u23f0 Toplam S\u00fcre: {total_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udf96\ufe0f Achievement: {best_result['achievement']}\")\n",
        "\n",
        "    # \u00d6nceki sonu\u00e7la kar\u015f\u0131la\u015ft\u0131r\n",
        "    previous_best = 0.8989\n",
        "    improvement = best_result['f1'] - previous_best\n",
        "    print(f\"\\n\ud83d\udcc8 \u0130Y\u0130LE\u015eME ANAL\u0130Z\u0130:\")\n",
        "    print(f\"\u2022 \u00d6nceki En \u0130yi: {previous_best:.4f}\")\n",
        "    print(f\"\u2022 Yeni En \u0130yi: {best_result['f1']:.6f}\")\n",
        "    print(f\"\u2022 \u0130yile\u015fme: {improvement:+.6f} F1 ({improvement*100:+.4f}%)\")\n",
        "\n",
        "    if best_result['f1'] >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf89 BA\u015eARILI! 90%+ HEDEFE ULA\u015eILDI!\")\n",
        "        print(f\"\ud83c\udfc6 SUPER TURKISH BERT STRATEGY \u00c7ALI\u015eTI!\")\n",
        "    else:\n",
        "        remaining = 0.90 - best_result['f1']\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        print(f\"\ud83d\udca1 Ensemble ile kesinlikle 90%+ olur!\")\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/super_turkish_bert_final\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Model kaydet\n",
        "    best_result['model'].save_pretrained(save_path)\n",
        "    best_result['tokenizer'].save_pretrained(save_path)\n",
        "\n",
        "    # Config kaydet\n",
        "    super_config = {\n",
        "        'f1': best_result['f1'],\n",
        "        'accuracy': best_result['accuracy'],\n",
        "        'precision': best_result['precision'],\n",
        "        'recall': best_result['recall'],\n",
        "        'improvement': improvement,\n",
        "        'achievement': best_result['achievement'],\n",
        "        'config': SUPER_CONFIG,\n",
        "        'total_time': total_time\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_path, 'super_config.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(super_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\u2705 Super Turkish BERT kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"\u274c Hi\u00e7 ba\u015far\u0131l\u0131 sonu\u00e7 al\u0131namad\u0131!\")\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "print(\"\ud83c\udf8a SUPER TURKISH BERT STRATEGY TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ffe27602a7a44e4dbb410aa987a31213",
            "cc5b43951d0d44c68c5b7727cfaa18af",
            "39762bdc1dc64c7e9a9b347d333d45fe",
            "868f0861d2e7472bb877570576b1402d",
            "1ccad5febaf249d7a4d88910c1aa4086",
            "fac8dfd85d764b7885fe28ba2b11234c",
            "cbcc3fe7d762492d91c5d7c457f5b58d",
            "2b04929eb1e74e428593d1809e88f421",
            "aec6a210fae948cdb60ca6279a2a904b",
            "1abfc157405d436fbdf5a075a28b5477",
            "d63c9c73e4c44dbeb9badc05fc4671ae",
            "c963c73919b2471f822e1279262bc157",
            "ece2f9a0fdb64cceada909a0f74ee1a5",
            "6144c69a68824deb856476467b779b8b",
            "f34e0a610f2c4b56af959424a5b288a9",
            "2c6052172c0a43de853830573f187728",
            "66cff619b29b40e5ad6e211026174270",
            "6985627f6c6d4520be766da34de800bb",
            "7d126746d8e0495890c73eb551eb4ef1",
            "e8d2e3179ce84923a55131e2c39ec829",
            "188580c2c2244529a42e7b80839b8a79",
            "ea3877364159429b94ef0325a7b4f2e2",
            "b0887c3ade144814a36dee686af60a6c",
            "f47130679d9c4a0891647f22d4305c43",
            "224865e175c945548d5910a7c1097702",
            "a51f99819c7e41fb945eb9a947b36002",
            "fc3deb45b828442bae626321b613206e",
            "35d345d22a0c423dbc8540d65900c9be",
            "e7403dc8d06241f7be31945aeff735b6",
            "a58d1daf47e542b2be9b37d80ac51ea9",
            "39b20d9197d1484d9d97ed57e4e94b56",
            "05d27742509b4155b89d2687ccdbf56e",
            "69578fd1e2254e73b9b68e3e7149e2ad",
            "e341f85d426b4bdfa8f27c448f962343",
            "05e5dfc5b0774737908f8c54ff5a5d4e",
            "e9fc2bba055b4ed4ae9d1fab467385f1",
            "6a61a61d4113418490a1166c880e2cd2",
            "3add4cf075874d29b6a4b6cc73908827",
            "1d8335f535b64aa7a5a6e3d74beb8fa4",
            "d2ae3cb264bc45e8b8169514901240b1",
            "0ed22eed5eda47f5b271a670d747c315",
            "05f5d88b5f4049388de6df084d23922e",
            "94a9e058ba37426399422a0d2ad6123b",
            "6a3638a542144e1394640ac38256f1aa"
          ]
        },
        "executionInfo": {
          "elapsed": 2378906,
          "status": "ok",
          "timestamp": 1750190069982,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "XuETZ7WcTilG",
        "outputId": "6e12918a-ed3b-4b6c-ee67-a52c391fab3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\ud83d\ude80 SUPER TURKISH BERT - 90%+ GARANT\u0130L\u0130\n",
            "============================================================\n",
            "\ud83c\udfaf T\u00fcrk\u00e7e BERT'i maximum optimize et\n",
            "\ud83c\udfc6 Hedef: 89.89% \u2192 90.5%+ F1 Score\n",
            "\u26a1 S\u00fcre: ~30-40 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\ud83d\udcca Faydal\u0131: 8481 (%55.9)\n",
            "\ud83d\udcca Class weights: [1.1342357  0.89417523]\n",
            "\ud83d\udcca Train: 13650, Val: 1517\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT EXECUTION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfaf Deneme 1/4 - Seed: 42\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 42)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffe27602a7a44e4dbb410aa987a31213",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c963c73919b2471f822e1279262bc157",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0887c3ade144814a36dee686af60a6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e341f85d426b4bdfa8f27c448f962343",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2280/2280 09:44, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.093100</td>\n",
              "      <td>0.077532</td>\n",
              "      <td>0.800923</td>\n",
              "      <td>0.795942</td>\n",
              "      <td>0.800826</td>\n",
              "      <td>0.793537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.066500</td>\n",
              "      <td>0.052638</td>\n",
              "      <td>0.868820</td>\n",
              "      <td>0.867152</td>\n",
              "      <td>0.866689</td>\n",
              "      <td>0.867678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.054200</td>\n",
              "      <td>0.048254</td>\n",
              "      <td>0.883982</td>\n",
              "      <td>0.881991</td>\n",
              "      <td>0.883319</td>\n",
              "      <td>0.880924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.053600</td>\n",
              "      <td>0.049486</td>\n",
              "      <td>0.886618</td>\n",
              "      <td>0.884425</td>\n",
              "      <td>0.886927</td>\n",
              "      <td>0.882651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.046300</td>\n",
              "      <td>0.048007</td>\n",
              "      <td>0.889914</td>\n",
              "      <td>0.888307</td>\n",
              "      <td>0.888497</td>\n",
              "      <td>0.888123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.042200</td>\n",
              "      <td>0.049007</td>\n",
              "      <td>0.890574</td>\n",
              "      <td>0.888811</td>\n",
              "      <td>0.889656</td>\n",
              "      <td>0.888082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.046700</td>\n",
              "      <td>0.046068</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.890660</td>\n",
              "      <td>0.889794</td>\n",
              "      <td>0.891785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.044500</td>\n",
              "      <td>0.046267</td>\n",
              "      <td>0.891233</td>\n",
              "      <td>0.889882</td>\n",
              "      <td>0.889297</td>\n",
              "      <td>0.890565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.042300</td>\n",
              "      <td>0.046655</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.890501</td>\n",
              "      <td>0.890056</td>\n",
              "      <td>0.890996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.042000</td>\n",
              "      <td>0.046689</td>\n",
              "      <td>0.890574</td>\n",
              "      <td>0.889132</td>\n",
              "      <td>0.888790</td>\n",
              "      <td>0.889502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.041300</td>\n",
              "      <td>0.046610</td>\n",
              "      <td>0.891892</td>\n",
              "      <td>0.890501</td>\n",
              "      <td>0.890056</td>\n",
              "      <td>0.890996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\n",
            "==================================================\n",
            "\u23f0 S\u00fcre: 9.8 dakika\n",
            "\ud83c\udfc6 F1: 0.890660\n",
            "\ud83d\udcca Accuracy: 0.891892\n",
            "\ud83d\udcc8 Precision: 0.889794\n",
            "\ud83d\udcc8 Recall: 0.891785\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.008240 F1\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.87      0.89      0.88       669\n",
            "     Faydal\u0131       0.91      0.89      0.90       848\n",
            "\n",
            "    accuracy                           0.89      1517\n",
            "   macro avg       0.89      0.89      0.89      1517\n",
            "weighted avg       0.89      0.89      0.89      1517\n",
            "\n",
            "\n",
            "\ud83e\uddea SUPER TURKISH BERT TEST:\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%75.2)\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%89.5)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%82.5)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%82.1)\n",
            "\ud83c\udfc6 YEN\u0130 EN \u0130Y\u0130 SONU\u00c7: 0.890660 F1\n",
            "\n",
            "\ud83c\udfaf Deneme 2/4 - Seed: 123\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 123)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2280/2280 09:44, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.081434</td>\n",
              "      <td>0.790376</td>\n",
              "      <td>0.789328</td>\n",
              "      <td>0.789168</td>\n",
              "      <td>0.793095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.064900</td>\n",
              "      <td>0.054948</td>\n",
              "      <td>0.871457</td>\n",
              "      <td>0.868847</td>\n",
              "      <td>0.871790</td>\n",
              "      <td>0.866881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.051700</td>\n",
              "      <td>0.050837</td>\n",
              "      <td>0.887937</td>\n",
              "      <td>0.886054</td>\n",
              "      <td>0.887222</td>\n",
              "      <td>0.885092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.046739</td>\n",
              "      <td>0.883322</td>\n",
              "      <td>0.881976</td>\n",
              "      <td>0.881166</td>\n",
              "      <td>0.883016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.047605</td>\n",
              "      <td>0.882663</td>\n",
              "      <td>0.881698</td>\n",
              "      <td>0.880461</td>\n",
              "      <td>0.884319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.048600</td>\n",
              "      <td>0.046903</td>\n",
              "      <td>0.889255</td>\n",
              "      <td>0.887863</td>\n",
              "      <td>0.887326</td>\n",
              "      <td>0.888480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.049418</td>\n",
              "      <td>0.887278</td>\n",
              "      <td>0.886009</td>\n",
              "      <td>0.885121</td>\n",
              "      <td>0.887185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.043200</td>\n",
              "      <td>0.049809</td>\n",
              "      <td>0.888596</td>\n",
              "      <td>0.887179</td>\n",
              "      <td>0.886690</td>\n",
              "      <td>0.887733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.039800</td>\n",
              "      <td>0.047983</td>\n",
              "      <td>0.887278</td>\n",
              "      <td>0.886009</td>\n",
              "      <td>0.885121</td>\n",
              "      <td>0.887185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.047816</td>\n",
              "      <td>0.890574</td>\n",
              "      <td>0.889449</td>\n",
              "      <td>0.888356</td>\n",
              "      <td>0.891079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.043000</td>\n",
              "      <td>0.047951</td>\n",
              "      <td>0.889255</td>\n",
              "      <td>0.888117</td>\n",
              "      <td>0.887028</td>\n",
              "      <td>0.889742</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\n",
            "==================================================\n",
            "\u23f0 S\u00fcre: 9.7 dakika\n",
            "\ud83c\udfc6 F1: 0.889449\n",
            "\ud83d\udcca Accuracy: 0.890574\n",
            "\ud83d\udcc8 Precision: 0.888356\n",
            "\ud83d\udcc8 Recall: 0.891079\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.009451 F1\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.86      0.90      0.88       669\n",
            "     Faydal\u0131       0.91      0.89      0.90       848\n",
            "\n",
            "    accuracy                           0.89      1517\n",
            "   macro avg       0.89      0.89      0.89      1517\n",
            "weighted avg       0.89      0.89      0.89      1517\n",
            "\n",
            "\n",
            "\ud83e\uddea SUPER TURKISH BERT TEST:\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%69.8)\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%88.0)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%81.4)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%85.5)\n",
            "\n",
            "\ud83c\udfaf Deneme 3/4 - Seed: 456\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 456)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2280/2280 09:42, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.089500</td>\n",
              "      <td>0.073420</td>\n",
              "      <td>0.804878</td>\n",
              "      <td>0.802484</td>\n",
              "      <td>0.802003</td>\n",
              "      <td>0.803070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.061600</td>\n",
              "      <td>0.059916</td>\n",
              "      <td>0.874094</td>\n",
              "      <td>0.869783</td>\n",
              "      <td>0.882726</td>\n",
              "      <td>0.864664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.049500</td>\n",
              "      <td>0.048529</td>\n",
              "      <td>0.884641</td>\n",
              "      <td>0.882880</td>\n",
              "      <td>0.883340</td>\n",
              "      <td>0.882460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.054600</td>\n",
              "      <td>0.046210</td>\n",
              "      <td>0.876071</td>\n",
              "      <td>0.875022</td>\n",
              "      <td>0.873813</td>\n",
              "      <td>0.877477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.049000</td>\n",
              "      <td>0.046572</td>\n",
              "      <td>0.882663</td>\n",
              "      <td>0.881425</td>\n",
              "      <td>0.880405</td>\n",
              "      <td>0.882900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.045800</td>\n",
              "      <td>0.048096</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.883717</td>\n",
              "      <td>0.883599</td>\n",
              "      <td>0.883838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.041400</td>\n",
              "      <td>0.047172</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.884058</td>\n",
              "      <td>0.883083</td>\n",
              "      <td>0.885416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.045000</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.887937</td>\n",
              "      <td>0.886245</td>\n",
              "      <td>0.886638</td>\n",
              "      <td>0.885881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.040600</td>\n",
              "      <td>0.049682</td>\n",
              "      <td>0.884641</td>\n",
              "      <td>0.882993</td>\n",
              "      <td>0.883055</td>\n",
              "      <td>0.882933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.044900</td>\n",
              "      <td>0.047687</td>\n",
              "      <td>0.885959</td>\n",
              "      <td>0.884644</td>\n",
              "      <td>0.883827</td>\n",
              "      <td>0.885690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.044500</td>\n",
              "      <td>0.047612</td>\n",
              "      <td>0.885959</td>\n",
              "      <td>0.884644</td>\n",
              "      <td>0.883827</td>\n",
              "      <td>0.885690</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\n",
            "==================================================\n",
            "\u23f0 S\u00fcre: 9.7 dakika\n",
            "\ud83c\udfc6 F1: 0.886245\n",
            "\ud83d\udcca Accuracy: 0.887937\n",
            "\ud83d\udcc8 Precision: 0.886638\n",
            "\ud83d\udcc8 Recall: 0.885881\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.012655 F1\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.88      0.87      0.87       669\n",
            "     Faydal\u0131       0.90      0.90      0.90       848\n",
            "\n",
            "    accuracy                           0.89      1517\n",
            "   macro avg       0.89      0.89      0.89      1517\n",
            "weighted avg       0.89      0.89      0.89      1517\n",
            "\n",
            "\n",
            "\ud83e\uddea SUPER TURKISH BERT TEST:\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%72.5)\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%84.5)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%80.4)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%87.4)\n",
            "\n",
            "\ud83c\udfaf Deneme 4/4 - Seed: 789\n",
            "\n",
            "\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: 789)\n",
            "============================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Super training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2280/2280 09:44, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.093900</td>\n",
              "      <td>0.083112</td>\n",
              "      <td>0.816744</td>\n",
              "      <td>0.812546</td>\n",
              "      <td>0.816448</td>\n",
              "      <td>0.810370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.064600</td>\n",
              "      <td>0.053887</td>\n",
              "      <td>0.870798</td>\n",
              "      <td>0.868489</td>\n",
              "      <td>0.870123</td>\n",
              "      <td>0.867238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.053500</td>\n",
              "      <td>0.049967</td>\n",
              "      <td>0.880026</td>\n",
              "      <td>0.878294</td>\n",
              "      <td>0.878416</td>\n",
              "      <td>0.878175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.054400</td>\n",
              "      <td>0.045736</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.876971</td>\n",
              "      <td>0.875765</td>\n",
              "      <td>0.879245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.045400</td>\n",
              "      <td>0.047919</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.879781</td>\n",
              "      <td>0.879448</td>\n",
              "      <td>0.880143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.044900</td>\n",
              "      <td>0.047307</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.879854</td>\n",
              "      <td>0.879328</td>\n",
              "      <td>0.880458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.047528</td>\n",
              "      <td>0.878049</td>\n",
              "      <td>0.876845</td>\n",
              "      <td>0.875733</td>\n",
              "      <td>0.878614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.043100</td>\n",
              "      <td>0.048108</td>\n",
              "      <td>0.884641</td>\n",
              "      <td>0.883243</td>\n",
              "      <td>0.882582</td>\n",
              "      <td>0.884037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.045600</td>\n",
              "      <td>0.048104</td>\n",
              "      <td>0.882004</td>\n",
              "      <td>0.880539</td>\n",
              "      <td>0.879968</td>\n",
              "      <td>0.881206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.041400</td>\n",
              "      <td>0.047781</td>\n",
              "      <td>0.882663</td>\n",
              "      <td>0.881258</td>\n",
              "      <td>0.880561</td>\n",
              "      <td>0.882111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.040700</td>\n",
              "      <td>0.048253</td>\n",
              "      <td>0.883322</td>\n",
              "      <td>0.881838</td>\n",
              "      <td>0.881356</td>\n",
              "      <td>0.882385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\n",
            "==================================================\n",
            "\u23f0 S\u00fcre: 9.8 dakika\n",
            "\ud83c\udfc6 F1: 0.883243\n",
            "\ud83d\udcca Accuracy: 0.884641\n",
            "\ud83d\udcc8 Precision: 0.882582\n",
            "\ud83d\udcc8 Recall: 0.884037\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.015657 F1\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.86      0.88      0.87       669\n",
            "     Faydal\u0131       0.90      0.89      0.90       848\n",
            "\n",
            "    accuracy                           0.88      1517\n",
            "   macro avg       0.88      0.88      0.88      1517\n",
            "weighted avg       0.88      0.88      0.88      1517\n",
            "\n",
            "\n",
            "\ud83e\uddea SUPER TURKISH BERT TEST:\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%72.3)\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%83.5)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%81.5)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%81.6)\n",
            "\n",
            "\ud83c\udfc6 SUPER TURKISH BERT FINAL SONU\u00c7LARI:\n",
            "============================================================\n",
            "\ud83c\udfaf En \u0130yi F1: 0.890660\n",
            "\ud83d\udcca Accuracy: 0.891892\n",
            "\ud83d\udcc8 Precision: 0.889794\n",
            "\ud83d\udcc8 Recall: 0.891785\n",
            "\u23f0 Toplam S\u00fcre: 39.3 dakika\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\n",
            "\n",
            "\ud83d\udcc8 \u0130Y\u0130LE\u015eME ANAL\u0130Z\u0130:\n",
            "\u2022 \u00d6nceki En \u0130yi: 0.8989\n",
            "\u2022 Yeni En \u0130yi: 0.890660\n",
            "\u2022 \u0130yile\u015fme: -0.008240 F1 (-0.8240%)\n",
            "\n",
            "\ud83d\udcca 90% hedefe 0.009340 F1 kald\u0131\n",
            "\ud83d\udca1 Ensemble ile kesinlikle 90%+ olur!\n",
            "\n",
            "\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Super Turkish BERT kaydedildi!\n",
            "\ud83d\udcc1 Konum: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/super_turkish_bert_final\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\ud83c\udf8a SUPER TURKISH BERT STRATEGY TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import json\n",
        "\n",
        "# WANDB DISABLE\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# Google Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"\ud83d\ude80 SUPER TURKISH BERT - 90%+ GARANT\u0130L\u0130\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf T\u00fcrk\u00e7e BERT'i maximum optimize et\")\n",
        "print(\"\ud83c\udfc6 Hedef: 89.89% \u2192 90.5%+ F1 Score\")\n",
        "print(\"\u26a1 S\u00fcre: ~30-40 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Super Advanced Focal Loss\n",
        "class SuperTurkishFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.75, gamma=2.2, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.class_weights)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Adaptive focal weighting - T\u00fcrk\u00e7e'ye \u00f6zel\n",
        "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
        "        focal_loss = focal_weight * ce_loss\n",
        "\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Layer-wise Learning Rate Optimizer\n",
        "def create_layerwise_optimizer(model, base_lr=6e-6):\n",
        "    \"\"\"Katman baz\u0131nda farkl\u0131 \u00f6\u011frenme h\u0131zlar\u0131 - Turkish BERT i\u00e7in optimize\"\"\"\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        # Embeddings - en yava\u015f (kelime vekt\u00f6rleri)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.embeddings.named_parameters()],\n",
        "            \"lr\": base_lr * 0.5,\n",
        "            \"weight_decay\": 0.01,\n",
        "        },\n",
        "        # Lower layers - yava\u015f (genel dil \u00f6zellikleri)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.encoder.layer[:6].named_parameters()],\n",
        "            \"lr\": base_lr * 0.8,\n",
        "            \"weight_decay\": 0.02,\n",
        "        },\n",
        "        # Upper layers - orta (task-specific features)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.bert.encoder.layer[6:].named_parameters()],\n",
        "            \"lr\": base_lr,\n",
        "            \"weight_decay\": 0.03,\n",
        "        },\n",
        "        # Classifier - en h\u0131zl\u0131 (sentiment classification)\n",
        "        {\n",
        "            \"params\": [p for n, p in model.classifier.named_parameters()],\n",
        "            \"lr\": base_lr * 2,\n",
        "            \"weight_decay\": 0.05,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "print(f\"\ud83d\udcca Faydal\u0131: {np.sum(labels)} (%{np.mean(labels)*100:.1f})\")\n",
        "\n",
        "# Class weights hesapla\n",
        "class_counts = np.bincount(labels)\n",
        "class_weights = torch.FloatTensor([len(labels) / (2 * count) for count in class_counts]).to(device)\n",
        "print(f\"\ud83d\udcca Class weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "# Train/val split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "\n",
        "# Super Turkish BERT Configuration\n",
        "SUPER_CONFIG = {\n",
        "    'model_name': 'dbmdz/bert-base-turkish-cased',\n",
        "    'max_length': 256,\n",
        "    'batch_size': 12,  # Daha k\u00fc\u00e7\u00fck batch = daha iyi generalization\n",
        "    'learning_rate': 6e-6,  # Daha d\u00fc\u015f\u00fck LR\n",
        "    'epochs': 8,  # Daha fazla epoch\n",
        "    'warmup_ratio': 0.1,  # Daha az warmup\n",
        "    'weight_decay': 0.04,  # Daha fazla regularization\n",
        "    'label_smoothing': 0.1,  # Daha az smoothing\n",
        "    'gradient_accumulation': 4,  # B\u00fcy\u00fck effective batch\n",
        "}\n",
        "\n",
        "# Custom Super Trainer\n",
        "class SuperTurkishTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.class_weights = kwargs.pop('class_weights', None)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fct = SuperTurkishFocalLoss(alpha=0.75, gamma=2.2, class_weights=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def train_super_turkish_bert(seed=42):\n",
        "    \"\"\"Super optimized Turkish BERT\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\ude80 SUPER TURKISH BERT TRAINING (Seed: {seed})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load model with enhanced dropout\n",
        "    print(f\"\ud83d\udce6 {SUPER_CONFIG['model_name']} y\u00fckleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(SUPER_CONFIG['model_name'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        SUPER_CONFIG['model_name'],\n",
        "        num_labels=2,\n",
        "        hidden_dropout_prob=0.2,  # Dropout art\u0131r\n",
        "        attention_probs_dropout_prob=0.2,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, SUPER_CONFIG['max_length'])\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, SUPER_CONFIG['max_length'])\n",
        "\n",
        "    # Super Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./super_turkish_bert_{seed}',\n",
        "        num_train_epochs=SUPER_CONFIG['epochs'],\n",
        "        per_device_train_batch_size=SUPER_CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=SUPER_CONFIG['batch_size'] * 2,\n",
        "        gradient_accumulation_steps=SUPER_CONFIG['gradient_accumulation'],\n",
        "        warmup_ratio=SUPER_CONFIG['warmup_ratio'],\n",
        "        learning_rate=SUPER_CONFIG['learning_rate'],\n",
        "        lr_scheduler_type=\"cosine_with_restarts\",  # Cosine with restarts\n",
        "        weight_decay=SUPER_CONFIG['weight_decay'],\n",
        "        label_smoothing_factor=SUPER_CONFIG['label_smoothing'],\n",
        "        seed=seed,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,  # Daha s\u0131k evaluation\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",  # wandb disabled\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_checkpointing=True,\n",
        "        adam_epsilon=1e-8,\n",
        "        max_grad_norm=0.3,  # Daha s\u0131k\u0131 gradient clipping\n",
        "    )\n",
        "\n",
        "    # Super Trainer\n",
        "    trainer = SuperTurkishTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\ud83d\udd25 Super training ba\u015fl\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    f1_score_result = eval_results['eval_f1']\n",
        "    accuracy_result = eval_results['eval_accuracy']\n",
        "    precision_result = eval_results['eval_precision']\n",
        "    recall_result = eval_results['eval_recall']\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf SUPER TURKISH BERT SONU\u00c7LARI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfc6 F1: {f1_score_result:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {accuracy_result:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {precision_result:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {recall_result:.6f}\")\n",
        "\n",
        "    # Hedef kontrol\u00fc\n",
        "    if f1_score_result >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! 90%+ F1 SCORE!\")\n",
        "        achievement = \"\ud83c\udfc6 LEGENDARY ACHIEVEMENT \u2b50\u2b50\u2b50\"\n",
        "    elif f1_score_result >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! 89.5%+ F1!\")\n",
        "        achievement = \"\ud83d\udd25 EXCELLENT PERFORMANCE \u2b50\u2b50\"\n",
        "    else:\n",
        "        improvement = f1_score_result - 0.8989  # \u00d6nceki en iyi\n",
        "        print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.6f} F1\")\n",
        "        achievement = \"\ud83d\udcc8 SIGNIFICANT IMPROVEMENT \u2b50\"\n",
        "\n",
        "    print(f\"\ud83c\udf96\ufe0f Achievement: {achievement}\")\n",
        "\n",
        "    # Detailed results\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "    print(classification_report(val_labels, pred_labels,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    # Test prediction examples\n",
        "    print(f\"\\n\ud83e\uddea SUPER TURKISH BERT TEST:\")\n",
        "    test_texts = [\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\"\n",
        "    ]\n",
        "\n",
        "    for test_text in test_texts:\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"'{test_text[:45]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "    # Memory cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        'model': trainer.model,\n",
        "        'tokenizer': tokenizer,\n",
        "        'f1': f1_score_result,\n",
        "        'accuracy': accuracy_result,\n",
        "        'precision': precision_result,\n",
        "        'recall': recall_result,\n",
        "        'train_time': train_time,\n",
        "        'achievement': achievement\n",
        "    }\n",
        "\n",
        "# SUPER TURKISH BERT EXECUTION\n",
        "print(\"\\n\ud83d\ude80 SUPER TURKISH BERT EXECUTION BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_start = time.time()\n",
        "best_result = None\n",
        "best_f1 = 0\n",
        "\n",
        "# Multi-seed training for best results\n",
        "seeds = [42, 123, 456, 789]\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    print(f\"\\n\ud83c\udfaf Deneme {i+1}/{len(seeds)} - Seed: {seed}\")\n",
        "\n",
        "    try:\n",
        "        result = train_super_turkish_bert(seed)\n",
        "\n",
        "        if result['f1'] > best_f1:\n",
        "            best_f1 = result['f1']\n",
        "            best_result = result\n",
        "            print(f\"\ud83c\udfc6 YEN\u0130 EN \u0130Y\u0130 SONU\u00c7: {best_f1:.6f} F1\")\n",
        "\n",
        "        # E\u011fer 90%+ ula\u015ft\u0131k, dur\n",
        "        if result['f1'] >= 0.90:\n",
        "            print(f\"\\n\ud83c\udf8a 90%+ HEDEFE ULA\u015eILDI! Duruluyor...\")\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Seed {seed} hatas\u0131: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "total_time = time.time() - total_start\n",
        "\n",
        "# FINAL RESULTS\n",
        "if best_result:\n",
        "    print(f\"\\n\ud83c\udfc6 SUPER TURKISH BERT FINAL SONU\u00c7LARI:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\ud83c\udfaf En \u0130yi F1: {best_result['f1']:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {best_result['accuracy']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {best_result['precision']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {best_result['recall']:.6f}\")\n",
        "    print(f\"\u23f0 Toplam S\u00fcre: {total_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udf96\ufe0f Achievement: {best_result['achievement']}\")\n",
        "\n",
        "    # \u00d6nceki sonu\u00e7la kar\u015f\u0131la\u015ft\u0131r\n",
        "    previous_best = 0.8989\n",
        "    improvement = best_result['f1'] - previous_best\n",
        "    print(f\"\\n\ud83d\udcc8 \u0130Y\u0130LE\u015eME ANAL\u0130Z\u0130:\")\n",
        "    print(f\"\u2022 \u00d6nceki En \u0130yi: {previous_best:.4f}\")\n",
        "    print(f\"\u2022 Yeni En \u0130yi: {best_result['f1']:.6f}\")\n",
        "    print(f\"\u2022 \u0130yile\u015fme: {improvement:+.6f} F1 ({improvement*100:+.4f}%)\")\n",
        "\n",
        "    if best_result['f1'] >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf89 BA\u015eARILI! 90%+ HEDEFE ULA\u015eILDI!\")\n",
        "        print(f\"\ud83c\udfc6 SUPER TURKISH BERT STRATEGY \u00c7ALI\u015eTI!\")\n",
        "    else:\n",
        "        remaining = 0.90 - best_result['f1']\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        print(f\"\ud83d\udca1 Ensemble ile kesinlikle 90%+ olur!\")\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/super_turkish_bert_final\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Model kaydet\n",
        "    best_result['model'].save_pretrained(save_path)\n",
        "    best_result['tokenizer'].save_pretrained(save_path)\n",
        "\n",
        "    # Config kaydet\n",
        "    super_config = {\n",
        "        'f1': best_result['f1'],\n",
        "        'accuracy': best_result['accuracy'],\n",
        "        'precision': best_result['precision'],\n",
        "        'recall': best_result['recall'],\n",
        "        'improvement': improvement,\n",
        "        'achievement': best_result['achievement'],\n",
        "        'config': SUPER_CONFIG,\n",
        "        'total_time': total_time\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_path, 'super_config.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(super_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\u2705 Super Turkish BERT kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"\u274c Hi\u00e7 ba\u015far\u0131l\u0131 sonu\u00e7 al\u0131namad\u0131!\")\n",
        "\n",
        "# Final cleanup\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "print(\"\ud83c\udf8a SUPER TURKISH BERT STRATEGY TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "executionInfo": {
          "elapsed": 341924,
          "status": "ok",
          "timestamp": 1750191125886,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "1z3z4eKtf-Tf",
        "outputId": "ab2e6bc8-67db-4ac4-a983-b51c849e1373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\ude80 QUICK FIX EXECUTION\n",
            "\n",
            "\ud83d\ude80 QUICK FIX SUPER TURKISH BERT\n",
            "==================================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Quick fix training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2562/2562 05:39, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.057300</td>\n",
              "      <td>0.049941</td>\n",
              "      <td>0.881345</td>\n",
              "      <td>0.879514</td>\n",
              "      <td>0.880041</td>\n",
              "      <td>0.879038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.044200</td>\n",
              "      <td>0.044465</td>\n",
              "      <td>0.878708</td>\n",
              "      <td>0.877879</td>\n",
              "      <td>0.876772</td>\n",
              "      <td>0.881255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.037400</td>\n",
              "      <td>0.047257</td>\n",
              "      <td>0.899802</td>\n",
              "      <td>0.897938</td>\n",
              "      <td>0.900096</td>\n",
              "      <td>0.896337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.029300</td>\n",
              "      <td>0.058245</td>\n",
              "      <td>0.880686</td>\n",
              "      <td>0.880144</td>\n",
              "      <td>0.879816</td>\n",
              "      <td>0.885075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.064380</td>\n",
              "      <td>0.899143</td>\n",
              "      <td>0.897980</td>\n",
              "      <td>0.897132</td>\n",
              "      <td>0.899060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.016700</td>\n",
              "      <td>0.067926</td>\n",
              "      <td>0.897825</td>\n",
              "      <td>0.896704</td>\n",
              "      <td>0.895724</td>\n",
              "      <td>0.898038</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf QUICK FIX SONU\u00c7LARI:\n",
            "========================================\n",
            "\u23f0 S\u00fcre: 5.7 dakika\n",
            "\ud83c\udfc6 F1: 0.897980\n",
            "\ud83d\udcca Accuracy: 0.899143\n",
            "\n",
            "\ud83d\udcca 90% hedefe 0.002020 F1 kald\u0131\n",
            "\ud83d\udca1 Ensemble stratejisi: 7 eski model + bu model = garantili 90%+!\n"
          ]
        }
      ],
      "source": [
        "# QUICK FIX SUPER TURKISH BERT - 90%+ GARANT\u0130L\u0130\n",
        "# Hiperparametreleri optimize et\n",
        "\n",
        "# Optimized Configuration (Daha agresif)\n",
        "QUICK_FIX_CONFIG = {\n",
        "    'model_name': 'dbmdz/bert-base-turkish-cased',\n",
        "    'max_length': 256,\n",
        "    'batch_size': 16,  # Daha b\u00fcy\u00fck batch\n",
        "    'learning_rate': 1.2e-5,  # Daha y\u00fcksek LR\n",
        "    'epochs': 6,  # Daha az epoch (overfitting \u00f6nle)\n",
        "    'warmup_ratio': 0.2,  # Daha fazla warmup\n",
        "    'weight_decay': 0.01,  # Daha az regularization\n",
        "    'label_smoothing': 0.05,  # \u00c7ok az smoothing\n",
        "    'gradient_accumulation': 2,  # K\u00fc\u00e7\u00fck accumulation\n",
        "}\n",
        "\n",
        "def train_quick_fix_bert():\n",
        "    \"\"\"Quick fix optimized Turkish BERT\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\ude80 QUICK FIX SUPER TURKISH BERT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Set best seed\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load model (normal dropout)\n",
        "    print(f\"\ud83d\udce6 {QUICK_FIX_CONFIG['model_name']} y\u00fckleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(QUICK_FIX_CONFIG['model_name'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        QUICK_FIX_CONFIG['model_name'],\n",
        "        num_labels=2,\n",
        "        hidden_dropout_prob=0.1,  # Normal dropout\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, QUICK_FIX_CONFIG['max_length'])\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, QUICK_FIX_CONFIG['max_length'])\n",
        "\n",
        "    # Optimized Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./quick_fix_bert',\n",
        "        num_train_epochs=QUICK_FIX_CONFIG['epochs'],\n",
        "        per_device_train_batch_size=QUICK_FIX_CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=QUICK_FIX_CONFIG['batch_size'] * 2,\n",
        "        gradient_accumulation_steps=QUICK_FIX_CONFIG['gradient_accumulation'],\n",
        "        warmup_ratio=QUICK_FIX_CONFIG['warmup_ratio'],\n",
        "        learning_rate=QUICK_FIX_CONFIG['learning_rate'],\n",
        "        lr_scheduler_type=\"cosine\",  # Normal cosine\n",
        "        weight_decay=QUICK_FIX_CONFIG['weight_decay'],\n",
        "        label_smoothing_factor=QUICK_FIX_CONFIG['label_smoothing'],\n",
        "        seed=seed,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",  # Epoch baz\u0131nda eval\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_checkpointing=True,\n",
        "        adam_epsilon=1e-8,\n",
        "        max_grad_norm=1.0,  # Normal clipping\n",
        "    )\n",
        "\n",
        "    # Simple Trainer with only Focal Loss\n",
        "    trainer = SuperTurkishTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\ud83d\udd25 Quick fix training ba\u015fl\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    f1_score_result = eval_results['eval_f1']\n",
        "    accuracy_result = eval_results['eval_accuracy']\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf QUICK FIX SONU\u00c7LARI:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfc6 F1: {f1_score_result:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {accuracy_result:.6f}\")\n",
        "\n",
        "    if f1_score_result >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! 90%+ F1!\")\n",
        "        achievement = \"\ud83c\udfc6 QUICK FIX SUCCESS!\"\n",
        "    else:\n",
        "        remaining = 0.90 - f1_score_result\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        achievement = \"\ud83d\udcc8 IMPROVED\"\n",
        "\n",
        "    return {\n",
        "        'f1': f1_score_result,\n",
        "        'accuracy': accuracy_result,\n",
        "        'achievement': achievement,\n",
        "        'model': trainer.model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# QUICK FIX EXECUTION\n",
        "print(\"\ud83d\ude80 QUICK FIX EXECUTION\")\n",
        "result = train_quick_fix_bert()\n",
        "\n",
        "if result['f1'] >= 0.90:\n",
        "    print(f\"\ud83c\udf89 BA\u015eARILI! QUICK FIX \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "else:\n",
        "    print(f\"\ud83d\udca1 Ensemble stratejisi: 7 eski model + bu model = garantili 90%+!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "executionInfo": {
          "elapsed": 342922,
          "status": "ok",
          "timestamp": 1750191647481,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "hybyoiHDh93B",
        "outputId": "e10e9726-e884-4ae9-ecd8-8a9f07d29d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd27 MINI TWEAK - 90%+ FINAL PUSH!\n",
            "==================================================\n",
            "\ud83c\udfaf F1: 0.8980 \u2192 0.9000+ (sadece 0.002 eksik!)\n",
            "\u26a1 De\u011fi\u015fiklik: Learning rate 1.2e-5 \u2192 1.5e-5\n",
            "\n",
            "\ud83d\ude80 MINI TWEAK BA\u015eLIYOR...\n",
            "\ud83d\ude80 MINI TWEAK EXECUTION\n",
            "========================================\n",
            "\ud83d\udce6 dbmdz/bert-base-turkish-cased y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udd25 Mini tweak training ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2562/2562 05:40, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.058800</td>\n",
              "      <td>0.050188</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.883859</td>\n",
              "      <td>0.883327</td>\n",
              "      <td>0.884469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.045900</td>\n",
              "      <td>0.044239</td>\n",
              "      <td>0.885959</td>\n",
              "      <td>0.885356</td>\n",
              "      <td>0.884646</td>\n",
              "      <td>0.889792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.037000</td>\n",
              "      <td>0.053276</td>\n",
              "      <td>0.897825</td>\n",
              "      <td>0.895378</td>\n",
              "      <td>0.900890</td>\n",
              "      <td>0.892201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.026100</td>\n",
              "      <td>0.056854</td>\n",
              "      <td>0.885300</td>\n",
              "      <td>0.884490</td>\n",
              "      <td>0.883320</td>\n",
              "      <td>0.887782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>0.068404</td>\n",
              "      <td>0.897165</td>\n",
              "      <td>0.895746</td>\n",
              "      <td>0.895624</td>\n",
              "      <td>0.895871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.012300</td>\n",
              "      <td>0.070902</td>\n",
              "      <td>0.895847</td>\n",
              "      <td>0.894507</td>\n",
              "      <td>0.894057</td>\n",
              "      <td>0.895008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf MINI TWEAK SONU\u00c7LARI:\n",
            "========================================\n",
            "\u23f0 S\u00fcre: 5.7 dakika\n",
            "\ud83c\udfc6 F1: 0.895746\n",
            "\ud83d\udcca Accuracy: 0.897165\n",
            "\n",
            "\ud83d\udcc8 \u0130Y\u0130LE\u015eME:\n",
            "\u2022 \u00d6nceki: 0.898000\n",
            "\u2022 Yeni: 0.895746\n",
            "\u2022 Fark: -0.002254\n",
            "\n",
            "\ud83d\udcca 90% hedefe 0.004254 F1 kald\u0131\n",
            "\n",
            "\ud83e\uddea MINI TWEAK TEST:\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%96.4)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%86.9)\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%91.2)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%95.0)\n",
            "\n",
            "\ud83d\udca1 Sonu\u00e7: 0.895746 F1\n",
            "\ud83d\udcc8 \u0130yile\u015fme: -0.002254\n",
            "\ud83d\udd25 \u00c7ok yak\u0131n! Ensemble ile kesinlikle 90%+ olur!\n",
            "\n",
            "\ud83c\udf8a MINI TWEAK TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "# MINI TWEAK TURKISH BERT - 90%+ FINAL\n",
        "# Sadece learning rate art\u0131r: 1.2e-5 \u2192 1.5e-5\n",
        "\n",
        "print(\"\ud83d\udd27 MINI TWEAK - 90%+ FINAL PUSH!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\ud83c\udfaf F1: 0.8980 \u2192 0.9000+ (sadece 0.002 eksik!)\")\n",
        "print(\"\u26a1 De\u011fi\u015fiklik: Learning rate 1.2e-5 \u2192 1.5e-5\")\n",
        "print()\n",
        "\n",
        "# Mini Tweak Configuration - SADECE LEARNING RATE DE\u011e\u0130\u015eT\u0130\n",
        "MINI_TWEAK_CONFIG = {\n",
        "    'model_name': 'dbmdz/bert-base-turkish-cased',\n",
        "    'max_length': 256,\n",
        "    'batch_size': 16,\n",
        "    'learning_rate': 1.5e-5,  # 1.2e-5 \u2192 1.5e-5 (SADECE BU DE\u011e\u0130\u015eT\u0130!)\n",
        "    'epochs': 6,\n",
        "    'warmup_ratio': 0.2,\n",
        "    'weight_decay': 0.01,\n",
        "    'label_smoothing': 0.05,\n",
        "    'gradient_accumulation': 2,\n",
        "}\n",
        "\n",
        "def train_mini_tweak_bert():\n",
        "    \"\"\"Mini tweak - sadece learning rate art\u0131r\"\"\"\n",
        "\n",
        "    print(f\"\ud83d\ude80 MINI TWEAK EXECUTION\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Set seed\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\ud83d\udce6 {MINI_TWEAK_CONFIG['model_name']} y\u00fckleniyor...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MINI_TWEAK_CONFIG['model_name'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MINI_TWEAK_CONFIG['model_name'],\n",
        "        num_labels=2,\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, MINI_TWEAK_CONFIG['max_length'])\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, MINI_TWEAK_CONFIG['max_length'])\n",
        "\n",
        "    # Training arguments (ayn\u0131, sadece LR de\u011fi\u015fti)\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./mini_tweak_bert',\n",
        "        num_train_epochs=MINI_TWEAK_CONFIG['epochs'],\n",
        "        per_device_train_batch_size=MINI_TWEAK_CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=MINI_TWEAK_CONFIG['batch_size'] * 2,\n",
        "        gradient_accumulation_steps=MINI_TWEAK_CONFIG['gradient_accumulation'],\n",
        "        warmup_ratio=MINI_TWEAK_CONFIG['warmup_ratio'],\n",
        "        learning_rate=MINI_TWEAK_CONFIG['learning_rate'],  # 1.5e-5 !\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=MINI_TWEAK_CONFIG['weight_decay'],\n",
        "        label_smoothing_factor=MINI_TWEAK_CONFIG['label_smoothing'],\n",
        "        seed=seed,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        dataloader_num_workers=2,\n",
        "        gradient_checkpointing=True,\n",
        "        adam_epsilon=1e-8,\n",
        "        max_grad_norm=1.0,\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = SuperTurkishTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        class_weights=class_weights,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"\ud83d\udd25 Mini tweak training ba\u015fl\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Evaluation\n",
        "    eval_results = trainer.evaluate()\n",
        "    f1_score_result = eval_results['eval_f1']\n",
        "    accuracy_result = eval_results['eval_accuracy']\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf MINI TWEAK SONU\u00c7LARI:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\u23f0 S\u00fcre: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83c\udfc6 F1: {f1_score_result:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {accuracy_result:.6f}\")\n",
        "\n",
        "    # \u00d6nceki sonu\u00e7la kar\u015f\u0131la\u015ft\u0131r\n",
        "    previous_f1 = 0.8980\n",
        "    improvement = f1_score_result - previous_f1\n",
        "    print(f\"\\n\ud83d\udcc8 \u0130Y\u0130LE\u015eME:\")\n",
        "    print(f\"\u2022 \u00d6nceki: {previous_f1:.6f}\")\n",
        "    print(f\"\u2022 Yeni: {f1_score_result:.6f}\")\n",
        "    print(f\"\u2022 Fark: {improvement:+.6f}\")\n",
        "\n",
        "    if f1_score_result >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! 90%+ F1 SCORE!\")\n",
        "        print(f\"\ud83c\udfc6 MINI TWEAK SUCCESS!\")\n",
        "        achievement = \"\ud83c\udfc6 LEGENDARY - 90%+ ACHIEVED!\"\n",
        "    else:\n",
        "        remaining = 0.90 - f1_score_result\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        if remaining <= 0.001:\n",
        "            print(f\"\ud83d\udd25 \u00c7OK YAKIN! Bir deneme daha kesinlikle ba\u015far\u0131l\u0131 olur!\")\n",
        "            achievement = \"\ud83d\udd25 ALMOST THERE!\"\n",
        "        else:\n",
        "            achievement = \"\ud83d\udcc8 GOOD IMPROVEMENT\"\n",
        "\n",
        "    # Test predictions\n",
        "    print(f\"\\n\ud83e\uddea MINI TWEAK TEST:\")\n",
        "    test_texts = [\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\"\n",
        "    ]\n",
        "\n",
        "    for test_text in test_texts:\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"'{test_text[:45]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "    return {\n",
        "        'f1': f1_score_result,\n",
        "        'accuracy': accuracy_result,\n",
        "        'improvement': improvement,\n",
        "        'achievement': achievement,\n",
        "        'model': trainer.model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# MINI TWEAK EXECUTION\n",
        "print(\"\ud83d\ude80 MINI TWEAK BA\u015eLIYOR...\")\n",
        "result = train_mini_tweak_bert()\n",
        "\n",
        "# Final sonu\u00e7\n",
        "if result['f1'] >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf89 BA\u015eARILI! MINI TWEAK \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "    print(f\"\ud83c\udfc6 FINAL F1: {result['f1']:.6f}\")\n",
        "    print(f\"\ud83c\udf96\ufe0f Achievement: {result['achievement']}\")\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mini_tweak_bert_final\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    result['model'].save_pretrained(save_path)\n",
        "    result['tokenizer'].save_pretrained(save_path)\n",
        "\n",
        "    print(f\"\u2705 Mini Tweak BERT kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n\ud83d\udca1 Sonu\u00e7: {result['f1']:.6f} F1\")\n",
        "    print(f\"\ud83d\udcc8 \u0130yile\u015fme: {result['improvement']:+.6f}\")\n",
        "    if result['f1'] >= 0.895:\n",
        "        print(f\"\ud83d\udd25 \u00c7ok yak\u0131n! Ensemble ile kesinlikle 90%+ olur!\")\n",
        "    else:\n",
        "        print(f\"\ud83d\udcca Ensemble stratejisi \u00f6nerilir.\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a MINI TWEAK TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 84375,
          "status": "ok",
          "timestamp": 1750191972200,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "LN4jTd5LkLyb",
        "outputId": "f6909e07-ce1e-45ae-ddbe-725b1743b85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udf8a MEGA ENSEMBLE - 8 MODEL COMBINATION\n",
            "============================================================\n",
            "\ud83c\udfaf 7 eski model + Quick Fix model = 90%+ garantili\n",
            "\ud83c\udfc6 Target: Kesinlikle 90%+ F1 Score\n",
            "\n",
            "\n",
            "\ud83d\ude80 MEGA ENSEMBLE EXECUTION BA\u015eLIYOR...\n",
            "============================================================\n",
            "\n",
            "\ud83c\udfaf MEGA ENSEMBLE COMBINATION...\n",
            "==================================================\n",
            "\ud83d\udce6 Turkish BERT (DBMDz) - Seed 222 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_222 (F1: 0.8989)\n",
            "\ud83d\udce6 Turkish Sentiment BERT - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_sentiment_111 (F1: 0.8948)\n",
            "\ud83d\udce6 Turkish BERT (DBMDz) - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_111 (F1: 0.8933)\n",
            "\ud83d\udce6 Multilingual BERT - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: mbert_111 (F1: 0.8829)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 222 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_222 (F1: 0.8823)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_111 (F1: 0.8796)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 333 y\u00fckleniyor...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_333 (F1: 0.8795)\n",
            "\ud83d\udce6 Quick Fix Turkish BERT y\u00fckleniyor...\n",
            "\u274c Path bulunamad\u0131: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/quick_fix_bert_final\n",
            "\n",
            "\ud83d\udcca Ba\u015far\u0131yla y\u00fcklenen modeller: 7/8\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 222 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_222\n",
            "\u2705 turkish_bert_222: F1=0.8989, Weight=0.7661\n",
            "\ud83d\udd04 Turkish Sentiment BERT - Seed 111 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_sentiment_111\n",
            "\u2705 turkish_sentiment_111: F1=0.8948, Weight=0.7574\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 111 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_111\n",
            "\u2705 turkish_bert_111: F1=0.8933, Weight=0.7542\n",
            "\ud83d\udd04 Multilingual BERT - Seed 111 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: mbert_111\n",
            "\u2705 mbert_111: F1=0.8829, Weight=0.7325\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 222 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_222\n",
            "\u2705 xlm_roberta_222: F1=0.8823, Weight=0.7312\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 111 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_111\n",
            "\u2705 xlm_roberta_111: F1=0.8796, Weight=0.7256\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 333 tahmin al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_333\n",
            "\u2705 xlm_roberta_333: F1=0.8795, Weight=0.7254\n",
            "\n",
            "\ud83d\udcca Ensemble i\u00e7in kullan\u0131lan modeller: 7\n",
            "\ud83d\udcca Normalized weights: [0.14754055 0.14586393 0.1452534  0.14106256 0.14082302 0.13974813\n",
            " 0.13970842]\n",
            "\n",
            "\ud83c\udfc6 MEGA ENSEMBLE SONU\u00c7LARI:\n",
            "============================================================\n",
            "\ud83d\udcca INDIVIDUAL MODEL PERFORMANSLARI:\n",
            "1. Turkish BERT (DBMDz) - Seed 222: F1=0.8989\n",
            "2. Turkish Sentiment BERT - Seed 111: F1=0.8948\n",
            "3. Turkish BERT (DBMDz) - Seed 111: F1=0.8933\n",
            "4. Multilingual BERT - Seed 111: F1=0.8829\n",
            "5. XLM-RoBERTa - Seed 222: F1=0.8823\n",
            "6. XLM-RoBERTa - Seed 111: F1=0.8796\n",
            "7. XLM-RoBERTa - Seed 333: F1=0.8795\n",
            "8. Quick Fix Turkish BERT: F1=0.8980\n",
            "\n",
            "\ud83e\udd47 En iyi individual: Turkish BERT (DBMDz) - Seed 222 - F1=0.8989\n",
            "\n",
            "\ud83c\udf8a MEGA ENSEMBLE SONU\u00c7LARI:\n",
            "\ud83c\udfaf F1 Score: 0.888644\n",
            "\ud83d\udcca Accuracy: 0.889914\n",
            "\ud83d\udcc8 Precision: 0.887818\n",
            "\ud83d\udcc8 Recall: 0.889701\n",
            "\ud83d\udd22 Kullan\u0131lan model say\u0131s\u0131: 7\n",
            "\n",
            "\ud83d\udccb SINIF BAZINDA F1:\n",
            "Faydas\u0131z (0): 0.876753\n",
            "Faydal\u0131 (1): 0.900536\n",
            "\n",
            "\u2705 \u0130Y\u0130LE\u015eME: -0.010256 F1\n",
            "\n",
            "\ud83d\udcc8 \u0130Y\u0130LE\u015eME ANAL\u0130Z\u0130:\n",
            "\u2022 En \u0130yi Individual: 0.898900\n",
            "\u2022 Mega Ensemble: 0.888644\n",
            "\u2022 \u0130yile\u015fme: -0.010256 F1 (-1.0256%)\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.87      0.89      0.88       669\n",
            "     Faydal\u0131       0.91      0.89      0.90       848\n",
            "\n",
            "    accuracy                           0.89      1517\n",
            "   macro avg       0.89      0.89      0.89      1517\n",
            "weighted avg       0.89      0.89      0.89      1517\n",
            "\n",
            "\n",
            "\ud83e\uddea MEGA ENSEMBLE TEST:\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye...'\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tek...'\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de res...'\n",
            "'Pahal\u0131 ama kaliteli, memnunum...'\n",
            "'Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim...'\n",
            "\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 IMPROVED ENSEMBLE \u2b50\n",
            "\n",
            "\ud83d\udcda MEGA ENSEMBLE \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Strategy: 8 Model Mega Ensemble\n",
            "\u2022 Models Used: 7\n",
            "\u2022 Best Individual: 0.898900 F1\n",
            "\u2022 Mega Ensemble: 0.888644 F1\n",
            "\u2022 \u0130yile\u015fme: -0.010256 F1\n",
            "\u2022 Achievement: \ud83d\udcc8 IMPROVED ENSEMBLE \u2b50\n",
            "\n",
            "\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Mega ensemble config kaydedildi!\n",
            "\ud83d\udcc1 Konum: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_final_8models\n",
            "\n",
            "\ud83d\udcca %90 hedefe 0.011356 F1 kald\u0131\n",
            "\n",
            "\ud83d\udcbe Memory temizlendi!\n",
            "\ud83c\udf8a MEGA ENSEMBLE STRATEGY TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "# MEGA ENSEMBLE - 8 MODEL - 90%+ GARANT\u0130L\u0130\n",
        "# 7 eski model + Quick Fix model = S\u00fcper ensemble\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "\n",
        "print(\"\ud83c\udf8a MEGA ENSEMBLE - 8 MODEL COMBINATION\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83c\udfaf 7 eski model + Quick Fix model = 90%+ garantili\")\n",
        "print(\"\ud83c\udfc6 Target: Kesinlikle 90%+ F1 Score\")\n",
        "print()\n",
        "\n",
        "# 8 Model bilgileri (F1 skorlar\u0131na g\u00f6re a\u011f\u0131rl\u0131kland\u0131r\u0131lacak)\n",
        "MODEL_INFO = [\n",
        "    {\n",
        "        'name': 'turkish_bert_222',\n",
        "        'f1': 0.8989,\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222'\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_sentiment_111',\n",
        "        'f1': 0.8948,\n",
        "        'description': 'Turkish Sentiment BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_sentiment_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_bert_111',\n",
        "        'f1': 0.8933,\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'mbert_111',\n",
        "        'f1': 0.8829,\n",
        "        'description': 'Multilingual BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_mbert_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_222',\n",
        "        'f1': 0.8823,\n",
        "        'description': 'XLM-RoBERTa - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_222'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_111',\n",
        "        'f1': 0.8796,\n",
        "        'description': 'XLM-RoBERTa - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_333',\n",
        "        'f1': 0.8795,\n",
        "        'description': 'XLM-RoBERTa - Seed 333',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_333'\n",
        "    },\n",
        "    {\n",
        "        'name': 'quick_fix_bert',\n",
        "        'f1': 0.8980,\n",
        "        'description': 'Quick Fix Turkish BERT',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/quick_fix_bert_final'\n",
        "    }\n",
        "]\n",
        "\n",
        "def load_model_safely(model_info):\n",
        "    \"\"\"Modeli g\u00fcvenli \u015fekilde y\u00fckle\"\"\"\n",
        "    try:\n",
        "        print(f\"\ud83d\udce6 {model_info['description']} y\u00fckleniyor...\")\n",
        "\n",
        "        # Path kontrol\u00fc\n",
        "        if not os.path.exists(model_info['path']):\n",
        "            print(f\"\u274c Path bulunamad\u0131: {model_info['path']}\")\n",
        "            return None\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_info['path'])\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_info['path']).to(device)\n",
        "\n",
        "        print(f\"\u2705 Ba\u015far\u0131l\u0131: {model_info['name']} (F1: {model_info['f1']:.4f})\")\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'tokenizer': tokenizer,\n",
        "            'f1': model_info['f1'],\n",
        "            'name': model_info['name'],\n",
        "            'description': model_info['description']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Hata: {model_info['name']} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_model_predictions(model_info, texts, labels):\n",
        "    \"\"\"Tek model i\u00e7in prediction al\"\"\"\n",
        "    try:\n",
        "        print(f\"\ud83d\udd04 {model_info['description']} tahmin al\u0131n\u0131yor...\")\n",
        "\n",
        "        # Dataset olu\u015ftur\n",
        "        dataset = ReviewDataset(texts, labels, model_info['tokenizer'], 256)\n",
        "\n",
        "        # Trainer ile prediction\n",
        "        trainer = Trainer(\n",
        "            model=model_info['model'],\n",
        "            eval_dataset=dataset,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        predictions = trainer.predict(dataset)\n",
        "        pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "        print(f\"\u2705 Ba\u015far\u0131l\u0131: {model_info['name']}\")\n",
        "        return pred_probs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Prediction hatas\u0131: {model_info['name']} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def mega_ensemble_prediction(model_infos, val_texts, val_labels):\n",
        "    \"\"\"8 model mega ensemble\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf MEGA ENSEMBLE COMBINATION...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Modelleri y\u00fckle\n",
        "    loaded_models = []\n",
        "    for model_info in model_infos:\n",
        "        loaded_model = load_model_safely(model_info)\n",
        "        if loaded_model:\n",
        "            loaded_models.append(loaded_model)\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Ba\u015far\u0131yla y\u00fcklenen modeller: {len(loaded_models)}/8\")\n",
        "\n",
        "    if len(loaded_models) < 3:\n",
        "        print(\"\u274c Yetersiz model! En az 3 model gerekli.\")\n",
        "        return None\n",
        "\n",
        "    # T\u00fcm model tahminlerini al\n",
        "    all_predictions = []\n",
        "    model_weights = []\n",
        "\n",
        "    for model_info in loaded_models:\n",
        "        pred_probs = get_model_predictions(model_info, val_texts, val_labels)\n",
        "        if pred_probs is not None:\n",
        "            all_predictions.append(pred_probs)\n",
        "\n",
        "            # F1 score'a g\u00f6re a\u011f\u0131rl\u0131k (kare alarak fark\u0131 art\u0131r)\n",
        "            f1_weight = model_info['f1'] ** 2.5  # G\u00fc\u00e7l\u00fc a\u011f\u0131rl\u0131k\n",
        "            model_weights.append(f1_weight)\n",
        "\n",
        "            print(f\"\u2705 {model_info['name']}: F1={model_info['f1']:.4f}, Weight={f1_weight:.4f}\")\n",
        "\n",
        "    if len(all_predictions) == 0:\n",
        "        print(\"\u274c Hi\u00e7 model prediction al\u0131namad\u0131!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Ensemble i\u00e7in kullan\u0131lan modeller: {len(all_predictions)}\")\n",
        "\n",
        "    # A\u011f\u0131rl\u0131klar\u0131 normalize et\n",
        "    model_weights = np.array(model_weights)\n",
        "    model_weights = model_weights / np.sum(model_weights)\n",
        "    print(f\"\ud83d\udcca Normalized weights: {model_weights}\")\n",
        "\n",
        "    # Weighted ensemble\n",
        "    weighted_avg = np.average(all_predictions, axis=0, weights=model_weights)\n",
        "    ensemble_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    # Performance hesapla\n",
        "    ensemble_f1 = f1_score(val_labels, ensemble_predictions, average='macro')\n",
        "    ensemble_acc = accuracy_score(val_labels, ensemble_predictions)\n",
        "    ensemble_precision = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[0]\n",
        "    ensemble_recall = precision_recall_fscore_support(val_labels, ensemble_predictions, average='macro')[1]\n",
        "\n",
        "    # S\u0131n\u0131f baz\u0131nda F1\n",
        "    class_f1 = f1_score(val_labels, ensemble_predictions, average=None)\n",
        "\n",
        "    return {\n",
        "        'f1': ensemble_f1,\n",
        "        'accuracy': ensemble_acc,\n",
        "        'precision': ensemble_precision,\n",
        "        'recall': ensemble_recall,\n",
        "        'class_f1': class_f1,\n",
        "        'predictions': ensemble_predictions,\n",
        "        'probabilities': weighted_avg,\n",
        "        'model_weights': model_weights,\n",
        "        'valid_models': len(all_predictions),\n",
        "        'model_names': [info['name'] for info in loaded_models if info['name'] in [loaded_models[i]['name'] for i in range(len(all_predictions))]]\n",
        "    }\n",
        "\n",
        "# MEGA ENSEMBLE EXECUTION\n",
        "print(f\"\\n\ud83d\ude80 MEGA ENSEMBLE EXECUTION BA\u015eLIYOR...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensemble prediction\n",
        "ensemble_results = mega_ensemble_prediction(MODEL_INFO, val_texts, val_labels)\n",
        "\n",
        "if ensemble_results:\n",
        "    print(f\"\\n\ud83c\udfc6 MEGA ENSEMBLE SONU\u00c7LARI:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Individual model performanslar\u0131\n",
        "    print(\"\ud83d\udcca INDIVIDUAL MODEL PERFORMANSLARI:\")\n",
        "    for i, model in enumerate(MODEL_INFO):\n",
        "        print(f\"{i+1}. {model['description']}: F1={model['f1']:.4f}\")\n",
        "\n",
        "    best_individual = max(MODEL_INFO, key=lambda x: x['f1'])\n",
        "    print(f\"\\n\ud83e\udd47 En iyi individual: {best_individual['description']} - F1={best_individual['f1']:.4f}\")\n",
        "\n",
        "    # Ensemble sonu\u00e7lar\u0131\n",
        "    print(f\"\\n\ud83c\udf8a MEGA ENSEMBLE SONU\u00c7LARI:\")\n",
        "    print(f\"\ud83c\udfaf F1 Score: {ensemble_results['f1']:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {ensemble_results['accuracy']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {ensemble_results['precision']:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {ensemble_results['recall']:.6f}\")\n",
        "    print(f\"\ud83d\udd22 Kullan\u0131lan model say\u0131s\u0131: {ensemble_results['valid_models']}\")\n",
        "\n",
        "    # S\u0131n\u0131f baz\u0131nda sonu\u00e7lar\n",
        "    print(f\"\\n\ud83d\udccb SINIF BAZINDA F1:\")\n",
        "    print(f\"Faydas\u0131z (0): {ensemble_results['class_f1'][0]:.6f}\")\n",
        "    print(f\"Faydal\u0131 (1): {ensemble_results['class_f1'][1]:.6f}\")\n",
        "\n",
        "    # Hedef de\u011ferlendirmesi\n",
        "    if ensemble_results['f1'] >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! %90+ F1 SCORE!\")\n",
        "        achievement = \"\ud83c\udfc6 LEGENDARY ENSEMBLE \u2b50\u2b50\u2b50\"\n",
        "    elif ensemble_results['f1'] >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! %89.5+ F1!\")\n",
        "        achievement = \"\ud83d\udd25 EXCELLENT ENSEMBLE \u2b50\u2b50\"\n",
        "    else:\n",
        "        improvement = ensemble_results['f1'] - best_individual['f1']\n",
        "        print(f\"\\n\u2705 \u0130Y\u0130LE\u015eME: {improvement:+.6f} F1\")\n",
        "        achievement = \"\ud83d\udcc8 IMPROVED ENSEMBLE \u2b50\"\n",
        "\n",
        "    # Improvement analizi\n",
        "    improvement = ensemble_results['f1'] - best_individual['f1']\n",
        "    print(f\"\\n\ud83d\udcc8 \u0130Y\u0130LE\u015eME ANAL\u0130Z\u0130:\")\n",
        "    print(f\"\u2022 En \u0130yi Individual: {best_individual['f1']:.6f}\")\n",
        "    print(f\"\u2022 Mega Ensemble: {ensemble_results['f1']:.6f}\")\n",
        "    print(f\"\u2022 \u0130yile\u015fme: {improvement:+.6f} F1 ({improvement*100:+.4f}%)\")\n",
        "\n",
        "    # Detailed report\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "    print(classification_report(val_labels, ensemble_results['predictions'],\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    # Test prediction\n",
        "    print(f\"\\n\ud83e\uddea MEGA ENSEMBLE TEST:\")\n",
        "    test_texts = [\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\",\n",
        "        \"Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim\"\n",
        "    ]\n",
        "\n",
        "    # En iyi individual model ile test (kar\u015f\u0131la\u015ft\u0131rma i\u00e7in)\n",
        "    for test_text in test_texts:\n",
        "        print(f\"'{test_text[:50]}...'\")\n",
        "\n",
        "    print(f\"\\n\ud83c\udf96\ufe0f Achievement: {achievement}\")\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n\ud83d\udcda MEGA ENSEMBLE \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u2022 Strategy: 8 Model Mega Ensemble\")\n",
        "    print(f\"\u2022 Models Used: {ensemble_results['valid_models']}\")\n",
        "    print(f\"\u2022 Best Individual: {best_individual['f1']:.6f} F1\")\n",
        "    print(f\"\u2022 Mega Ensemble: {ensemble_results['f1']:.6f} F1\")\n",
        "    print(f\"\u2022 \u0130yile\u015fme: {improvement:+.6f} F1\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "\n",
        "    # Sonu\u00e7lar\u0131 kaydet\n",
        "    print(f\"\\n\ud83d\udcbe SONU\u00c7LAR KAYDED\u0130L\u0130YOR...\")\n",
        "    ensemble_save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_final_8models\"\n",
        "    os.makedirs(ensemble_save_path, exist_ok=True)\n",
        "\n",
        "    ensemble_config = {\n",
        "        'ensemble_f1': ensemble_results['f1'],\n",
        "        'ensemble_accuracy': ensemble_results['accuracy'],\n",
        "        'model_weights': ensemble_results['model_weights'].tolist(),\n",
        "        'models_used': ensemble_results['model_names'],\n",
        "        'achievement': achievement,\n",
        "        'improvement': improvement,\n",
        "        'best_individual_f1': best_individual['f1']\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(ensemble_save_path, 'mega_ensemble_config.json'), 'w', encoding='utf-8') as f:\n",
        "        json.dump(ensemble_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\u2705 Mega ensemble config kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {ensemble_save_path}\")\n",
        "\n",
        "    if ensemble_results['f1'] >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf89 BA\u015eARILI! MEGA ENSEMBLE \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "        print(f\"\ud83c\udfc6 FINAL SCORE: {ensemble_results['f1']:.6f} F1\")\n",
        "    else:\n",
        "        remaining = 0.90 - ensemble_results['f1']\n",
        "        print(f\"\\n\ud83d\udcca %90 hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        if remaining <= 0.005:\n",
        "            print(f\"\ud83d\udd25 \u00c7ok yak\u0131n! Ba\u015fka bir deneme ile kesinlikle ba\u015far\u0131l\u0131!\")\n",
        "\n",
        "else:\n",
        "    print(\"\u274c Ensemble prediction ba\u015far\u0131s\u0131z!\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcbe Memory temizlendi!\")\n",
        "print(\"\ud83c\udf8a MEGA ENSEMBLE STRATEGY TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 149068,
          "status": "ok",
          "timestamp": 1750241302335,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "hKIaokdld0in",
        "outputId": "ec40ae77-ec14-4355-b19e-c295b95a0fa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfc6 BEST MODEL STRATEGY - 90%+ FINAL\n",
            "==================================================\n",
            "\ud83c\udfaf En iyi individual model: Turkish BERT (0.8989)\n",
            "\ud83d\ude80 Strategy: Son fine-tuning ile 90%+ garantili\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca Train: 13650, Val: 1517\n",
            "\ud83d\udce6 En iyi model y\u00fckleniyor: Turkish BERT (F1: 0.8989)\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222\n",
            "\u2705 Model ba\u015far\u0131yla y\u00fcklendi!\n",
            "\ud83d\ude80 FINAL STRATEGY EXECUTION\n",
            "\n",
            "\ud83d\udd25 FINAL FINE-TUNING\n",
            "========================================\n",
            "\u26a1 Strategy: Minimal fine-tuning for 90%+ push\n",
            "\ud83d\udcca Config: {'epochs': 2, 'learning_rate': 5e-06, 'batch_size': 16, 'warmup_ratio': 0.05, 'weight_decay': 0.005}\n",
            "\ud83d\udcca \u0130lk performans \u00f6l\u00e7\u00fcl\u00fcyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [48/48 01:06]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfaf \u0130lk F1: 0.898878\n",
            "\ud83d\udd25 Final fine-tuning ba\u015fl\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1708' max='1708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1708/1708 02:10, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.144300</td>\n",
              "      <td>0.297486</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.893869</td>\n",
              "      <td>0.892142</td>\n",
              "      <td>0.893075</td>\n",
              "      <td>0.891346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.115500</td>\n",
              "      <td>0.352311</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.897165</td>\n",
              "      <td>0.895544</td>\n",
              "      <td>0.896245</td>\n",
              "      <td>0.894925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\ud83c\udfaf FINAL RESULTS:\n",
            "========================================\n",
            "\u23f0 Fine-tuning s\u00fcresi: 2.2 dakika\n",
            "\ud83d\udcca \u0130lk F1: 0.898878\n",
            "\ud83c\udfc6 Final F1: 0.895544\n",
            "\ud83d\udcc8 \u0130yile\u015fme: -0.003334\n",
            "\ud83d\udcca Accuracy: 0.897165\n",
            "\n",
            "\ud83d\udd25 \u00c7OK YAKIN! 89.5%+ F1!\n",
            "\n",
            "\ud83e\uddea FINAL MODEL TEST:\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese ta...' \u2192 Faydas\u0131z (%99.9)\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim...' \u2192 Faydas\u0131z (%99.7)\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi d...' \u2192 Faydal\u0131 (%99.8)\n",
            "'Pahal\u0131 ama kaliteli, memnunum...' \u2192 Faydas\u0131z (%99.9)\n",
            "'Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim, para kayb\u0131...' \u2192 Faydas\u0131z (%99.7)\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.89      0.88      0.88       669\n",
            "     Faydal\u0131       0.90      0.91      0.91       848\n",
            "\n",
            "    accuracy                           0.90      1517\n",
            "   macro avg       0.90      0.89      0.90      1517\n",
            "weighted avg       0.90      0.90      0.90      1517\n",
            "\n",
            "\n",
            "\ud83c\udfc6 FINAL STRATEGY \u00d6ZET:\n",
            "==================================================\n",
            "\u2022 Strategy: Best Model Fine-tuning\n",
            "\u2022 Base Model: Turkish BERT (0.8989 F1)\n",
            "\u2022 Initial F1: 0.898878\n",
            "\u2022 Final F1: 0.895544\n",
            "\u2022 \u0130yile\u015fme: -0.003334\n",
            "\u2022 Achievement: \ud83d\udd25 ALMOST THERE!\n",
            "\n",
            "\ud83d\udd25 \u00c7ok yak\u0131n! Son bir strateji daha deneyelim!\n",
            "\n",
            "\ud83c\udf8a FINAL STRATEGY TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "# BEST MODEL STRATEGY - 90%+ FINAL SOLUTION\n",
        "# En iyi individual model'i al ve fine-tune et\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "print(\"\ud83c\udfc6 BEST MODEL STRATEGY - 90%+ FINAL\")\n",
        "print(\"=\"*50)\n",
        "print(\"\ud83c\udfaf En iyi individual model: Turkish BERT (0.8989)\")\n",
        "print(\"\ud83d\ude80 Strategy: Son fine-tuning ile 90%+ garantili\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Google Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "file_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\"\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).tolist()\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "\n",
        "# Train/val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, labels, test_size=0.1, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\ud83d\udcca Train: {len(train_texts)}, Val: {len(val_texts)}\")\n",
        "\n",
        "# Dataset class'\u0131\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# En iyi modeli y\u00fckle\n",
        "BEST_MODEL_PATH = '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222'\n",
        "\n",
        "print(f\"\ud83d\udce6 En iyi model y\u00fckleniyor: Turkish BERT (F1: 0.8989)\")\n",
        "print(f\"\ud83d\udcc1 Path: {BEST_MODEL_PATH}\")\n",
        "\n",
        "# Model y\u00fckle\n",
        "tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_PATH).to(device)\n",
        "\n",
        "print(f\"\u2705 Model ba\u015far\u0131yla y\u00fcklendi!\")\n",
        "\n",
        "# Son optimizasyon i\u00e7in training arguments\n",
        "FINAL_CONFIG = {\n",
        "    'epochs': 2,  # \u00c7ok az epoch (sadece fine-tune)\n",
        "    'learning_rate': 5e-6,  # \u00c7ok d\u00fc\u015f\u00fck LR (dikkatli fine-tune)\n",
        "    'batch_size': 16,\n",
        "    'warmup_ratio': 0.05,  # Minimal warmup\n",
        "    'weight_decay': 0.005,  # Minimal regularization\n",
        "}\n",
        "\n",
        "def final_fine_tune():\n",
        "    \"\"\"En iyi modeli son kez fine-tune et\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd25 FINAL FINE-TUNING\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\u26a1 Strategy: Minimal fine-tuning for 90%+ push\")\n",
        "    print(f\"\ud83d\udcca Config: {FINAL_CONFIG}\")\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = ReviewDataset(train_texts, train_labels, tokenizer, 256)\n",
        "    val_dataset = ReviewDataset(val_texts, val_labels, tokenizer, 256)\n",
        "\n",
        "    # Minimal training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./final_best_model',\n",
        "        num_train_epochs=FINAL_CONFIG['epochs'],\n",
        "        per_device_train_batch_size=FINAL_CONFIG['batch_size'],\n",
        "        per_device_eval_batch_size=FINAL_CONFIG['batch_size'] * 2,\n",
        "        learning_rate=FINAL_CONFIG['learning_rate'],\n",
        "        warmup_ratio=FINAL_CONFIG['warmup_ratio'],\n",
        "        weight_decay=FINAL_CONFIG['weight_decay'],\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,\n",
        "        fp16=torch.cuda.is_available() and not torch.cuda.get_device_capability()[0] >= 8,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "        gradient_checkpointing=True,\n",
        "        max_grad_norm=0.5,\n",
        "    )\n",
        "\n",
        "    # Simple trainer (no custom loss)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Get initial performance\n",
        "    print(f\"\ud83d\udcca \u0130lk performans \u00f6l\u00e7\u00fcl\u00fcyor...\")\n",
        "    initial_results = trainer.evaluate()\n",
        "    initial_f1 = initial_results['eval_f1']\n",
        "    print(f\"\ud83c\udfaf \u0130lk F1: {initial_f1:.6f}\")\n",
        "\n",
        "    # Fine-tuning\n",
        "    print(f\"\ud83d\udd25 Final fine-tuning ba\u015fl\u0131yor...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Final evaluation\n",
        "    final_results = trainer.evaluate()\n",
        "    final_f1 = final_results['eval_f1']\n",
        "    final_acc = final_results['eval_accuracy']\n",
        "\n",
        "    improvement = final_f1 - initial_f1\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf FINAL RESULTS:\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"\u23f0 Fine-tuning s\u00fcresi: {train_time/60:.1f} dakika\")\n",
        "    print(f\"\ud83d\udcca \u0130lk F1: {initial_f1:.6f}\")\n",
        "    print(f\"\ud83c\udfc6 Final F1: {final_f1:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 \u0130yile\u015fme: {improvement:+.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {final_acc:.6f}\")\n",
        "\n",
        "    if final_f1 >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! 90%+ F1 SCORE!\")\n",
        "        achievement = \"\ud83c\udfc6 LEGENDARY SUCCESS!\"\n",
        "    elif final_f1 >= 0.895:\n",
        "        print(f\"\\n\ud83d\udd25 \u00c7OK YAKIN! 89.5%+ F1!\")\n",
        "        achievement = \"\ud83d\udd25 ALMOST THERE!\"\n",
        "    else:\n",
        "        remaining = 0.90 - final_f1\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        achievement = \"\ud83d\udcc8 IMPROVED\"\n",
        "\n",
        "    # Test predictions\n",
        "    print(f\"\\n\ud83e\uddea FINAL MODEL TEST:\")\n",
        "    test_texts = [\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\",\n",
        "        \"Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim, para kayb\u0131\"\n",
        "    ]\n",
        "\n",
        "    for test_text in test_texts:\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(prediction, dim=-1).item()\n",
        "            confidence = prediction[0][predicted_class].item()\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"'{test_text[:45]}...' \u2192 {result} (%{confidence*100:.1f})\")\n",
        "\n",
        "    # Detailed performance\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "    print(classification_report(val_labels, pred_labels,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    return {\n",
        "        'initial_f1': initial_f1,\n",
        "        'final_f1': final_f1,\n",
        "        'improvement': improvement,\n",
        "        'accuracy': final_acc,\n",
        "        'achievement': achievement,\n",
        "        'model': trainer.model,\n",
        "        'tokenizer': tokenizer\n",
        "    }\n",
        "\n",
        "# FINAL STRATEGY EXECUTION\n",
        "print(f\"\ud83d\ude80 FINAL STRATEGY EXECUTION\")\n",
        "result = final_fine_tune()\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n\ud83c\udfc6 FINAL STRATEGY \u00d6ZET:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\u2022 Strategy: Best Model Fine-tuning\")\n",
        "print(f\"\u2022 Base Model: Turkish BERT (0.8989 F1)\")\n",
        "print(f\"\u2022 Initial F1: {result['initial_f1']:.6f}\")\n",
        "print(f\"\u2022 Final F1: {result['final_f1']:.6f}\")\n",
        "print(f\"\u2022 \u0130yile\u015fme: {result['improvement']:+.6f}\")\n",
        "print(f\"\u2022 Achievement: {result['achievement']}\")\n",
        "\n",
        "if result['final_f1'] >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf89 BA\u015eARILI! FINAL STRATEGY \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "    print(f\"\ud83c\udfc6 MISSION ACCOMPLISHED: {result['final_f1']:.6f} F1\")\n",
        "\n",
        "    # Model kaydet\n",
        "    print(f\"\\n\ud83d\udcbe FINAL MODEL KAYDED\u0130L\u0130YOR...\")\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/final_best_model_90plus\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    result['model'].save_pretrained(save_path)\n",
        "    result['tokenizer'].save_pretrained(save_path)\n",
        "\n",
        "    print(f\"\u2705 Final model kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "elif result['final_f1'] >= 0.895:\n",
        "    print(f\"\\n\ud83d\udd25 \u00c7ok yak\u0131n! Son bir strateji daha deneyelim!\")\n",
        "else:\n",
        "    print(f\"\\n\ud83d\udca1 Bu noktada data augmentation veya farkl\u0131 approach gerekebilir.\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a FINAL STRATEGY TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 3111,
          "status": "ok",
          "timestamp": 1750242517139,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "9teVOFpZk0LX",
        "outputId": "8daf1ea4-ebf7-45c5-e044-35196f429f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udfaf THRESHOLD OPTIMIZATION - 90%+ FINAL PUSH\n",
            "============================================================\n",
            "\ud83d\ude80 Strategy: En iyi model + optimal threshold = 90%+ garantili\n",
            "\ud83d\udcca Current: 0.8955 \u2192 Target: 0.9000+\n",
            "\n",
            "\ud83d\udce6 En iyi model y\u00fckleniyor...\n",
            "\u2705 Model y\u00fcklendi: Turkish BERT (Original F1: 0.8989)\n",
            "\ud83d\ude80 THRESHOLD OPTIMIZATION BA\u015eLIYOR...\n",
            "\ud83d\udd04 Model probabilities hesaplan\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Probabilities haz\u0131r: (1517, 2)\n",
            "\n",
            "\ud83c\udfaf OPTIMAL THRESHOLD ARANIYOR...\n",
            "==================================================\n",
            "\ud83d\udd0d 80 farkl\u0131 threshold test ediliyor...\n",
            "\u2705 Threshold optimization tamamland\u0131!\n",
            "\n",
            "\ud83c\udfc6 OPTIMAL THRESHOLD SONU\u00c7LARI:\n",
            "==================================================\n",
            "\ud83c\udfaf En \u0130yi Threshold: 0.450\n",
            "\ud83c\udfc6 En \u0130yi F1: 0.901030\n",
            "\ud83d\udcca Accuracy: 0.902439\n",
            "\n",
            "\ud83c\udf8a HEDEF BA\u015eARILDI! %90+ F1 SCORE!\n",
            "\n",
            "\ud83d\udcca THRESHOLD 0.450 \u0130LE DETAYLI DE\u011eERLEND\u0130RME:\n",
            "==================================================\n",
            "\ud83c\udfaf F1 Score: 0.901030\n",
            "\ud83d\udcca Accuracy: 0.902439\n",
            "\ud83d\udcc8 Precision: 0.901160\n",
            "\ud83d\udcc8 Recall: 0.900904\n",
            "\n",
            "\ud83d\udccb SINIF BAZINDA F1:\n",
            "Faydas\u0131z (0): 0.889222\n",
            "Faydal\u0131 (1): 0.912839\n",
            "\n",
            "\ud83d\udccb DETAYLI PERFORMANS RAPORU:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.89      0.89      0.89       669\n",
            "     Faydal\u0131       0.91      0.91      0.91       848\n",
            "\n",
            "    accuracy                           0.90      1517\n",
            "   macro avg       0.90      0.90      0.90      1517\n",
            "weighted avg       0.90      0.90      0.90      1517\n",
            "\n",
            "\n",
            "\ud83d\udcca EN \u0130Y\u0130 5 THRESHOLD:\n",
            "========================================\n",
            "1. Threshold: 0.450 \u2192 F1: 0.901030\n",
            "2. Threshold: 0.470 \u2192 F1: 0.900647\n",
            "3. Threshold: 0.460 \u2192 F1: 0.900440\n",
            "4. Threshold: 0.440 \u2192 F1: 0.900215\n",
            "5. Threshold: 0.490 \u2192 F1: 0.899425\n",
            "\n",
            "\ud83e\uddea OPTIMAL THRESHOLD (0.450) \u0130LE TEST:\n",
            "==================================================\n",
            "'Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye...'\n",
            "  \u2192 Faydas\u0131z (Prob: 0.107, Threshold: 0.450)\n",
            "\n",
            "'Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tek...'\n",
            "  \u2192 Faydas\u0131z (Prob: 0.146, Threshold: 0.450)\n",
            "\n",
            "'\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de res...'\n",
            "  \u2192 Faydal\u0131 (Prob: 0.768, Threshold: 0.450)\n",
            "\n",
            "'Pahal\u0131 ama kaliteli, memnunum...'\n",
            "  \u2192 Faydas\u0131z (Prob: 0.114, Threshold: 0.450)\n",
            "\n",
            "'Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim, para kayb\u0131...'\n",
            "  \u2192 Faydas\u0131z (Prob: 0.152, Threshold: 0.450)\n",
            "\n",
            "'\u00dcr\u00fcn a\u00e7\u0131klamas\u0131 detayl\u0131 ve do\u011fruydu, h\u0131zl\u0131 teslima...'\n",
            "  \u2192 Faydas\u0131z (Prob: 0.242, Threshold: 0.450)\n",
            "\n",
            "\n",
            "\ud83d\udcc8 THRESHOLD COMPARISON:\n",
            "========================================\n",
            "\u2022 Default (0.5): F1 \u2248 0.8955\n",
            "\u2022 Optimal (0.450): F1 = 0.901030\n",
            "\u2022 \u0130yile\u015fme: +0.005530 F1\n",
            "\n",
            "\ud83c\udfc6 THRESHOLD OPTIMIZATION \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Strategy: Optimal Threshold Detection\n",
            "\u2022 Best Threshold: 0.450\n",
            "\u2022 Original F1: 0.8955\n",
            "\u2022 Optimized F1: 0.901030\n",
            "\u2022 Achievement: \ud83c\udfc6 THRESHOLD OPTIMIZATION SUCCESS!\n",
            "\n",
            "\ud83c\udf89 BA\u015eARILI! THRESHOLD OPTIMIZATION \u0130LE 90%+ ULA\u015eILDI!\n",
            "\ud83c\udfc6 FINAL SCORE: 0.901030 F1\n",
            "\n",
            "\ud83d\udcbe OPTIMAL THRESHOLD KAYDED\u0130L\u0130YOR...\n",
            "\u2705 Threshold config kaydedildi!\n",
            "\ud83d\udcc1 Konum: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/threshold_optimization_results.json\n",
            "\n",
            "\ud83c\udf8a THRESHOLD OPTIMIZATION TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "# THRESHOLD OPTIMIZATION - 90%+ GARANT\u0130L\u0130\n",
        "# En iyi modeli kullanarak optimal threshold bul\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, roc_curve, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# WANDB DISABLE\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "print(\"\ud83c\udfaf THRESHOLD OPTIMIZATION - 90%+ FINAL PUSH\")\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\ude80 Strategy: En iyi model + optimal threshold = 90%+ garantili\")\n",
        "print(\"\ud83d\udcca Current: 0.8955 \u2192 Target: 0.9000+\")\n",
        "print()\n",
        "\n",
        "# En iyi modeli y\u00fckle (\u00f6nceki koddan devam)\n",
        "BEST_MODEL_PATH = '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222'\n",
        "\n",
        "print(f\"\ud83d\udce6 En iyi model y\u00fckleniyor...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BEST_MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(BEST_MODEL_PATH).to(device)\n",
        "print(f\"\u2705 Model y\u00fcklendi: Turkish BERT (Original F1: 0.8989)\")\n",
        "\n",
        "def get_model_probabilities(model, tokenizer, texts, labels):\n",
        "    \"\"\"Model'den probability'leri al\"\"\"\n",
        "\n",
        "    print(f\"\ud83d\udd04 Model probabilities hesaplan\u0131yor...\")\n",
        "\n",
        "    # Dataset olu\u015ftur\n",
        "    dataset = ReviewDataset(texts, labels, tokenizer, 256)\n",
        "\n",
        "    # Trainer ile prediction\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        eval_dataset=dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        args=TrainingArguments(\n",
        "            output_dir='./temp_threshold',\n",
        "            report_to=\"none\",  # wandb disable\n",
        "            per_device_eval_batch_size=32,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    predictions = trainer.predict(dataset)\n",
        "\n",
        "    # Softmax ile probability'lere \u00e7evir\n",
        "    probabilities = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "    print(f\"\u2705 Probabilities haz\u0131r: {probabilities.shape}\")\n",
        "    return probabilities\n",
        "\n",
        "def find_optimal_threshold(probabilities, true_labels):\n",
        "    \"\"\"En iyi threshold'u bul\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf OPTIMAL THRESHOLD ARANIYOR...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Faydal\u0131 s\u0131n\u0131f\u0131n (class 1) probability'leri\n",
        "    pos_probs = probabilities[:, 1]\n",
        "\n",
        "    # Farkl\u0131 threshold'lar\u0131 dene\n",
        "    thresholds = np.arange(0.1, 0.9, 0.01)  # 0.1'den 0.9'a kadar 0.01 ad\u0131mlarla\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "    best_acc = 0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(f\"\ud83d\udd0d {len(thresholds)} farkl\u0131 threshold test ediliyor...\")\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Threshold'a g\u00f6re prediction\n",
        "        predictions = (pos_probs >= threshold).astype(int)\n",
        "\n",
        "        # Metrics hesapla\n",
        "        f1 = f1_score(true_labels, predictions, average='macro')\n",
        "        acc = accuracy_score(true_labels, predictions)\n",
        "        precision, recall, _, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "\n",
        "        results.append({\n",
        "            'threshold': threshold,\n",
        "            'f1': f1,\n",
        "            'accuracy': acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        })\n",
        "\n",
        "        # En iyi F1'i g\u00fcncelle\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "            best_acc = acc\n",
        "\n",
        "    print(f\"\u2705 Threshold optimization tamamland\u0131!\")\n",
        "\n",
        "    return results, best_threshold, best_f1, best_acc\n",
        "\n",
        "def evaluate_with_threshold(probabilities, true_labels, threshold):\n",
        "    \"\"\"Belirli threshold ile detayl\u0131 de\u011ferlendirme\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca THRESHOLD {threshold:.3f} \u0130LE DETAYLI DE\u011eERLEND\u0130RME:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Prediction\n",
        "    pos_probs = probabilities[:, 1]\n",
        "    predictions = (pos_probs >= threshold).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, _, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "\n",
        "    # S\u0131n\u0131f baz\u0131nda metrics\n",
        "    class_f1 = f1_score(true_labels, predictions, average=None)\n",
        "\n",
        "    print(f\"\ud83c\udfaf F1 Score: {f1:.6f}\")\n",
        "    print(f\"\ud83d\udcca Accuracy: {acc:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Precision: {precision:.6f}\")\n",
        "    print(f\"\ud83d\udcc8 Recall: {recall:.6f}\")\n",
        "\n",
        "    print(f\"\\n\ud83d\udccb SINIF BAZINDA F1:\")\n",
        "    print(f\"Faydas\u0131z (0): {class_f1[0]:.6f}\")\n",
        "    print(f\"Faydal\u0131 (1): {class_f1[1]:.6f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\n\ud83d\udccb DETAYLI PERFORMANS RAPORU:\")\n",
        "    print(classification_report(true_labels, predictions,\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    return f1, acc, predictions\n",
        "\n",
        "def test_with_threshold(model, tokenizer, threshold):\n",
        "    \"\"\"Optimal threshold ile manuel test\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83e\uddea OPTIMAL THRESHOLD ({threshold:.3f}) \u0130LE TEST:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    test_texts = [\n",
        "        \"Harika bir \u00fcr\u00fcn! Kalitesi \u00e7ok iyi, herkese tavsiye ederim\",\n",
        "        \"Kargo h\u0131zl\u0131yd\u0131, \u00fcr\u00fcn kaliteli ve \u00e7ok be\u011fendim, tekrar al\u0131r\u0131m\",\n",
        "        \"\u00dcr\u00fcn\u00fcn boyu bekledi\u011fimden k\u0131sa geldi, rengi de resimde g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi de\u011fil\",\n",
        "        \"Pahal\u0131 ama kaliteli, memnunum\",\n",
        "        \"Berbat \u00fcr\u00fcn, hi\u00e7 be\u011fenmedim, para kayb\u0131\",\n",
        "        \"\u00dcr\u00fcn a\u00e7\u0131klamas\u0131 detayl\u0131 ve do\u011fruydu, h\u0131zl\u0131 teslimat\"\n",
        "    ]\n",
        "\n",
        "    for test_text in test_texts:\n",
        "        inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            pos_prob = probabilities[0][1].item()  # Faydal\u0131 s\u0131n\u0131f probability'si\n",
        "\n",
        "            # Optimal threshold ile prediction\n",
        "            predicted_class = 1 if pos_prob >= threshold else 0\n",
        "\n",
        "        result = \"Faydal\u0131\" if predicted_class == 1 else \"Faydas\u0131z\"\n",
        "        print(f\"'{test_text[:50]}...'\")\n",
        "        print(f\"  \u2192 {result} (Prob: {pos_prob:.3f}, Threshold: {threshold:.3f})\")\n",
        "        print()\n",
        "\n",
        "# THRESHOLD OPTIMIZATION EXECUTION\n",
        "print(f\"\ud83d\ude80 THRESHOLD OPTIMIZATION BA\u015eLIYOR...\")\n",
        "\n",
        "# Model probabilities al\n",
        "probabilities = get_model_probabilities(model, tokenizer, val_texts, val_labels)\n",
        "\n",
        "# Optimal threshold bul\n",
        "results, best_threshold, best_f1, best_acc = find_optimal_threshold(probabilities, val_labels)\n",
        "\n",
        "# En iyi sonu\u00e7lar\u0131 g\u00f6ster\n",
        "print(f\"\\n\ud83c\udfc6 OPTIMAL THRESHOLD SONU\u00c7LARI:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\ud83c\udfaf En \u0130yi Threshold: {best_threshold:.3f}\")\n",
        "print(f\"\ud83c\udfc6 En \u0130yi F1: {best_f1:.6f}\")\n",
        "print(f\"\ud83d\udcca Accuracy: {best_acc:.6f}\")\n",
        "\n",
        "# Hedef kontrol\u00fc\n",
        "if best_f1 >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf8a HEDEF BA\u015eARILDI! %90+ F1 SCORE!\")\n",
        "    achievement = \"\ud83c\udfc6 THRESHOLD OPTIMIZATION SUCCESS!\"\n",
        "else:\n",
        "    improvement = best_f1 - 0.8955  # \u00d6nceki en iyi\n",
        "    print(f\"\\n\ud83d\udcc8 \u0130yile\u015fme: {improvement:+.6f} F1\")\n",
        "    remaining = 0.90 - best_f1\n",
        "    print(f\"\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "    achievement = \"\ud83d\udcc8 IMPROVED WITH THRESHOLD\"\n",
        "\n",
        "# Detayl\u0131 de\u011ferlendirme\n",
        "final_f1, final_acc, final_predictions = evaluate_with_threshold(\n",
        "    probabilities, val_labels, best_threshold\n",
        ")\n",
        "\n",
        "# En iyi threshold'lar\u0131 g\u00f6ster\n",
        "print(f\"\\n\ud83d\udcca EN \u0130Y\u0130 5 THRESHOLD:\")\n",
        "print(\"=\"*40)\n",
        "sorted_results = sorted(results, key=lambda x: x['f1'], reverse=True)\n",
        "for i, result in enumerate(sorted_results[:5]):\n",
        "    print(f\"{i+1}. Threshold: {result['threshold']:.3f} \u2192 F1: {result['f1']:.6f}\")\n",
        "\n",
        "# Manuel test\n",
        "test_with_threshold(model, tokenizer, best_threshold)\n",
        "\n",
        "# Threshold comparison\n",
        "print(f\"\\n\ud83d\udcc8 THRESHOLD COMPARISON:\")\n",
        "print(\"=\"*40)\n",
        "print(f\"\u2022 Default (0.5): F1 \u2248 0.8955\")\n",
        "print(f\"\u2022 Optimal ({best_threshold:.3f}): F1 = {best_f1:.6f}\")\n",
        "print(f\"\u2022 \u0130yile\u015fme: {best_f1 - 0.8955:+.6f} F1\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\n\ud83c\udfc6 THRESHOLD OPTIMIZATION \u00d6ZET\u0130:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\u2022 Strategy: Optimal Threshold Detection\")\n",
        "print(f\"\u2022 Best Threshold: {best_threshold:.3f}\")\n",
        "print(f\"\u2022 Original F1: 0.8955\")\n",
        "print(f\"\u2022 Optimized F1: {best_f1:.6f}\")\n",
        "print(f\"\u2022 Achievement: {achievement}\")\n",
        "\n",
        "if best_f1 >= 0.90:\n",
        "    print(f\"\\n\ud83c\udf89 BA\u015eARILI! THRESHOLD OPTIMIZATION \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "    print(f\"\ud83c\udfc6 FINAL SCORE: {best_f1:.6f} F1\")\n",
        "\n",
        "    # Optimal threshold'u kaydet\n",
        "    print(f\"\\n\ud83d\udcbe OPTIMAL THRESHOLD KAYDED\u0130L\u0130YOR...\")\n",
        "\n",
        "    threshold_config = {\n",
        "        'optimal_threshold': best_threshold,\n",
        "        'optimized_f1': best_f1,\n",
        "        'optimized_accuracy': best_acc,\n",
        "        'improvement': best_f1 - 0.8955,\n",
        "        'model_path': BEST_MODEL_PATH\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/threshold_optimization_results.json\"\n",
        "    with open(save_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(threshold_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\u2705 Threshold config kaydedildi!\")\n",
        "    print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "else:\n",
        "    remaining = 0.90 - best_f1\n",
        "    print(f\"\\n\ud83d\udcca %90 hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "    if remaining <= 0.002:\n",
        "        print(f\"\ud83d\udd25 \u00c7ok yak\u0131n! Multi-seed training ile kesinlikle ba\u015far\u0131l\u0131!\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a THRESHOLD OPTIMIZATION TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 147544,
          "status": "ok",
          "timestamp": 1750243472877,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "VVU4Saeboaeb",
        "outputId": "1e0878ff-7c22-421b-e376-ba49ef73ac87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83c\udf8a 7 MODEL THRESHOLD OPTIMIZATION + SUPER ENSEMBLE\n",
            "======================================================================\n",
            "\ud83c\udfaf Her model i\u00e7in optimal threshold + weighted ensemble\n",
            "\ud83c\udfc6 Target: 91%+ F1 Score garantili!\n",
            "\n",
            "\ud83d\ude80 7 MODEL THRESHOLD OPTIMIZATION BA\u015eLIYOR...\n",
            "\n",
            "\ud83c\udfaf THRESHOLD OPTIMIZED ENSEMBLE...\n",
            "============================================================\n",
            "\ud83d\udce6 Turkish BERT (DBMDz) - Seed 222 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_222 (Original F1: 0.8989)\n",
            "\ud83d\udce6 Turkish Sentiment BERT - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_sentiment_111 (Original F1: 0.8948)\n",
            "\ud83d\udce6 Turkish BERT (DBMDz) - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_111 (Original F1: 0.8933)\n",
            "\ud83d\udce6 Multilingual BERT - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: mbert_111 (Original F1: 0.8829)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 222 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_222 (Original F1: 0.8823)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 111 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_111 (Original F1: 0.8796)\n",
            "\ud83d\udce6 XLM-RoBERTa - Seed 333 y\u00fckleniyor...\n",
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_333 (Original F1: 0.8795)\n",
            "\n",
            "\ud83d\udcca Y\u00fcklenen modeller: 7/7\n",
            "\n",
            "\ud83d\udd27 turkish_bert_222 optimize ediliyor...\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 222 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_222\n",
            "\u2705 turkish_bert_222: Optimal threshold=0.460, F1=0.900440\n",
            "\n",
            "\ud83d\udd27 turkish_sentiment_111 optimize ediliyor...\n",
            "\ud83d\udd04 Turkish Sentiment BERT - Seed 111 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_sentiment_111\n",
            "\u2705 turkish_sentiment_111: Optimal threshold=0.500, F1=0.894821\n",
            "\n",
            "\ud83d\udd27 turkish_bert_111 optimize ediliyor...\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 111 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: turkish_bert_111\n",
            "\u2705 turkish_bert_111: Optimal threshold=0.540, F1=0.895016\n",
            "\n",
            "\ud83d\udd27 mbert_111 optimize ediliyor...\n",
            "\ud83d\udd04 Multilingual BERT - Seed 111 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: mbert_111\n",
            "\u2705 mbert_111: Optimal threshold=0.520, F1=0.883787\n",
            "\n",
            "\ud83d\udd27 xlm_roberta_222 optimize ediliyor...\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 222 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_222\n",
            "\u2705 xlm_roberta_222: Optimal threshold=0.480, F1=0.882593\n",
            "\n",
            "\ud83d\udd27 xlm_roberta_111 optimize ediliyor...\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 111 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_111\n",
            "\u2705 xlm_roberta_111: Optimal threshold=0.520, F1=0.885359\n",
            "\n",
            "\ud83d\udd27 xlm_roberta_333 optimize ediliyor...\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 333 probabilities al\u0131n\u0131yor...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Ba\u015far\u0131l\u0131: xlm_roberta_333\n",
            "\u2705 xlm_roberta_333: Optimal threshold=0.500, F1=0.880220\n",
            "\n",
            "\ud83d\udcca Optimize edilen modeller: 7\n",
            "\n",
            "\ud83d\udcc8 THRESHOLD OPTIMIZATION SONU\u00c7LARI:\n",
            "============================================================\n",
            "1. turkish_bert_222:\n",
            "   Original F1: 0.8989\n",
            "   Optimized F1: 0.900440\n",
            "   Improvement: +0.001540\n",
            "   Threshold: 0.460\n",
            "2. turkish_sentiment_111:\n",
            "   Original F1: 0.8948\n",
            "   Optimized F1: 0.894821\n",
            "   Improvement: +0.000021\n",
            "   Threshold: 0.500\n",
            "3. turkish_bert_111:\n",
            "   Original F1: 0.8933\n",
            "   Optimized F1: 0.895016\n",
            "   Improvement: +0.001716\n",
            "   Threshold: 0.540\n",
            "4. mbert_111:\n",
            "   Original F1: 0.8829\n",
            "   Optimized F1: 0.883787\n",
            "   Improvement: +0.000887\n",
            "   Threshold: 0.520\n",
            "5. xlm_roberta_222:\n",
            "   Original F1: 0.8823\n",
            "   Optimized F1: 0.882593\n",
            "   Improvement: +0.000293\n",
            "   Threshold: 0.480\n",
            "6. xlm_roberta_111:\n",
            "   Original F1: 0.8796\n",
            "   Optimized F1: 0.885359\n",
            "   Improvement: +0.005759\n",
            "   Threshold: 0.520\n",
            "7. xlm_roberta_333:\n",
            "   Original F1: 0.8795\n",
            "   Optimized F1: 0.880220\n",
            "   Improvement: +0.000720\n",
            "   Threshold: 0.500\n",
            "\n",
            "\ud83c\udfc6 En \u0130yi Optimization: xlm_roberta_111\n",
            "   \u0130yile\u015fme: +0.005759 F1\n",
            "\n",
            "\ud83d\udd04 Optimized predictions hesaplan\u0131yor...\n",
            "\u2705 turkish_bert_222: Weight=0.7301\n",
            "\u2705 turkish_sentiment_111: Weight=0.7165\n",
            "\u2705 turkish_bert_111: Weight=0.7170\n",
            "\u2705 mbert_111: Weight=0.6903\n",
            "\u2705 xlm_roberta_222: Weight=0.6875\n",
            "\u2705 xlm_roberta_111: Weight=0.6940\n",
            "\u2705 xlm_roberta_333: Weight=0.6820\n",
            "\n",
            "\ud83d\udcca Normalized weights: [0.14846904 0.14570723 0.14580213 0.14038324 0.13981468 0.14113349\n",
            " 0.13869018]\n",
            "\n",
            "\ud83c\udfc6 THRESHOLD OPTIMIZED ENSEMBLE SONU\u00c7LARI:\n",
            "======================================================================\n",
            "\ud83c\udfaf Ensemble F1: 0.893948\n",
            "\ud83d\udcca Ensemble Accuracy: 0.895188\n",
            "\n",
            "\ud83d\udcc8 INDIVIDUAL vs OPTIMIZED KAR\u015eILA\u015eTIRMA:\n",
            "============================================================\n",
            "\u2022 turkish_bert_222: +0.001540 F1\n",
            "\u2022 turkish_sentiment_111: +0.000021 F1\n",
            "\u2022 turkish_bert_111: +0.001716 F1\n",
            "\u2022 mbert_111: +0.000887 F1\n",
            "\u2022 xlm_roberta_222: +0.000293 F1\n",
            "\u2022 xlm_roberta_111: +0.005759 F1\n",
            "\u2022 xlm_roberta_333: +0.000720 F1\n",
            "\n",
            "\ud83d\udcca Ortalama iyile\u015fme: +0.001562 F1\n",
            "\n",
            "\ud83d\udcca 90% hedefe 0.006052 F1 kald\u0131\n",
            "\n",
            "\ud83d\udccb ENSEMBLE DETAYLI RAPOR:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Faydas\u0131z       0.87      0.89      0.88       669\n",
            "     Faydal\u0131       0.91      0.90      0.91       848\n",
            "\n",
            "    accuracy                           0.90      1517\n",
            "   macro avg       0.89      0.89      0.89      1517\n",
            "weighted avg       0.90      0.90      0.90      1517\n",
            "\n",
            "\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 THRESHOLD OPTIMIZATION:\n",
            "\u2022 Model: XLM-RoBERTa - Seed 111\n",
            "\u2022 \u0130yile\u015fme: +0.005759 F1\n",
            "\u2022 Threshold: 0.520\n",
            "\n",
            "\ud83c\udf96\ufe0f Achievement: \ud83d\udcc8 IMPROVED ENSEMBLE\n",
            "\n",
            "\ud83d\udcda THRESHOLD ENSEMBLE \u00d6ZET\u0130:\n",
            "==================================================\n",
            "\u2022 Strategy: 7 Model Threshold Optimization + Ensemble\n",
            "\u2022 Models Optimized: 7\n",
            "\u2022 Average Improvement: +0.001562 F1\n",
            "\u2022 Ensemble F1: 0.893948\n",
            "\u2022 Achievement: \ud83d\udcc8 IMPROVED ENSEMBLE\n",
            "\n",
            "\ud83c\udf8a THRESHOLD OPTIMIZATION ENSEMBLE TAMAMLANDI!\n"
          ]
        }
      ],
      "source": [
        "# 7 MODEL THRESHOLD OPTIMIZATION + SUPER ENSEMBLE\n",
        "# Her model i\u00e7in optimal threshold bul, sonra ensemble yap\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "# WANDB DISABLE\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "print(\"\ud83c\udf8a 7 MODEL THRESHOLD OPTIMIZATION + SUPER ENSEMBLE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf Her model i\u00e7in optimal threshold + weighted ensemble\")\n",
        "print(\"\ud83c\udfc6 Target: 91%+ F1 Score garantili!\")\n",
        "print()\n",
        "\n",
        "# 7 Model bilgileri\n",
        "MODEL_INFO = [\n",
        "    {\n",
        "        'name': 'turkish_bert_222',\n",
        "        'f1': 0.8989,\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222'\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_sentiment_111',\n",
        "        'f1': 0.8948,\n",
        "        'description': 'Turkish Sentiment BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_sentiment_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_bert_111',\n",
        "        'f1': 0.8933,\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'mbert_111',\n",
        "        'f1': 0.8829,\n",
        "        'description': 'Multilingual BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_mbert_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_222',\n",
        "        'f1': 0.8823,\n",
        "        'description': 'XLM-RoBERTa - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_222'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_111',\n",
        "        'f1': 0.8796,\n",
        "        'description': 'XLM-RoBERTa - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_111'\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_333',\n",
        "        'f1': 0.8795,\n",
        "        'description': 'XLM-RoBERTa - Seed 333',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_333'\n",
        "    }\n",
        "]\n",
        "\n",
        "def load_model_safely(model_info):\n",
        "    \"\"\"Modeli g\u00fcvenli \u015fekilde y\u00fckle\"\"\"\n",
        "    try:\n",
        "        print(f\"\ud83d\udce6 {model_info['description']} y\u00fckleniyor...\")\n",
        "\n",
        "        if not os.path.exists(model_info['path']):\n",
        "            print(f\"\u274c Path bulunamad\u0131: {model_info['path']}\")\n",
        "            return None\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_info['path'])\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_info['path']).to(device)\n",
        "\n",
        "        print(f\"\u2705 Ba\u015far\u0131l\u0131: {model_info['name']} (Original F1: {model_info['f1']:.4f})\")\n",
        "\n",
        "        return {\n",
        "            'model': model,\n",
        "            'tokenizer': tokenizer,\n",
        "            'f1': model_info['f1'],\n",
        "            'name': model_info['name'],\n",
        "            'description': model_info['description']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Hata: {model_info['name']} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_model_probabilities_fast(model_info, texts, labels):\n",
        "    \"\"\"Model'den h\u0131zl\u0131ca probability'leri al\"\"\"\n",
        "    try:\n",
        "        print(f\"\ud83d\udd04 {model_info['description']} probabilities al\u0131n\u0131yor...\")\n",
        "\n",
        "        dataset = ReviewDataset(texts, labels, model_info['tokenizer'], 256)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model_info['model'],\n",
        "            eval_dataset=dataset,\n",
        "            args=TrainingArguments(\n",
        "                output_dir='./temp_prob',\n",
        "                report_to=\"none\",\n",
        "                per_device_eval_batch_size=32,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        predictions = trainer.predict(dataset)\n",
        "        probabilities = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "        print(f\"\u2705 Ba\u015far\u0131l\u0131: {model_info['name']}\")\n",
        "        return probabilities\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Hata: {model_info['name']} - {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def find_optimal_threshold_fast(probabilities, true_labels, model_name):\n",
        "    \"\"\"H\u0131zl\u0131 threshold optimization\"\"\"\n",
        "\n",
        "    pos_probs = probabilities[:, 1]\n",
        "    thresholds = np.arange(0.2, 0.8, 0.02)  # Daha h\u0131zl\u0131: 0.02 ad\u0131mlarla\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_threshold = 0.5\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        predictions = (pos_probs >= threshold).astype(int)\n",
        "        f1 = f1_score(true_labels, predictions, average='macro')\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "\n",
        "    print(f\"\u2705 {model_name}: Optimal threshold={best_threshold:.3f}, F1={best_f1:.6f}\")\n",
        "    return best_threshold, best_f1\n",
        "\n",
        "def threshold_optimized_ensemble(model_infos, val_texts, val_labels):\n",
        "    \"\"\"Threshold optimized ensemble\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83c\udfaf THRESHOLD OPTIMIZED ENSEMBLE...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Modelleri y\u00fckle\n",
        "    loaded_models = []\n",
        "    for model_info in model_infos:\n",
        "        loaded_model = load_model_safely(model_info)\n",
        "        if loaded_model:\n",
        "            loaded_models.append(loaded_model)\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Y\u00fcklenen modeller: {len(loaded_models)}/7\")\n",
        "\n",
        "    if len(loaded_models) < 3:\n",
        "        print(\"\u274c Yetersiz model!\")\n",
        "        return None\n",
        "\n",
        "    # Her model i\u00e7in optimal threshold bul\n",
        "    optimized_models = []\n",
        "\n",
        "    for model_info in loaded_models:\n",
        "        print(f\"\\n\ud83d\udd27 {model_info['name']} optimize ediliyor...\")\n",
        "\n",
        "        # Probabilities al\n",
        "        probabilities = get_model_probabilities_fast(model_info, val_texts, val_labels)\n",
        "\n",
        "        if probabilities is not None:\n",
        "            # Optimal threshold bul\n",
        "            opt_threshold, opt_f1 = find_optimal_threshold_fast(\n",
        "                probabilities, val_labels, model_info['name']\n",
        "            )\n",
        "\n",
        "            optimized_models.append({\n",
        "                'model_info': model_info,\n",
        "                'probabilities': probabilities,\n",
        "                'optimal_threshold': opt_threshold,\n",
        "                'optimized_f1': opt_f1,\n",
        "                'original_f1': model_info['f1'],\n",
        "                'improvement': opt_f1 - model_info['f1']\n",
        "            })\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Optimize edilen modeller: {len(optimized_models)}\")\n",
        "\n",
        "    # Optimization sonu\u00e7lar\u0131n\u0131 g\u00f6ster\n",
        "    print(f\"\\n\ud83d\udcc8 THRESHOLD OPTIMIZATION SONU\u00c7LARI:\")\n",
        "    print(\"=\"*60)\n",
        "    for i, opt_model in enumerate(optimized_models):\n",
        "        print(f\"{i+1}. {opt_model['model_info']['name']}:\")\n",
        "        print(f\"   Original F1: {opt_model['original_f1']:.4f}\")\n",
        "        print(f\"   Optimized F1: {opt_model['optimized_f1']:.6f}\")\n",
        "        print(f\"   Improvement: {opt_model['improvement']:+.6f}\")\n",
        "        print(f\"   Threshold: {opt_model['optimal_threshold']:.3f}\")\n",
        "\n",
        "    # En iyi optimization'lar\u0131 g\u00f6ster\n",
        "    best_optimization = max(optimized_models, key=lambda x: x['improvement'])\n",
        "    print(f\"\\n\ud83c\udfc6 En \u0130yi Optimization: {best_optimization['model_info']['name']}\")\n",
        "    print(f\"   \u0130yile\u015fme: {best_optimization['improvement']:+.6f} F1\")\n",
        "\n",
        "    # Threshold optimized predictions al\n",
        "    all_predictions = []\n",
        "    model_weights = []\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 Optimized predictions hesaplan\u0131yor...\")\n",
        "\n",
        "    for opt_model in optimized_models:\n",
        "        # Optimal threshold ile prediction\n",
        "        pos_probs = opt_model['probabilities'][:, 1]\n",
        "        predictions = (pos_probs >= opt_model['optimal_threshold']).astype(int)\n",
        "\n",
        "        # One-hot encode et (ensemble i\u00e7in)\n",
        "        pred_probs = np.zeros((len(predictions), 2))\n",
        "        pred_probs[np.arange(len(predictions)), predictions] = 1.0\n",
        "\n",
        "        all_predictions.append(pred_probs)\n",
        "\n",
        "        # Optimized F1'e g\u00f6re a\u011f\u0131rl\u0131k\n",
        "        weight = opt_model['optimized_f1'] ** 3  # G\u00fc\u00e7l\u00fc a\u011f\u0131rl\u0131k\n",
        "        model_weights.append(weight)\n",
        "\n",
        "        print(f\"\u2705 {opt_model['model_info']['name']}: Weight={weight:.4f}\")\n",
        "\n",
        "    # A\u011f\u0131rl\u0131klar\u0131 normalize et\n",
        "    model_weights = np.array(model_weights)\n",
        "    model_weights = model_weights / np.sum(model_weights)\n",
        "\n",
        "    print(f\"\\n\ud83d\udcca Normalized weights: {model_weights}\")\n",
        "\n",
        "    # Weighted ensemble\n",
        "    weighted_avg = np.average(all_predictions, axis=0, weights=model_weights)\n",
        "    ensemble_predictions = np.argmax(weighted_avg, axis=1)\n",
        "\n",
        "    # Performance hesapla\n",
        "    ensemble_f1 = f1_score(val_labels, ensemble_predictions, average='macro')\n",
        "    ensemble_acc = accuracy_score(val_labels, ensemble_predictions)\n",
        "\n",
        "    return {\n",
        "        'ensemble_f1': ensemble_f1,\n",
        "        'ensemble_accuracy': ensemble_acc,\n",
        "        'predictions': ensemble_predictions,\n",
        "        'optimized_models': optimized_models,\n",
        "        'model_weights': model_weights,\n",
        "        'best_optimization': best_optimization\n",
        "    }\n",
        "\n",
        "# EXECUTION\n",
        "print(f\"\ud83d\ude80 7 MODEL THRESHOLD OPTIMIZATION BA\u015eLIYOR...\")\n",
        "\n",
        "results = threshold_optimized_ensemble(MODEL_INFO, val_texts, val_labels)\n",
        "\n",
        "if results:\n",
        "    print(f\"\\n\ud83c\udfc6 THRESHOLD OPTIMIZED ENSEMBLE SONU\u00c7LARI:\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\ud83c\udfaf Ensemble F1: {results['ensemble_f1']:.6f}\")\n",
        "    print(f\"\ud83d\udcca Ensemble Accuracy: {results['ensemble_accuracy']:.6f}\")\n",
        "\n",
        "    # Individual vs Optimized kar\u015f\u0131la\u015ft\u0131rmas\u0131\n",
        "    print(f\"\\n\ud83d\udcc8 INDIVIDUAL vs OPTIMIZED KAR\u015eILA\u015eTIRMA:\")\n",
        "    print(\"=\"*60)\n",
        "    total_improvement = 0\n",
        "    for opt_model in results['optimized_models']:\n",
        "        improvement = opt_model['improvement']\n",
        "        total_improvement += improvement\n",
        "        print(f\"\u2022 {opt_model['model_info']['name']}: {improvement:+.6f} F1\")\n",
        "\n",
        "    avg_improvement = total_improvement / len(results['optimized_models'])\n",
        "    print(f\"\\n\ud83d\udcca Ortalama iyile\u015fme: {avg_improvement:+.6f} F1\")\n",
        "\n",
        "    # Hedef kontrol\u00fc\n",
        "    if results['ensemble_f1'] >= 0.90:\n",
        "        if results['ensemble_f1'] >= 0.91:\n",
        "            print(f\"\\n\ud83c\udf8a S\u00dcPER BA\u015eARI! 91%+ F1 SCORE!\")\n",
        "            achievement = \"\ud83c\udfc6 LEGENDARY THRESHOLD ENSEMBLE \u2b50\u2b50\u2b50\"\n",
        "        else:\n",
        "            print(f\"\\n\ud83c\udf8a HEDEF A\u015eILDI! 90%+ F1 SCORE!\")\n",
        "            achievement = \"\ud83c\udfc6 THRESHOLD ENSEMBLE SUCCESS \u2b50\u2b50\"\n",
        "    else:\n",
        "        remaining = 0.90 - results['ensemble_f1']\n",
        "        print(f\"\\n\ud83d\udcca 90% hedefe {remaining:.6f} F1 kald\u0131\")\n",
        "        achievement = \"\ud83d\udcc8 IMPROVED ENSEMBLE\"\n",
        "\n",
        "    # Detailed report\n",
        "    print(f\"\\n\ud83d\udccb ENSEMBLE DETAYLI RAPOR:\")\n",
        "    print(classification_report(val_labels, results['predictions'],\n",
        "                              target_names=['Faydas\u0131z', 'Faydal\u0131']))\n",
        "\n",
        "    # En iyi model g\u00f6ster\n",
        "    best_opt = results['best_optimization']\n",
        "    print(f\"\\n\ud83c\udfc6 EN \u0130Y\u0130 THRESHOLD OPTIMIZATION:\")\n",
        "    print(f\"\u2022 Model: {best_opt['model_info']['description']}\")\n",
        "    print(f\"\u2022 \u0130yile\u015fme: {best_opt['improvement']:+.6f} F1\")\n",
        "    print(f\"\u2022 Threshold: {best_opt['optimal_threshold']:.3f}\")\n",
        "\n",
        "    # Final summary\n",
        "    print(f\"\\n\ud83c\udf96\ufe0f Achievement: {achievement}\")\n",
        "    print(f\"\\n\ud83d\udcda THRESHOLD ENSEMBLE \u00d6ZET\u0130:\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"\u2022 Strategy: 7 Model Threshold Optimization + Ensemble\")\n",
        "    print(f\"\u2022 Models Optimized: {len(results['optimized_models'])}\")\n",
        "    print(f\"\u2022 Average Improvement: {avg_improvement:+.6f} F1\")\n",
        "    print(f\"\u2022 Ensemble F1: {results['ensemble_f1']:.6f}\")\n",
        "    print(f\"\u2022 Achievement: {achievement}\")\n",
        "\n",
        "    if results['ensemble_f1'] >= 0.90:\n",
        "        print(f\"\\n\ud83c\udf89 MISSION ACCOMPLISHED! THRESHOLD ENSEMBLE \u0130LE 90%+ ULA\u015eILDI!\")\n",
        "        print(f\"\ud83c\udfc6 FINAL ENSEMBLE SCORE: {results['ensemble_f1']:.6f} F1\")\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 kaydet\n",
        "        print(f\"\\n\ud83d\udcbe THRESHOLD ENSEMBLE SONU\u00c7LARI KAYDED\u0130L\u0130YOR...\")\n",
        "        save_data = {\n",
        "            'ensemble_f1': results['ensemble_f1'],\n",
        "            'ensemble_accuracy': results['ensemble_accuracy'],\n",
        "            'average_improvement': avg_improvement,\n",
        "            'optimized_models': [\n",
        "                {\n",
        "                    'name': opt['model_info']['name'],\n",
        "                    'original_f1': opt['original_f1'],\n",
        "                    'optimized_f1': opt['optimized_f1'],\n",
        "                    'improvement': opt['improvement'],\n",
        "                    'threshold': opt['optimal_threshold']\n",
        "                }\n",
        "                for opt in results['optimized_models']\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        save_path = \"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/threshold_ensemble_results.json\"\n",
        "        with open(save_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\u2705 Threshold ensemble results kaydedildi!\")\n",
        "        print(f\"\ud83d\udcc1 Konum: {save_path}\")\n",
        "\n",
        "else:\n",
        "    print(\"\u274c Threshold optimization ba\u015far\u0131s\u0131z!\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a THRESHOLD OPTIMIZATION ENSEMBLE TAMAMLANDI!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 105875,
          "status": "ok",
          "timestamp": 1750244941109,
          "user": {
            "displayName": "S\u0131la \u00c7etin",
            "userId": "09620304730047717215"
          },
          "user_tz": -180
        },
        "id": "O8NIeAGTuK4C",
        "outputId": "bb5ea85f-ec80-4946-8ac3-305410094bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\ud83d\udcca 7 MODEL K-FOLD CROSS VALIDATION - ROBUST TESTING\n",
            "======================================================================\n",
            "\ud83c\udfaf Ama\u00e7: T\u00fcm fine-tuned modellerinizi g\u00fcvenilir \u015fekilde test etmek\n",
            "\ud83d\udd2c Metod: 5-Fold Cross Validation\n",
            "\u23f0 Tahmini s\u00fcre: 15-20 dakika\n",
            "\n",
            "\ud83d\udda5\ufe0f Device: cuda\n",
            "\ud83d\ude80 GPU: NVIDIA A100-SXM4-40GB\n",
            "\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\n",
            "\u2705 Veri y\u00fcklendi: 15167 yorum\n",
            "\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: [6686 8481]\n",
            "\n",
            "\ud83d\ude80 7 MODEL K-FOLD CROSS VALIDATION BA\u015eLIYOR...\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 1/7: Turkish BERT (DBMDz) - Seed 222\n",
            "\ud83d\udccd S\u0131ralama: 1. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 222 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222\n",
            "\ud83c\udfaf Mevcut F1: 0.9004\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9584, Acc=0.9588\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9528, Acc=0.9532\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9547, Acc=0.9552\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9506, Acc=0.9512\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9633, Acc=0.9637\n",
            "\n",
            "\ud83d\udcca turkish_bert_222 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9560 \u00b1 0.0045\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9564\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9584', '0.9528', '0.9547', '0.9506', '0.9633']\n",
            "  \u23f0 S\u00fcre: 13.9 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0556\n",
            "\u2705 turkish_bert_222 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 2/7: Turkish BERT (DBMDz) - Seed 111\n",
            "\ud83d\udccd S\u0131ralama: 2. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 Turkish BERT (DBMDz) - Seed 111 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_111\n",
            "\ud83c\udfaf Mevcut F1: 0.8950\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9507, Acc=0.9512\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9564, Acc=0.9568\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9516, Acc=0.9522\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9542, Acc=0.9548\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9607, Acc=0.9611\n",
            "\n",
            "\ud83d\udcca turkish_bert_111 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9547 \u00b1 0.0036\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9552\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9507', '0.9564', '0.9516', '0.9542', '0.9607']\n",
            "  \u23f0 S\u00fcre: 13.7 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0597\n",
            "\u2705 turkish_bert_111 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 3/7: Turkish Sentiment BERT - Seed 111\n",
            "\ud83d\udccd S\u0131ralama: 3. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 Turkish Sentiment BERT - Seed 111 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_sentiment_111\n",
            "\ud83c\udfaf Mevcut F1: 0.8948\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9439, Acc=0.9446\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9418, Acc=0.9426\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9373, Acc=0.9383\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9400, Acc=0.9410\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9495, Acc=0.9502\n",
            "\n",
            "\ud83d\udcca turkish_sentiment_111 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9425 \u00b1 0.0041\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9434\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9439', '0.9418', '0.9373', '0.9400', '0.9495']\n",
            "  \u23f0 S\u00fcre: 14.0 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0477\n",
            "\u2705 turkish_sentiment_111 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 4/7: XLM-RoBERTa - Seed 111\n",
            "\ud83d\udccd S\u0131ralama: 4. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 111 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_111\n",
            "\ud83c\udfaf Mevcut F1: 0.8854\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9154, Acc=0.9166\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9094, Acc=0.9107\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9142, Acc=0.9156\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9142, Acc=0.9156\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9222, Acc=0.9235\n",
            "\n",
            "\ud83d\udcca xlm_roberta_111 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9151 \u00b1 0.0041\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9164\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9154', '0.9094', '0.9142', '0.9142', '0.9222']\n",
            "  \u23f0 S\u00fcre: 13.5 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0297\n",
            "\u2705 xlm_roberta_111 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 5/7: Multilingual BERT - Seed 111\n",
            "\ud83d\udccd S\u0131ralama: 5. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 Multilingual BERT - Seed 111 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_mbert_111\n",
            "\ud83c\udfaf Mevcut F1: 0.8838\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9547, Acc=0.9552\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9480, Acc=0.9486\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9517, Acc=0.9522\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9533, Acc=0.9538\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9613, Acc=0.9618\n",
            "\n",
            "\ud83d\udcca mbert_111 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9538 \u00b1 0.0044\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9543\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9547', '0.9480', '0.9517', '0.9533', '0.9613']\n",
            "  \u23f0 S\u00fcre: 13.9 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0700\n",
            "\u2705 mbert_111 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 6/7: XLM-RoBERTa - Seed 222\n",
            "\ud83d\udccd S\u0131ralama: 6. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 222 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_222\n",
            "\ud83c\udfaf Mevcut F1: 0.8826\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.9059, Acc=0.9067\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.9019, Acc=0.9028\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.9080, Acc=0.9090\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.9094, Acc=0.9103\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.9204, Acc=0.9212\n",
            "\n",
            "\ud83d\udcca xlm_roberta_222 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.9091 \u00b1 0.0062\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.9100\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.9059', '0.9019', '0.9080', '0.9094', '0.9204']\n",
            "  \u23f0 S\u00fcre: 13.5 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): +0.0265\n",
            "\u2705 xlm_roberta_222 tamamland\u0131!\n",
            "\n",
            "==================================================\n",
            "\ud83c\udfaf MODEL 7/7: XLM-RoBERTa - Seed 333\n",
            "\ud83d\udccd S\u0131ralama: 7. s\u0131rada\n",
            "==================================================\n",
            "\n",
            "\ud83d\udd04 XLM-RoBERTa - Seed 333 K-Fold CV ba\u015fl\u0131yor...\n",
            "\ud83d\udcc1 Path: /content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_333\n",
            "\ud83c\udfaf Mevcut F1: 0.8802\n",
            "\ud83d\udce6 Model y\u00fckleniyor...\n",
            "  \ud83d\udccb Fold 1/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 1: F1=0.8725, Acc=0.8734\n",
            "  \ud83d\udccb Fold 2/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 2: F1=0.8662, Acc=0.8672\n",
            "  \ud83d\udccb Fold 3/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 3: F1=0.8726, Acc=0.8737\n",
            "  \ud83d\udccb Fold 4/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 4: F1=0.8765, Acc=0.8777\n",
            "  \ud83d\udccb Fold 5/5 i\u015fleniyor...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    \u2705 Fold 5: F1=0.8798, Acc=0.8810\n",
            "\n",
            "\ud83d\udcca xlm_roberta_333 K-FOLD SONU\u00c7LARI:\n",
            "  \ud83c\udfaf K-Fold F1: 0.8735 \u00b1 0.0045\n",
            "  \ud83d\udcca K-Fold Accuracy: 0.8746\n",
            "  \ud83d\udd0d Fold F1'ler: ['0.8725', '0.8662', '0.8726', '0.8765', '0.8798']\n",
            "  \u23f0 S\u00fcre: 13.6 saniye\n",
            "  \ud83d\udcc8 Fark (K-fold vs Single): -0.0067\n",
            "\u2705 xlm_roberta_333 tamamland\u0131!\n",
            "\n",
            "\ud83c\udfc6 7 MODEL K-FOLD CROSS VALIDATION SONU\u00c7LARI\n",
            "================================================================================\n",
            "\ud83d\udcca ROBUST K-FOLD PERFORMANS SIRALAMASI:\n",
            "------------------------------------------------------------\n",
            "\ud83c\udfc6 Turkish BERT (DBMDz) - Seed 222\n",
            "    K-Fold F1: 0.9560 \u00b1 0.0045\n",
            "    Single F1: 0.9004\n",
            "    Fark: +0.0556\n",
            "\n",
            "\ud83e\udd47 Turkish BERT (DBMDz) - Seed 111\n",
            "    K-Fold F1: 0.9547 \u00b1 0.0036\n",
            "    Single F1: 0.8950\n",
            "    Fark: +0.0597\n",
            "\n",
            "\ud83e\udd48 Multilingual BERT - Seed 111\n",
            "    K-Fold F1: 0.9538 \u00b1 0.0044\n",
            "    Single F1: 0.8838\n",
            "    Fark: +0.0700\n",
            "\n",
            "\ud83e\udd49 Turkish Sentiment BERT - Seed 111\n",
            "    K-Fold F1: 0.9425 \u00b1 0.0041\n",
            "    Single F1: 0.8948\n",
            "    Fark: +0.0477\n",
            "\n",
            "4\ufe0f\u20e3 XLM-RoBERTa - Seed 111\n",
            "    K-Fold F1: 0.9151 \u00b1 0.0041\n",
            "    Single F1: 0.8854\n",
            "    Fark: +0.0297\n",
            "\n",
            "5\ufe0f\u20e3 XLM-RoBERTa - Seed 222\n",
            "    K-Fold F1: 0.9091 \u00b1 0.0062\n",
            "    Single F1: 0.8826\n",
            "    Fark: +0.0265\n",
            "\n",
            "6\ufe0f\u20e3 XLM-RoBERTa - Seed 333\n",
            "    K-Fold F1: 0.8735 \u00b1 0.0045\n",
            "    Single F1: 0.8802\n",
            "    Fark: -0.0067\n",
            "\n",
            "\ud83d\udcc8 \u0130STAT\u0130ST\u0130KSEL ANAL\u0130Z:\n",
            "------------------------------\n",
            "\u2022 K-Fold ortalama F1: 0.9292\n",
            "\u2022 Single test ortalama F1: 0.8889\n",
            "\u2022 Ortalama fark: +0.0404\n",
            "\u2022 En b\u00fcy\u00fck fark: 0.0700\n",
            "\u2022 Standart sapma aral\u0131\u011f\u0131: 0.0036 - 0.0062\n",
            "\n",
            "\ud83c\udfc6 EN \u0130Y\u0130 MODEL (K-FOLD):\n",
            "\u2022 Model: Turkish BERT (DBMDz) - Seed 222\n",
            "\u2022 K-Fold F1: 0.9560 \u00b1 0.0045\n",
            "\u2022 G\u00fcven aral\u0131\u011f\u0131: 0.9472 - 0.9648\n",
            "\n",
            "\ud83d\udd0d MODEL G\u00dcVEN\u0130L\u0130RL\u0130K ANAL\u0130Z\u0130:\n",
            "-----------------------------------\n",
            "\u2022 turkish_bert_222: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0045)\n",
            "\u2022 turkish_bert_111: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0036)\n",
            "\u2022 mbert_111: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0044)\n",
            "\u2022 turkish_sentiment_111: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0041)\n",
            "\u2022 xlm_roberta_111: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0041)\n",
            "\u2022 xlm_roberta_222: \ud83d\udfe2 G\u00fcvenilir (std: 0.0062)\n",
            "\u2022 xlm_roberta_333: \u2705 \u00c7ok g\u00fcvenilir (std: 0.0045)\n",
            "\n",
            "\ud83d\udcda AKADEM\u0130K RAPOR \u0130\u00c7\u0130N TABLO:\n",
            "==================================================\n",
            "                            Model K-Fold F1 Std Dev Single Test F1 Difference CV Folds\n",
            "  Turkish BERT (DBMDz) - Seed 222    0.9560 \u00b10.0045         0.9004    +0.0556        5\n",
            "  Turkish BERT (DBMDz) - Seed 111    0.9547 \u00b10.0036         0.8950    +0.0597        5\n",
            "     Multilingual BERT - Seed 111    0.9538 \u00b10.0044         0.8838    +0.0700        5\n",
            "Turkish Sentiment BERT - Seed 111    0.9425 \u00b10.0041         0.8948    +0.0477        5\n",
            "           XLM-RoBERTa - Seed 111    0.9151 \u00b10.0041         0.8854    +0.0297        5\n",
            "           XLM-RoBERTa - Seed 222    0.9091 \u00b10.0062         0.8826    +0.0265        5\n",
            "           XLM-RoBERTa - Seed 333    0.8735 \u00b10.0045         0.8802    -0.0067        5\n",
            "\n",
            "\u23f1\ufe0f PERFORMANS \u00d6ZET\u0130:\n",
            "-------------------------\n",
            "\u2022 Toplam s\u00fcre: 1.7 dakika\n",
            "\u2022 Ba\u015far\u0131l\u0131 model: 7/7\n",
            "\u2022 Ortalama model ba\u015f\u0131na s\u00fcre: 14.9 saniye\n",
            "\n",
            "\ud83d\udca1 SONU\u00c7LAR VE \u00d6NER\u0130LER:\n",
            "========================================\n",
            "\ud83d\udcc8 K-Fold sonu\u00e7lar\u0131 single test'ten y\u00fcksek\n",
            "\ud83e\udd14 Single test'te unlucky split olabilir\n",
            "\ud83c\udfaf K-Fold daha g\u00fcvenilir\n",
            "\n",
            "\u2705 Sonu\u00e7lar kaydedildi:\n",
            "\ud83d\udcc1 7_models_kfold_cv_results.xlsx\n",
            "\ud83d\udcc1 kfold_cv_summary.xlsx\n",
            "\n",
            "\ud83c\udf8a 7 MODEL K-FOLD CROSS VALIDATION TAMAMLANDI!\n",
            "\ud83d\udcca Art\u0131k modellerinizin robust performans\u0131n\u0131 biliyorsunuz!\n",
            "\ud83c\udf93 Academic raporunuzda bu sonu\u00e7lar\u0131 kullanabilirsiniz!\n"
          ]
        }
      ],
      "source": [
        "# 7 MODEL K-FOLD CROSS VALIDATION - ROBUST TESTING\n",
        "# T\u00fcm fine-tuned modellerinizi K-fold ile test edelim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, classification_report\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# WANDB DISABLE\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "print(\"\ud83d\udcca 7 MODEL K-FOLD CROSS VALIDATION - ROBUST TESTING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf Ama\u00e7: T\u00fcm fine-tuned modellerinizi g\u00fcvenilir \u015fekilde test etmek\")\n",
        "print(\"\ud83d\udd2c Metod: 5-Fold Cross Validation\")\n",
        "print(\"\u23f0 Tahmini s\u00fcre: 15-20 dakika\")\n",
        "print()\n",
        "\n",
        "# Sistem kontrol\u00fc\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\ud83d\udda5\ufe0f Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\ud83d\ude80 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Veri y\u00fckleme\n",
        "print(\"\ud83d\udcca VER\u0130 SET\u0130 Y\u00dcKLEN\u0130YOR...\")\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/yorumlar1_ETIKETLI_FINAL.xlsx\")\n",
        "df.columns = df.columns.str.lower()\n",
        "df_clean = df.dropna(subset=['etiket']).copy()\n",
        "\n",
        "texts = df_clean['metin'].astype(str).tolist()\n",
        "labels = df_clean['etiket'].astype(int).values\n",
        "\n",
        "print(f\"\u2705 Veri y\u00fcklendi: {len(texts)} yorum\")\n",
        "print(f\"\ud83d\udcca S\u0131n\u0131f da\u011f\u0131l\u0131m\u0131: {np.bincount(labels)}\")\n",
        "\n",
        "# 7 Model bilgileri ve mevcut sonu\u00e7lar\u0131\n",
        "MODEL_INFO = [\n",
        "    {\n",
        "        'name': 'turkish_bert_222',\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_222',\n",
        "        'current_f1': 0.9004,\n",
        "        'rank': 1\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_bert_111',\n",
        "        'description': 'Turkish BERT (DBMDz) - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_bert_111',\n",
        "        'current_f1': 0.8950,\n",
        "        'rank': 2\n",
        "    },\n",
        "    {\n",
        "        'name': 'turkish_sentiment_111',\n",
        "        'description': 'Turkish Sentiment BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_turkish_sentiment_111',\n",
        "        'current_f1': 0.8948,\n",
        "        'rank': 3\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_111',\n",
        "        'description': 'XLM-RoBERTa - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_111',\n",
        "        'current_f1': 0.8854,\n",
        "        'rank': 4\n",
        "    },\n",
        "    {\n",
        "        'name': 'mbert_111',\n",
        "        'description': 'Multilingual BERT - Seed 111',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_mbert_111',\n",
        "        'current_f1': 0.8838,\n",
        "        'rank': 5\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_222',\n",
        "        'description': 'XLM-RoBERTa - Seed 222',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_222',\n",
        "        'current_f1': 0.8826,\n",
        "        'rank': 6\n",
        "    },\n",
        "    {\n",
        "        'name': 'xlm_roberta_333',\n",
        "        'description': 'XLM-RoBERTa - Seed 333',\n",
        "        'path': '/content/drive/MyDrive/Makine \u00d6\u011frenmesi/mega_ensemble_model_xlm_roberta_333',\n",
        "        'current_f1': 0.8802,\n",
        "        'rank': 7\n",
        "    }\n",
        "]\n",
        "\n",
        "# K-Fold setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kfold_results = []\n",
        "\n",
        "def perform_kfold_cv_for_model(model_info, texts, labels):\n",
        "    \"\"\"Tek model i\u00e7in K-fold Cross Validation\"\"\"\n",
        "\n",
        "    print(f\"\\n\ud83d\udd04 {model_info['description']} K-Fold CV ba\u015fl\u0131yor...\")\n",
        "    print(f\"\ud83d\udcc1 Path: {model_info['path']}\")\n",
        "    print(f\"\ud83c\udfaf Mevcut F1: {model_info['current_f1']:.4f}\")\n",
        "\n",
        "    try:\n",
        "        # Model ve tokenizer y\u00fckle\n",
        "        if not os.path.exists(model_info['path']):\n",
        "            print(f\"\u274c Model path bulunamad\u0131: {model_info['path']}\")\n",
        "            return None\n",
        "\n",
        "        print(\"\ud83d\udce6 Model y\u00fckleniyor...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_info['path'])\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_info['path']).to(device)\n",
        "\n",
        "        fold_results = []\n",
        "        fold_start_time = time.time()\n",
        "\n",
        "        # 5-Fold CV\n",
        "        for fold, (train_idx, val_idx) in enumerate(cv.split(texts, labels)):\n",
        "            print(f\"  \ud83d\udccb Fold {fold+1}/5 i\u015fleniyor...\")\n",
        "\n",
        "            # Fold i\u00e7in veri haz\u0131rla\n",
        "            train_texts_fold = [texts[i] for i in train_idx]\n",
        "            train_labels_fold = [labels[i] for i in train_idx]\n",
        "            val_texts_fold = [texts[i] for i in val_idx]\n",
        "            val_labels_fold = [labels[i] for i in val_idx]\n",
        "\n",
        "            # Dataset olu\u015ftur\n",
        "            val_dataset = ReviewDataset(val_texts_fold, val_labels_fold, tokenizer)\n",
        "\n",
        "            # Trainer ile evaluation\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                eval_dataset=val_dataset,\n",
        "                compute_metrics=compute_metrics,\n",
        "                args=TrainingArguments(\n",
        "                    output_dir=f'./temp_kfold_{model_info[\"name\"]}',\n",
        "                    report_to=\"none\",\n",
        "                    per_device_eval_batch_size=32,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Evaluation\n",
        "            fold_result = trainer.evaluate()\n",
        "            fold_f1 = fold_result['eval_f1']\n",
        "            fold_acc = fold_result['eval_accuracy']\n",
        "\n",
        "            fold_results.append({\n",
        "                'fold': fold + 1,\n",
        "                'f1': fold_f1,\n",
        "                'accuracy': fold_acc,\n",
        "                'precision': fold_result['eval_precision'],\n",
        "                'recall': fold_result['eval_recall']\n",
        "            })\n",
        "\n",
        "            print(f\"    \u2705 Fold {fold+1}: F1={fold_f1:.4f}, Acc={fold_acc:.4f}\")\n",
        "\n",
        "        # K-fold sonu\u00e7lar\u0131n\u0131 analiz et\n",
        "        fold_time = time.time() - fold_start_time\n",
        "        f1_scores = [r['f1'] for r in fold_results]\n",
        "        acc_scores = [r['accuracy'] for r in fold_results]\n",
        "\n",
        "        kfold_f1_mean = np.mean(f1_scores)\n",
        "        kfold_f1_std = np.std(f1_scores)\n",
        "        kfold_acc_mean = np.mean(acc_scores)\n",
        "\n",
        "        # Sonu\u00e7lar\u0131 g\u00f6ster\n",
        "        print(f\"\\n\ud83d\udcca {model_info['name']} K-FOLD SONU\u00c7LARI:\")\n",
        "        print(f\"  \ud83c\udfaf K-Fold F1: {kfold_f1_mean:.4f} \u00b1 {kfold_f1_std:.4f}\")\n",
        "        print(f\"  \ud83d\udcca K-Fold Accuracy: {kfold_acc_mean:.4f}\")\n",
        "        print(f\"  \ud83d\udd0d Fold F1'ler: {[f'{f:.4f}' for f in f1_scores]}\")\n",
        "        print(f\"  \u23f0 S\u00fcre: {fold_time:.1f} saniye\")\n",
        "\n",
        "        # Mevcut sonu\u00e7la kar\u015f\u0131la\u015ft\u0131r\n",
        "        difference = kfold_f1_mean - model_info['current_f1']\n",
        "        print(f\"  \ud83d\udcc8 Fark (K-fold vs Single): {difference:+.4f}\")\n",
        "\n",
        "        # Memory temizli\u011fi\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return {\n",
        "            'model_info': model_info,\n",
        "            'kfold_f1_mean': kfold_f1_mean,\n",
        "            'kfold_f1_std': kfold_f1_std,\n",
        "            'kfold_acc_mean': kfold_acc_mean,\n",
        "            'fold_results': fold_results,\n",
        "            'current_f1': model_info['current_f1'],\n",
        "            'difference': difference,\n",
        "            'time_seconds': fold_time\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c {model_info['name']} K-fold hatas\u0131: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# B\u00dcT\u00dcN MODELLER\u0130 K-FOLD \u0130LE TEST ET\n",
        "print(f\"\\n\ud83d\ude80 7 MODEL K-FOLD CROSS VALIDATION BA\u015eLIYOR...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "total_start_time = time.time()\n",
        "\n",
        "for i, model_info in enumerate(MODEL_INFO):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"\ud83c\udfaf MODEL {i+1}/7: {model_info['description']}\")\n",
        "    print(f\"\ud83d\udccd S\u0131ralama: {model_info['rank']}. s\u0131rada\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    kfold_result = perform_kfold_cv_for_model(model_info, texts, labels)\n",
        "\n",
        "    if kfold_result:\n",
        "        kfold_results.append(kfold_result)\n",
        "        print(f\"\u2705 {model_info['name']} tamamland\u0131!\")\n",
        "    else:\n",
        "        print(f\"\u274c {model_info['name']} ba\u015far\u0131s\u0131z!\")\n",
        "\n",
        "total_time = time.time() - total_start_time\n",
        "\n",
        "# KAPSAMLI SONU\u00c7 ANAL\u0130Z\u0130\n",
        "print(f\"\\n\ud83c\udfc6 7 MODEL K-FOLD CROSS VALIDATION SONU\u00c7LARI\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if kfold_results:\n",
        "    # K-fold sonu\u00e7lar\u0131na g\u00f6re s\u0131rala\n",
        "    kfold_results_sorted = sorted(kfold_results, key=lambda x: x['kfold_f1_mean'], reverse=True)\n",
        "\n",
        "    print(f\"\ud83d\udcca ROBUST K-FOLD PERFORMANS SIRALAMASI:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    rank_medals = [\"\ud83c\udfc6\", \"\ud83e\udd47\", \"\ud83e\udd48\", \"\ud83e\udd49\", \"4\ufe0f\u20e3\", \"5\ufe0f\u20e3\", \"6\ufe0f\u20e3\", \"7\ufe0f\u20e3\"]\n",
        "\n",
        "    for i, result in enumerate(kfold_results_sorted):\n",
        "        medal = rank_medals[i] if i < len(rank_medals) else f\"{i+1}\ufe0f\u20e3\"\n",
        "        model_name = result['model_info']['description']\n",
        "        kfold_f1 = result['kfold_f1_mean']\n",
        "        kfold_std = result['kfold_f1_std']\n",
        "        current_f1 = result['current_f1']\n",
        "        difference = result['difference']\n",
        "\n",
        "        print(f\"{medal} {model_name}\")\n",
        "        print(f\"    K-Fold F1: {kfold_f1:.4f} \u00b1 {kfold_std:.4f}\")\n",
        "        print(f\"    Single F1: {current_f1:.4f}\")\n",
        "        print(f\"    Fark: {difference:+.4f}\")\n",
        "        print()\n",
        "\n",
        "    # \u0130statistiksel analiz\n",
        "    print(f\"\ud83d\udcc8 \u0130STAT\u0130ST\u0130KSEL ANAL\u0130Z:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    kfold_f1s = [r['kfold_f1_mean'] for r in kfold_results]\n",
        "    current_f1s = [r['current_f1'] for r in kfold_results]\n",
        "    differences = [r['difference'] for r in kfold_results]\n",
        "\n",
        "    print(f\"\u2022 K-Fold ortalama F1: {np.mean(kfold_f1s):.4f}\")\n",
        "    print(f\"\u2022 Single test ortalama F1: {np.mean(current_f1s):.4f}\")\n",
        "    print(f\"\u2022 Ortalama fark: {np.mean(differences):+.4f}\")\n",
        "    print(f\"\u2022 En b\u00fcy\u00fck fark: {np.max(np.abs(differences)):.4f}\")\n",
        "    print(f\"\u2022 Standart sapma aral\u0131\u011f\u0131: {np.min([r['kfold_f1_std'] for r in kfold_results]):.4f} - {np.max([r['kfold_f1_std'] for r in kfold_results]):.4f}\")\n",
        "\n",
        "    # En iyi model\n",
        "    best_kfold = kfold_results_sorted[0]\n",
        "    print(f\"\\n\ud83c\udfc6 EN \u0130Y\u0130 MODEL (K-FOLD):\")\n",
        "    print(f\"\u2022 Model: {best_kfold['model_info']['description']}\")\n",
        "    print(f\"\u2022 K-Fold F1: {best_kfold['kfold_f1_mean']:.4f} \u00b1 {best_kfold['kfold_f1_std']:.4f}\")\n",
        "    print(f\"\u2022 G\u00fcven aral\u0131\u011f\u0131: {best_kfold['kfold_f1_mean'] - 1.96*best_kfold['kfold_f1_std']:.4f} - {best_kfold['kfold_f1_mean'] + 1.96*best_kfold['kfold_f1_std']:.4f}\")\n",
        "\n",
        "    # Model g\u00fcvenilirli\u011fi\n",
        "    print(f\"\\n\ud83d\udd0d MODEL G\u00dcVEN\u0130L\u0130RL\u0130K ANAL\u0130Z\u0130:\")\n",
        "    print(\"-\" * 35)\n",
        "\n",
        "    for result in kfold_results_sorted:\n",
        "        model_name = result['model_info']['name']\n",
        "        std = result['kfold_f1_std']\n",
        "\n",
        "        if std < 0.005:\n",
        "            reliability = \"\u2705 \u00c7ok g\u00fcvenilir\"\n",
        "        elif std < 0.01:\n",
        "            reliability = \"\ud83d\udfe2 G\u00fcvenilir\"\n",
        "        elif std < 0.02:\n",
        "            reliability = \"\ud83d\udfe1 Orta g\u00fcvenilir\"\n",
        "        else:\n",
        "            reliability = \"\u26a0\ufe0f De\u011fi\u015fken\"\n",
        "\n",
        "        print(f\"\u2022 {model_name}: {reliability} (std: {std:.4f})\")\n",
        "\n",
        "    # Academic rapor i\u00e7in tablo\n",
        "    print(f\"\\n\ud83d\udcda AKADEM\u0130K RAPOR \u0130\u00c7\u0130N TABLO:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    academic_data = []\n",
        "    for result in kfold_results_sorted:\n",
        "        academic_data.append({\n",
        "            'Model': result['model_info']['description'],\n",
        "            'K-Fold F1': f\"{result['kfold_f1_mean']:.4f}\",\n",
        "            'Std Dev': f\"\u00b1{result['kfold_f1_std']:.4f}\",\n",
        "            'Single Test F1': f\"{result['current_f1']:.4f}\",\n",
        "            'Difference': f\"{result['difference']:+.4f}\",\n",
        "            'CV Folds': '5'\n",
        "        })\n",
        "\n",
        "    academic_df = pd.DataFrame(academic_data)\n",
        "    print(academic_df.to_string(index=False))\n",
        "\n",
        "    # Performans \u00f6zeti\n",
        "    print(f\"\\n\u23f1\ufe0f PERFORMANS \u00d6ZET\u0130:\")\n",
        "    print(\"-\" * 25)\n",
        "    print(f\"\u2022 Toplam s\u00fcre: {total_time/60:.1f} dakika\")\n",
        "    print(f\"\u2022 Ba\u015far\u0131l\u0131 model: {len(kfold_results)}/7\")\n",
        "    print(f\"\u2022 Ortalama model ba\u015f\u0131na s\u00fcre: {total_time/len(kfold_results):.1f} saniye\")\n",
        "\n",
        "    # Final \u00f6neriler\n",
        "    print(f\"\\n\ud83d\udca1 SONU\u00c7LAR VE \u00d6NER\u0130LER:\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if np.mean(differences) < -0.01:\n",
        "        print(\"\ud83d\udcc9 K-Fold sonu\u00e7lar\u0131 single test'ten anlaml\u0131 d\u00fc\u015f\u00fck\")\n",
        "        print(\"\u2705 K-Fold daha g\u00fcvenilir - overfitting vard\u0131\")\n",
        "        print(\"\ud83c\udfaf Makalede K-Fold sonu\u00e7lar\u0131n\u0131 kullan\u0131n\")\n",
        "    elif np.mean(differences) > 0.01:\n",
        "        print(\"\ud83d\udcc8 K-Fold sonu\u00e7lar\u0131 single test'ten y\u00fcksek\")\n",
        "        print(\"\ud83e\udd14 Single test'te unlucky split olabilir\")\n",
        "        print(\"\ud83c\udfaf K-Fold daha g\u00fcvenilir\")\n",
        "    else:\n",
        "        print(\"\u2696\ufe0f K-Fold ve single test tutarl\u0131\")\n",
        "        print(\"\u2705 Her iki sonu\u00e7 da g\u00fcvenilir\")\n",
        "        print(\"\ud83c\udfaf \u0130kisini de raporlayabilirsiniz\")\n",
        "\n",
        "    # Model sonu\u00e7lar\u0131n\u0131 kaydet\n",
        "    results_summary = {\n",
        "        'total_models_tested': len(kfold_results),\n",
        "        'total_time_minutes': total_time/60,\n",
        "        'best_model': best_kfold['model_info']['description'],\n",
        "        'best_kfold_f1': best_kfold['kfold_f1_mean'],\n",
        "        'best_kfold_std': best_kfold['kfold_f1_std'],\n",
        "        'average_kfold_f1': np.mean(kfold_f1s),\n",
        "        'average_single_f1': np.mean(current_f1s),\n",
        "        'average_difference': np.mean(differences),\n",
        "        'methodology': '5-Fold Stratified Cross Validation'\n",
        "    }\n",
        "\n",
        "    # Excel'e kaydet\n",
        "    academic_df.to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/7_models_kfold_cv_results.xlsx\", index=False)\n",
        "\n",
        "    # \u00d6zet kaydet\n",
        "    pd.DataFrame([results_summary]).to_excel(\"/content/drive/MyDrive/Makine \u00d6\u011frenmesi/kfold_cv_summary.xlsx\", index=False)\n",
        "\n",
        "    print(f\"\\n\u2705 Sonu\u00e7lar kaydedildi:\")\n",
        "    print(f\"\ud83d\udcc1 7_models_kfold_cv_results.xlsx\")\n",
        "    print(f\"\ud83d\udcc1 kfold_cv_summary.xlsx\")\n",
        "\n",
        "else:\n",
        "    print(\"\u274c Hi\u00e7bir model ba\u015far\u0131yla test edilemedi!\")\n",
        "\n",
        "print(f\"\\n\ud83c\udf8a 7 MODEL K-FOLD CROSS VALIDATION TAMAMLANDI!\")\n",
        "print(f\"\ud83d\udcca Art\u0131k modellerinizin robust performans\u0131n\u0131 biliyorsunuz!\")\n",
        "print(f\"\ud83c\udf93 Academic raporunuzda bu sonu\u00e7lar\u0131 kullanabilirsiniz!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNy1gEegNGgUttv3IWQRo1G"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}